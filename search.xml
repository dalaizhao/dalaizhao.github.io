<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[os-链接]]></title>
    <url>%2Fp%2F797a.html</url>
    <content type="text"><![CDATA[链接；将不同部分的代码和数据收集和组合成为一个单一文件的过程，这个文件可被加载（或被拷贝）到存储器执行了。链接是由叫链接器（linker）的程序自动执行的。 静态链接 加载时的共享库的动态链接 运行时的共享库的动态链接 编译器驱动程序预处理器（cpp）、编译器（ccl）、汇编器（as）、链接器（ld） shell调用一个在操作系统中叫加载器（loader）的函数，它拷贝可执行文件p中的代码和数据到存储器，然后将控制器转移到这个程序开头。 静态链接unix ld静态链接器以一组可重定位目标文件和命令行参数作为输入，生成一个一个完全链接的可以加载和运行的可执行目标文件作为输出。 为了创建可执行文件，链接器必须完成两个主要任务： 符号解析：目标文件定义和引用符号，符号解析的目的是将每个符号引用和符号定义联系起来 重定位：编译器和汇编器生成从地址零开始的代码和数据节，链接器通过把每个符号定义与一个存储器位置联系起来，然后修改所有对这些符号的引用，使得他们指向这个存储器位置，从而重定位这些节。 目标文件目标文件有三种形式： 可重定位目标文件：包含二进制代码和数据，其形式可以在编译时与其他可重定位目标文件合并起来，创建一个可执行目标文件。 可执行目标文件：包含二进制代码和数据，其形式可以被直接拷贝到存储器并执行。 共享目标文件：一种特殊类型的可重定位目标文件，可以在加载或者运行时，被动态地加载到存储器并链接。 编译器和汇编器生成可重定位目标文件（包括共享目标文件）；链接器生成可执行目标文件； 可重定位目标文件 ELF头（ELF header）以一个16字节的序列开始，这个序列描述了字的大小和生成该文件的系统的字节顺序。其中包括ELF头的大小，目标文件类型（比如可重定位、可执行或者共享的），机器类型（IA32），节头部表的文件偏移，以及节头部表中的表目大小和数量。 .text：已编译程序的机器代码 .rodata：只读数据，比如switch语句的跳转表 .data: 已初始化的全局变量 .bss：未初始化的全局变量 .symtab：符号表，它存放在程序中被定义和引用的函数和全局变量的信息。 .rel.text：当链接器把这个目标文件和其他文件结合时，.text节中的许多位置都需要修改。 .rel.data：被模块定义或引用的任何全局变量的信息 .debug：一个调试符号表，其有些表目是程序中定义的局部变量和类型定义。 .line：原始C源程序中的行号和.text节中机器执行之间的映射 .strtab: 一个字符串表，包括.symtab 符号和符号表C程序员使用static属性在模块内部隐藏变量和函数声明，就像用public 和 private声明一样，使用static属性的全局变量和函数时私有的，反之公共的。 GNU READELF 工具展示重复定位目标文件内容 符号解析链接器解析符号引用的方法是将每个引用与它输入的可重定位文件的符号表的一个确定的符号定义联系起来。 编译器还确保静态本地变量，它们也会有本地链接器符号，拥有唯一的名字。 C++/java的方法重载，编译器将每个唯一的方法和参数列表组合编码成一个对链接器来说唯一的名字，这种编码过程叫做毁坏，而相反的过程叫做恢复。 链接器如何解析多处定义的全局符号？？在编译时，编译器输出每个全局符号给汇编器，或者是强，或者是弱，而汇编器把这个消息隐含地编码在可重复定位文件的符号表里。 函数和已初始化的全局变量是强符号，未初始化的全局变量是弱符号。 根据强弱符号的定义，unix链接器使用下面的规则来处理多处定义的符号： 规则1: 不允许有多个强符号 规则2：如果有一个强符号和多个弱符号，那么选择强符号 规则3:如果有多个弱符号，那么从这些弱符号中任意选择一个 与静态库链接在unix系统中，静态库以一种称为存档的特殊文件格式存放在磁盘中。 AR工具打包成静态库文件 链接器如何使用静态库来解析引用如果在命令中，定义一个符号的库出现在引用这个符号的目标文件之前，那么引用就不能被解析，链接会失败。 重定位重定位：合并输入模块，并为每个符号分配运行时地址，分两步 重定位节和符号定义：在这一步中，链接器将所有的相同类型的节合并为同一类型的新的聚合节。 重定位节中的符号引用：在这一步中，链接器修改代码节和数据节中对每个符号的引用，使的它们指向正确的运行时地址。依赖于重定位表目的可重定位目标模块中的数据结构。 重定位表目汇编器遇到对最终位置未知的目标引用时就会生成一个重定位表目，告诉链接器在将目标文件合并成可执行文件时如何修改这个引用。代码的重定位表目放在.relo.text中，已初始化数据的重定位表目放在.relo.data中。 可执行目标文件典型的ELF可执行文件的各类信息 加载可执行目标文件加载器将可执行目标文件中的代码和数据从磁盘拷贝到存储器中，然后通过跳转到程序的第一条指令，即入口点来运行该程序，这个将程序拷贝到存储器并运行的过程叫做加载。 动态链接共享库共享库致力于解决静态库缺陷的现代创新产物。共享库是一个目标模块，在运行时，可以加载到任意的存储器地址，并在存储器中和一个程序链接起来。这个过程称为动态链接，是由一个叫动态链接器的程序来执行的。 微软的操作系统大量地利用了共享库，它们称为DDL（动态链接库）。 对于一个共享库只有一个.so文件，创建共享库libvector.so： -fPIC选项指示编译器生成与位置无关的代码-shared选项指示链接器创建一个共享的目标文件 unix&gt; gcc -o p2 main2.c ./libvertor.so 从应用程序中加载和链接共享库与位置无关的代码（PIC）编译库代码，使得不需要链接修改库代码，就可以在任何地址加载和执行这些代码，这样的代码叫做与位置无关的代码（PIC）。 处理目标文件的工具 总结链接可以在编译时静态编译器来完成，也可以在加载时和运行时由动态链接器来完成。链接器处理称为目标文件的二进制文件，它有三种不同的形式：可重定位的、可执行的和共享的。可重定位的目标文件由静态链接器组合成一个可执行的目标文件，它可以加载到存储器中并执行。共享目标文件（共享库）是在运行时由动态链接器链接和加载的，或者隐含地调用程序被加载和开始执行时，或者根据需要在程序调用dlopen库的函数时。 链接器的两个主要任务是符号解析和重定位。符号解析将目标文件中的每个全局符号都绑定到一个唯一的定义，而重定位确定每个符号的最终存储器地址，并修改时对那些目标的引用。]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>os</tag>
        <tag>csapp</tag>
        <tag>c</tag>
        <tag>链接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[os-线程总结]]></title>
    <url>%2Fp%2Fe68.html</url>
    <content type="text"></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>os</tag>
        <tag>csapp</tag>
        <tag>进程</tag>
        <tag>线程</tag>
        <tag>c</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[os-进程总结]]></title>
    <url>%2Fp%2F692d.html</url>
    <content type="text"></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>os</tag>
        <tag>csapp</tag>
        <tag>进程</tag>
        <tag>线程</tag>
        <tag>c</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[os-进程线程关系及区别]]></title>
    <url>%2Fp%2F8e8c.html</url>
    <content type="text"><![CDATA[关系&amp;区别进程(process)是资源分配的最小单位，有自己独立的地址空间，单个进程的崩溃不会影响其它进程； 线程(thread)是处理器调度的最小单位，共享进程空间，也有自己的独立内存（PC、栈、栈指针等空间），单个线程的崩溃会导致整个进程的崩溃，影响其它的线程； 对于一些要求同时进行且又要共享某些变量的并发操作，只能用线程，不能用进程。 一进程里至少有一个线程。 通俗的讲 “如果进程是工厂，线程就是工人” 优缺点 地址空间和其它资源 进程间相互独立，同一进程的各线程间共享。 某进程内的线程在其它进程不可见。 通信：进程间通信IPC 线程间可以直接读写进程数据段（如全局变量）来进行通信 进程同步和互斥需要辅助，以保证数据的一致性。 调度和切换 线程上下文切换比进程上下文切换要快得多。 多线程OS中 进程不是一个可执行的实体。]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>os</tag>
        <tag>csapp</tag>
        <tag>进程</tag>
        <tag>线程</tag>
        <tag>c</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[命令-man使用方法]]></title>
    <url>%2Fp%2Fdaf6.html</url>
    <content type="text"><![CDATA[linux的man手册共有以下几个章节（参数）: 参数 描述 1 标准命令，用户命令章节，所有用户都可以使用 2 系统调用 3 库函数(c) 4 设备及特殊文件说明 5 配置文件格式及相关参数 6 游戏 7 杂项 8 管理员命令 man手册段落含义： NAME：命令的名称及简要说明 DESCRIPTION:命令功能的详细描述 OPTIONS：所支持的选项的相关说明 SYNOPSIS：使用格式 EXAMPLES：使用示例 NOTES：相关注意事项 FILES：相关的配置文件 SEE ALSO：相关参考]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>man</tag>
        <tag>命令</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git命令收集]]></title>
    <url>%2Fp%2Fea70.html</url>
    <content type="text"><![CDATA[git命令收集，要用时就怕忘记，备份一份。 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 新建代码库12345678# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]# 下载一个项目和它的整个代码历史$ git clone [url] 配置12345678910111213141516171819# 显示当前的Git配置$ git config --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name "[name]"$ git config [--global] user.email "[email address]"# git status等命令自动着色git config --global color.ui true git config --global color.status autogit config --global color.diff autogit config --global color.branch autogit config --global color.interactive auto# remove proxy configuration on gitgit config --global --unset http.proxy 增加/删除文件123456789101112131415161718192021# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交$ git add -p# 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file]# 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 代码提交123456789101112131415161718# 提交暂存区到仓库区$ git commit -m [message]# 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a# 提交时显示所有diff信息$ git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化$ git commit --amend [file1] [file2] ... 分支123456789101112131415161718192021222324252627282930313233343536373839404142# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支$ git checkout -b [branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 切换到上一个分支$ git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支$ git merge [branch]# 选择一个commit，合并进当前分支$ git cherry-pick [commit]# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 标签1234567891011121314151617181920212223242526# 列出所有tag$ git tag# 新建一个tag在当前commit$ git tag [tag]# 新建一个tag在指定commit$ git tag [tag] [commit]# 删除本地tag$ git tag -d [tag]# 删除远程tag$ git push origin :refs/tags/[tagName]# 查看tag信息$ git show [tag]# 提交指定tag$ git push [remote] [tag]# 提交所有tag$ git push [remote] --tags# 新建一个分支，指向某个tag$ git checkout -b [branch] [tag] 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 搜索提交历史，根据关键词$ git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其"提交说明"必须符合搜索条件$ git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示过去5次提交$ git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序$ git shortlog -sn# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码$ git diff --shortstat "@&#123;0 day ago&#125;"# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示所有提交，包括孤立节点$ git reflog 远程同步1234567891011121314151617181920212223# 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file]# 恢复暂存区的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file]# 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]# 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop 其它12# 生成一个可供发布的压缩包$ git archive 参考阮一峰的网络日志gist.github]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jedis API 详细示例]]></title>
    <url>%2Fp%2F56c8.html</url>
    <content type="text"><![CDATA[Redis 简介Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。 Redis 与其他 key - value 缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 Redis 优势 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 Redis与其他key-value存储有什么不同？ Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。 Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 redis 数据类型Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 maven依赖12345678910111213&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/redis.clients/jedis --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Jedis API 操作示例Jedis 连接池连接Jedis线程安全单列模式可配置多个redis数据源的连接池。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class JedisApi &#123; private static final Logger LOG = LoggerFactory.getLogger(JedisApi.class); private volatile static JedisApi jedisApi; /** * 保存多个连接源 */ private static Map&lt;String, JedisPool&gt; poolMap = new HashMap&lt;String, JedisPool&gt;(); private JedisApi() &#123; &#125; /** * @Description: jedisPool 池 * @Param: [ip, port] * @return: redis.clients.jedis.JedisPool * @Author: imdalai * @Date: 2018/1/15 */ private static JedisPool getPool(String ip, int port) &#123; try &#123; String key = ip + ":" + port; JedisPool pool = null; if (!poolMap.containsKey(key)) &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxIdle(RedisConfig.MAX_IDLE); config.setMaxTotal(RedisConfig.MAX_ACTIVE); // 在获取连接的时候检查有效性, 默认false config.setTestOnBorrow(true); // 在空闲时检查有效性, 默认false config.setTestOnReturn(true); pool = new JedisPool(config, ip, port, RedisConfig.TIME_OUT); poolMap.put(key, pool); &#125; else &#123; pool = poolMap.get(key); &#125; return pool; &#125; catch (Exception e) &#123; LOG.error("init jedis pool failed ! " + e.getMessage(), e); &#125; return null; &#125; /** * @Description: 线程安全单列模式 * @Param: [] * @return: JedisApi * @Author: imdalai * @Date: 2018/1/15 */ public static JedisApi getRedisApi() &#123; if (jedisApi == null) &#123; synchronized (JedisApi.class) &#123; if (jedisApi == null) &#123; jedisApi = new JedisApi(); &#125; &#125; &#125; return jedisApi; &#125; /** * @Description: 获取一个jedis连接 * @Param: [ip, port] * @return: redis.clients.jedis.Jedis * @Author: imdalai * @Date: 2018/1/15 */ public Jedis getRedis(String ip, int port) &#123; Jedis jedis = null; int count = 0; while (jedis == null &amp;&amp; count &lt;= RedisConfig.RETRY_NUM) &#123; try &#123; jedis = getPool(ip, port).getResource(); &#125; catch (Exception e) &#123; LOG.error("get redis failed ! " + e.getMessage(), e); count++; &#125; &#125; return jedis; &#125; /** * @Description: 释放jedis到jedisPool中 * @Param: [jedis, ip, port] * @return: void * @Author: imdalai * @Date: 2018/1/15 */ public void closeRedis(Jedis jedis) &#123; if (jedis != null) &#123; try &#123; jedis.close(); &#125; catch (Exception e) &#123; LOG.error("colse jedis failed ! " + e.getMessage(), e); &#125; &#125; &#125;&#125; 键（key）12345678910111213141516171819202122232425public static void testKey() &#123; System.out.println("====key功能展示===="); try &#123; jedis.select(0); System.out.println("清除数据：" + jedis.flushDB()); System.out.println("判断某个键是否存在：" + jedis.exists("1")); System.out.println("新增&#123;1，a&#125;键值对:" + jedis.set("1", "a")); System.out.println(jedis.exists("1")); System.out.println("新增&#123;2，b&#125;键值对:" + jedis.set("2", "b")); System.out.println("系统中所有的键如下：" + jedis.keys("*").toString()); System.out.println("删除键 1:" + jedis.del("1")); System.out.println("判断键 1是否存在：" + jedis.exists("1")); System.out.println("设置键 2的过期时间为5s:" + jedis.expire("2", 5)); TimeUnit.SECONDS.sleep(2); System.out.println("查看键 2的剩余生存时间：" + jedis.ttl("2")); System.out.println("移除键 2的生存时间：" + jedis.persist("2")); System.out.println("查看键 2的剩余生存时间：" + jedis.ttl("2")); System.out.println("查看键 2所存储的值的类型：" + jedis.type("2")); System.out.println("查看键 2的值：" + jedis.get("2")); System.out.println(""); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 输出结果： ====key功能展示====清除数据：OK判断某个键是否存在：false新增{1，a}键值对:OKtrue新增{2，b}键值对 :OK系统中所有的键如下：[1, 2]删除键 1:1判断键 1是否存在：false设置键 2的过期时间为5s:1查看键 2的剩余生存时间：3移除键 2的生存时间：1查看键 2的剩余生存时间：-1查看键 2所存储的值的类型：string查看键 2的值：b 字符串、整型和浮点数string是redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。 string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。 string类型是Redis最基本的数据类型，一个键最大能存储512MB。 字符串 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static void testString() &#123; try &#123; jedis.select(1); jedis.flushDB(); System.out.println("====字符串功能展示===="); System.out.println("增:"); System.out.println(jedis.set("a", "1")); System.out.println(jedis.set("b", "2")); System.out.println(jedis.set("c", "3")); System.out.println("删除键 a:" + jedis.del("a")); System.out.println("获取键 a:" + jedis.get("a")); System.out.println("修改键 b:" + jedis.set("b", "bChanged")); System.out.println("获取键 b 的值:" + jedis.get("b")); System.out.println("在键 c后面加入值：" + jedis.append("c", "End")); System.out.println("获取键 c的值：" + jedis.get("c")); System.out.println("增加多个键值对：" + jedis.mset("key01", "value01", "key02", "value02", "key03", "value03")); System.out.println("获取多个键值对：" + jedis.mget("key01", "key02", "key03")); System.out.println("获取多个键值对：" + jedis.mget("key01", "key02", "key03", "key04")); System.out.println("删除多个键值对：" + jedis.del(new String[]&#123;"key01", "key02"&#125;)); System.out.println("获取多个键值对：" + jedis.mget("key01", "key02", "key03")); jedis.flushDB(); System.out.println("新增键值对防止覆盖原先值:"); System.out.println(jedis.setnx("key001", "value001")); System.out.println(jedis.setnx("key002", "value002")); System.out.println(jedis.setnx("key002", "value002-new")); System.out.println("获取键key001的值：" + jedis.get("key001")); System.out.println("获取键key002的值：" + jedis.get("key002")); System.out.println("新增键值对并设置有效时间:"); System.out.println(jedis.setex("key003", 2, "value003")); System.out.println("获取键key003的值：" + jedis.get("key003")); TimeUnit.SECONDS.sleep(3); System.out.println("获取键key003的值：" + jedis.get("key003")); System.out.println("获取原值，更新为新值:"); System.out.println(jedis.getSet("key002", "key2GetSet")); System.out.println("获取键key002的值：" + jedis.get("key002")); System.out.println("截取key002的值的字符串：" + jedis.getrange("key002", 2, 5)); System.out.println(""); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 输出结果： ====字符串功能展示====增:OKOKOK删除键 a:1获取键 a:null修改键 b:OK获取键 b 的值:bChanged在键 c后面加入值：4获取键 c的值：3End增加多个键值对：OK获取多个键值对：[value01, value02, value03]获取多个键值对：[value01, value02, value03, null]删除多个键值对：2获取多个键值对：[null, null, value03]新增键值对防止覆盖原先值:110获取键key001的值：value001获取键key002的值：value002新增键值对并设置有效时间:OK获取键key003的值：value003获取键key003的值：null获取原值，更新为新值:value002获取键key002的值：key2GetSet截取key002的值的字符串：y2Ge 整数和浮点数 123456789101112131415161718192021222324public static void testNumber() &#123; try &#123; jedis.select(2); jedis.flushDB(); System.out.println("====整数和浮点数功能展示===="); jedis.set("key001", "1"); jedis.set("key002", "2"); jedis.set("key003", "3.3"); System.out.println("获取键key001的值：" + jedis.get("key001")); System.out.println("获取键key002的值：" + jedis.get("key002")); System.out.println("将键key001的值+1：" + jedis.incr("key001")); System.out.println("获取键key001的值：" + jedis.get("key001")); System.out.println("将键key002的值-1：" + jedis.decr("key002")); System.out.println("获取键key002的值：" + jedis.get("key002")); System.out.println("将key001的值加上整数5：" + jedis.incrBy("key001", 5)); System.out.println("获取key001的值：" + jedis.get("key001")); System.out.println("将key002的值减去整数5：" + jedis.decrBy("key002", 5)); System.out.println("获取key002的值：" + jedis.get("key002")); System.out.println(""); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 输出结果： ====整数和浮点数功能展示====获取键key001的值：1获取键key002的值：2将键key001的值+1：2获取键key001的值：2将键key002的值-1：1获取键key002的值：1将key001的值加上整数5：7获取key001的值：7将key002的值减去整数5：-4获取key002的值：-4 List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 1234567891011121314151617181920212223242526272829303132333435363738394041public static void testList() &#123; jedis.select(3); jedis.flushDB(); System.out.println("====列表list功能展示===="); jedis.lpush("collections", "ArrayList", "LinkedList", "Vector", "Stack", "queue"); jedis.lpush("collections", "HashMap"); jedis.lpush("collections", "HashMap"); jedis.lpush("collections", "HashMap"); jedis.lpush("collections", "HashMap"); jedis.lpush("number", "1"); jedis.lpush("number", "2"); jedis.lpush("number", "3"); // -1 代表倒数第一个 System.out.println("collections 的内容：" + jedis.lrange("collections", 0, -1)); System.out.println("collections区间0-2内容：" + jedis.lrange("collections", 0, 2)); System.out.println("================="); // 删除列表指定的值 ，第二个参数为删除的个数（有重复时），后add进去的值先被删，类似于出栈 System.out.println("删除指定元素个数：" + jedis.lrem("collections", 2, "HashMap")); System.out.println("collections 的内容：" + jedis.lrange("collections", 0, -1)); System.out.println("删除区间0-4以外的数据：" + jedis.ltrim("collections", 0, 4)); System.out.println("collections 的内容：" + jedis.lrange("collections", 0, -1)); System.out.println("collections列表出栈（左端）：" + jedis.lpop("collections")); System.out.println("collections的内容：" + jedis.lrange("collections", 0, -1)); System.out.println("collections添加元素，从列表右端，与lpush相对应：" + jedis.rpush("collections", "EnumMap")); System.out.println("collections的内容：" + jedis.lrange("collections", 0, -1)); System.out.println("collections列表出栈（右端）：" + jedis.rpop("collections")); System.out.println("collections的内容：" + jedis.lrange("collections", 0, -1)); System.out.println("修改collections指定下标1的内容：" + jedis.lset("collections", 1, "LinkedArrayList")); System.out.println("collections的内容：" + jedis.lrange("collections", 0, -1)); System.out.println("================="); System.out.println("collections的长度：" + jedis.llen("collections")); System.out.println("获取collections下标为2的元素：" + jedis.lindex("collections", 2)); System.out.println("================="); jedis.lpush("sortedList", "3", "6", "2", "0", "7", "4"); System.out.println("sortedList排序前：" + jedis.lrange("sortedList", 0, -1)); System.out.println(jedis.sort("sortedList")); System.out.println("sortedList排序后：" + jedis.lrange("sortedList", 0, -1)); System.out.println(""); &#125; 输出结果： ====列表list功能展示====collections 的内容：[HashMap, HashMap, HashMap, HashMap, queue, Stack, Vector, LinkedList, ArrayList]collections区间0-2内容：[HashMap, HashMap, HashMap]=================删除指定元素个数：2collections 的内容：[HashMap, HashMap, queue, Stack, Vector, LinkedList, ArrayList]删除区间0-4以外的数据：OKcollections 的内容：[HashMap, HashMap, queue, Stack, Vector]collections列表出栈（左端）：HashMapcollections的内容：[HashMap, queue, Stack, Vector]collections添加元素，从列表右端，与lpush相对应：5collections的内容：[HashMap, queue, Stack, Vector, EnumMap]collections列表出栈（右端）：EnumMapcollections的内容：[HashMap, queue, Stack, Vector]修改collections指定下标1的内容：OKcollections的内容：[HashMap, LinkedArrayList, Stack, Vector]=================collections的长度：4获取collections下标为2的元素：Stack================sortedList排序前：[4, 7, 0, 2, 6, 3][0, 2, 3, 4, 6, 7]sortedList排序后：[4, 7, 0, 2, 6, 3] 集合（Set）Redis的Set是string类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 12345678910111213141516171819202122232425262728293031323334353637383940public static void testSet() &#123; try &#123; jedis.select(4); jedis.flushDB(); System.out.println("========测试集合（set）========="); System.out.println("集合set添加数据：" + jedis.sadd("setElement", "e1", "e7", "e3", "e6", "e0", "e4")); System.out.println(jedis.sadd("setElement", "e6")); System.out.println("setElement的所有元素：" + jedis.smembers("setElement")); System.out.println("删除元素e0:" + jedis.srem("setElement", "e0")); System.out.println("setElement的所有元素：" + jedis.smembers("setElement")); System.out.println("删除两个元素e7和e6：" + jedis.srem("setElement", "e7", "e6")); System.out.println("setElement的所有元素为：" + jedis.smembers("setElement")); System.out.println("随机的移除集合中的一个元素：" + jedis.spop("setElement")); System.out.println("随机的移除集合中的一个元素：" + jedis.spop("setElement")); System.out.println("setElement的所有元素为：" + jedis.smembers("setElement")); System.out.println("setElement中包含元素的个数：" + jedis.scard("setElement")); System.out.println("e3是否在setElement中：" + jedis.sismember("setElement", "e3")); System.out.println("e1是否在setElement中：" + jedis.sismember("setElement", "e1")); System.out.println("================="); System.out.println(jedis.sadd("setElement1", "e1", "e2", "e4", "e3", "e0", "e8", "e7", "e5")); System.out.println(jedis.sadd("setElement2", "e1", "e2", "e4", "e3", "e0", "e8")); System.out.println("将setElement1中删除e1并存入setElement3中：" + jedis.smove("setElement1", "setElement3", "e1")); System.out.println("将setElement1中删除e2并存入setElement3中：" + jedis.smove("setElement1", "setElement3", "e2")); System.out.println("setElement1中的元素：" + jedis.smembers("setElement1")); System.out.println("setElement3中的元素：" + jedis.smembers("setElement3")); System.out.println("集合运算:"); System.out.println("setElement1中的元素：" + jedis.smembers("setElement1")); System.out.println("setElement2中的元素：" + jedis.smembers("setElement2")); System.out.println("setElement1和setElement2的交集:" + jedis.sinter("setElement1", "setElement2")); System.out.println("setElement1和setElement2的并集:" + jedis.sunion("setElement1", "setElement2")); // setElement1中有，setElement2中没有 System.out.println("setElement1和setElement2的差集:" + jedis.sdiff("setElement1", "setElement2")); System.out.println(""); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 输出结果： ========测试集合（set）=========集合set添加数据：60setElement的所有元素：[e3, e6, e1, e7, e0, e4]删除元素e0:1setElement的所有元素：[e6, e3, e1, e7, e4]删除两个元素e7和e6：2setElement的所有元素为：[e3, e1, e4]随机的移除集合中的一个元素：e3随机的移除集合中的一个元素：e4setElement的所有元素为：[e1]setElement中包含元素的个数：1e3是否在setElement中：falsee1是否在setElement中：true=================86将setElement1中删除e1并存入setElement3中：1将setElement1中删除e2并存入setElement3中：1setElement1中的元素：[e5, e8, e3, e7, e0, e4]setElement3中的元素：[e1, e2]集合运算:setElement1中的元素：[e5, e8, e3, e7, e0, e4]setElement2中的元素：[e3, e4, e2, e8, e1, e0]setElement1和setElement2的交集:[e3, e4, e8, e0]setElement1和setElement2的并集:[e5, e8, e3, e1, e7, e0, e2, e4]setElement1和setElement2的差集:[e5, e7] hashRedis hash 是一个键值(key=&gt;value)对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 1234567891011121314151617181920212223242526272829303132333435public static void testHash() &#123; try &#123; System.out.println("=======集合（Set）======="); jedis.select(5); jedis.flushDB(); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("key001", "value001"); map.put("key002", "value002"); map.put("key003", "value003"); jedis.hmset("hash", map); jedis.hset("hash", "key004", "value004"); // return Map&lt;String,String&gt; System.out.println("散列hash的所有键值对为：" + jedis.hgetAll("hash")); // return Set&lt;String&gt; System.out.println("散列hash的所有键为：" + jedis.hkeys("hash")); // return List&lt;String&gt; System.out.println("散列hash的所有值为：" + jedis.hvals("hash")); System.out.println("将key006保存的值加上一个整数，如果key006不存在则添加key006：" + jedis.hincrBy("hash", "key006", 6)); System.out.println("散列hash的所有键值对为：" + jedis.hgetAll("hash")); System.out.println("将key006保存的值加上一个整数，如果key006不存在则添加key006：" + jedis.hincrBy("hash", "key006", 3)); System.out.println("散列hash的所有键值对为：" + jedis.hgetAll("hash")); System.out.println("删除一个或者多个键值对：" + jedis.hdel("hash", "key002")); System.out.println("散列hash的所有键值对为：" + jedis.hgetAll("hash")); System.out.println("散列hash中键值对的个数：" + jedis.hlen("hash")); System.out.println("判断hash中是否存在key002：" + jedis.hexists("hash", "key002")); System.out.println("判断hash中是否存在key003：" + jedis.hexists("hash", "key003")); System.out.println("获取hash中的值：" + jedis.hmget("hash", "key003")); System.out.println("获取hash中的值：" + jedis.hmget("hash", "key003", "key004")); System.out.println(""); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 输出结果： =======集合（Set）=======散列hash的所有键值对为：{key004=value004, key003=value003, key002=value002, key001=value001}散列hash的所有键为：[key004, key003, key002, key001]散列hash的所有值为：[value001, value003, value002, value004]将key006保存的值加上一个整数，如果key006不存在则添加key006：6散列hash的所有键值对为：{key004=value004, key003=value003, key006=6, key002=value002, key001=value001}将key006保存的值加上一个整数，如果key006不存在则添加key006：9散列hash的所有键值对为：{key004=value004, key003=value003, key006=9, key002=value002, key001=value001}删除一个或者多个键值对：1散列hash的所有键值对为：{key004=value004, key003=value003, key006=9, key001=value001}散列hash中键值对的个数：4判断hash中是否存在key002：false判断hash中是否存在key003：true获取hash中的值：[value003]获取hash中的值：[value003, value004] 有序集合zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 123456789101112131415161718192021222324252627282930313233public static void testSortSet() &#123; try &#123; System.out.println("=======有序集合======="); jedis.select(6); jedis.flushDB(); Map&lt;String, Double&gt; map = new HashMap&lt;String, Double&gt;(); map.put("key2", 1.2); map.put("key3", 4.0); map.put("key4", 5.0); map.put("key5", 0.2); System.out.println(jedis.zadd("zset", 3, "key1")); System.out.println(jedis.zadd("zset", map)); System.out.println("zset中的所有元素：" + jedis.zrange("zset", 0, -1)); System.out.println("zset中的所有元素：" + jedis.zrangeWithScores("zset", 0, -1)); System.out.println("zset中的所有元素：" + jedis.zrangeByScore("zset", 0, 100)); System.out.println("zset中的所有元素：" + jedis.zrangeByScoreWithScores("zset", 0, 100)); System.out.println("zset中key2的分值：" + jedis.zscore("zset", "key2")); System.out.println("zset中key2的排名：" + jedis.zrank("zset", "key2")); System.out.println("删除zset中的元素key3：" + jedis.zrem("zset", "key3")); System.out.println("zset中的所有元素：" + jedis.zrange("zset", 0, -1)); System.out.println("zset中元素的个数：" + jedis.zcard("zset")); System.out.println("zset中分值在1-4之间的元素的个数：" + jedis.zcount("zset", 1, 4)); System.out.println("key2的分值加上5：" + jedis.zincrby("zset", 5, "key2")); System.out.println("key3的分值加上4：" + jedis.zincrby("zset", 4, "key3")); System.out.println("zset中的所有元素：" + jedis.zrange("zset", 0, -1)); System.out.println(""); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 输出结果： =======有序集合=======14zset中的所有元素：[key5, key2, key1, key3, key4]zset中的所有元素：[[[107, 101, 121, 53],0.2], [[107, 101, 121, 50],1.2], [[107, 101, 121, 49],3.0], [[107, 101, 121, 51],4.0], [[107, 101, 121, 52],5.0]]zset中的所有元素：[key5, key2, key1, key3, key4]zset中的所有元素：[[[107, 101, 121, 53],0.2], [[107, 101, 121, 50],1.2], [[107, 101, 121, 49],3.0], [[107, 101, 121, 51],4.0], [[107, 101, 121, 52],5.0]]zset中key2的分值：1.2zset中key2的排名：1删除zset中的元素key3：1zset中的所有元素：[key5, key2, key1, key4]zset中元素的个数：4zset中分值在1-4之间的元素的个数：2key2的分值加上5：6.2key3的分值加上4：4.0zset中的所有元素：[key5, key1, key3, key4, key2] 参考redisAPI简单使用API整理原处-朱小斯]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>jedis api</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark LDA 官方文档]]></title>
    <url>%2Fp%2F91ef.html</url>
    <content type="text"><![CDATA[学习[spark lda]https://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda() 英文文档,为了学习方便顺便翻译了。 理论参数狄利克雷分配（Latent Dirichlet allocation LDA）是一个从文本文献集合中推断主题的主题模型。 LDA可以被认为是如下的聚类算法： 主题对应于聚类中心，文档对应于数据集中的示例（数据行）。 主题和文档都存在于特征空间中，其中特征向量是词计数的向量（词袋）。 LDA不是使用传统距离来估计聚类，而是使用基于文本文档如何生成的统计模型的函数。 LDA通过setOptimizer函数支持不同的推理算法。 EMLDAOptimizer利用似然函数的期望最大化来学习聚类，并产生综合结果，而OnlineLDAOptimizer使用迭代小批量采样进行在线变分推理，并且通常是内存友好的。 LDA将文档集合作为词计数的向量和以下参数（使用构建器模式进行设置）： k：主题数（即集群中心） optimizer：用于学习LDA模型的optimizer，可以是EMLDAOptimizer或OnlineLDAOptimizer docConcentration：Dirichlet参数，用于事先分配文档在主题上的分布。 较大的值可以促进更平滑的推断分配。 topicConcentration：Dirichlet参数，用于关于术语（词）的主题分布。 较大的值可以促进更平滑的推断分配。 maxIterations：限制迭代次数。 checkpointInterval：如果使用点检查（在Spark配置中设置），则此参数指定检查点的创建频率。 如果maxIterations较大，则使用检查点可以帮助减少磁盘上的洗牌文件大小，并有助于故障恢复。 所有spark.mllib的LDA模型都支持： describeTopics：将主题返回为最重要的术语和术语权重数组 topicsMatrix：返回一个vocabSize由k矩阵，其中每列是一个主题 注意：在积极开发下，LDA仍然是一个实验性的功能。 因此，某些功能仅在优化程序生成的两个优化程序/模型之一中可用。 目前，分布式模型可以被转换成本地模型，但是反之亦然。 以下讨论将分别描述每个optimizer/model 对。 期望最大化在EMLDAOptimizer和DistributedLDAModel中实现。对于提供给LDA的参数： docConcentration：只支持对称先验，因此提供的k维向量中的所有值必须相同。 所有的值也必须是[Math Processing Error]&gt; 1.0。 提供向量（-1）的默认行为（统一的k维向量值为[数学处理错误]（50 / k）+1 topicConcentration：只支持对称的先验值，值必须为[Math Processing Error]&gt; 1.0。 导致默认值为[数学处理错误] 0.1 + 1 maxIterations：EM迭代的最大次数。 注意：做足够的迭代很重要。 在早期迭代中，EM经常具有无用的主题，但是这些主题经过多次迭代后会显着提高。 根据你的数据集，使用至少20次和50-100次迭代通常是合理的。 EMLDAOptimizer生成一个DistributedLDAModel，它不仅存储推断的主题，而且存储训练语料库中每个文档的完整训练语料库和主题分布。 DistributedLDAModel支持： topTopicsPerDocument：训练语料库中每个文档的热门主题及其权重 topDocumentsPerTopic：每个主题的顶端文档以及文档中主题的相应权重。 logPrior：给定超参数的估计主题和文档 - 主题分布的对数概率docConcentration和topicConcentration logLikelihood：给定推断主题和文档 - 主题分布的训练语料库的对数似然性 在线变分贝叶斯在OnlineLDAOptimizer和LocalLDAModel中实现。 对于提供给LDA的参数： docConcentration：不对称的先验可以通过在每个k维中传入一个值等于Dirichlet参数的向量来使用。 值应该是[数学处理错误]&gt; = 0。 提供向量（-1）导致默认行为（具有值[数学处理错误]（1.0 / k）的统一k维向量） topicConcentration：仅支持对称的先验。 值必须是[数学处理错误]&gt; = 0。 提供-1结果的默认值为[数学处理错误]（1.0 / k）。 maxIterations：要提交的最大小装数。 另外，OnlineLDAOptimizer接受以下参数： miniBatchFraction：在每次迭代中采样和使用的语料库的分数 optimizeDocConcentration：如果设置为true，则在每个小批次之后执行超参数docConcentration（aka）的最大似然估计，并在返回的LocalLDAModel tau0和kappa中设置优化docConcentration：用于学习 这是通过[数学处理误差]（τ0+ iter）-κ来计算的，其中[数学处理误差] iter是当前的迭代次数。 OnlineLDAOptimizer生成一个LocalLDAModel，它只存储推断的主题。 LocalLDAModel支持： logLikelihood（文档）：根据推断的主题计算提供文档的下限。 logPerplexity（文档）：根据推断的主题计算所提供文档的困惑度的上限。 示例在下面的例子中，我们加载了表示文档语料库的字数统计向量。 然后，我们使用LDA从文档中推断出三个主题。 所需簇的数量传递给算法。 然后我们输出主题，表示为概率分布的单词。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package sparkLDA;import scala.Tuple2;import org.apache.spark.api.java.*;import org.apache.spark.api.java.function.Function;import org.apache.spark.mllib.clustering.DistributedLDAModel;import org.apache.spark.mllib.clustering.LDA;import org.apache.spark.mllib.linalg.Matrix;import org.apache.spark.mllib.linalg.Vector;import org.apache.spark.mllib.linalg.Vectors;import java.io.File;import java.io.IOException;import org.apache.spark.SparkConf;public class SparkLDA &#123; private static String HADOOP_HOME_DIR = checkHadoopHome(); // 检查spark 环境 是否设置成功 private static String checkHadoopHome() &#123; // first check the Dflag hadoop.home.dir with JVM scope String home = System.getProperty("hadoop.home.dir"); // fall back to the system/user-global env variable if (home == null) &#123; home = System.getenv("HADOOP_HOME"); &#125; try &#123; // couldn't find either setting for hadoop's home directory if (home == null) &#123; throw new IOException("HADOOP_HOME or hadoop.home.dir are not set."); &#125; if (home.startsWith("\"") &amp;&amp; home.endsWith("\"")) &#123; home = home.substring(1, home.length() - 1); &#125; // check that the home setting is actually a directory that exists File homedir = new File(home); if (!homedir.isAbsolute() || !homedir.exists() || !homedir.isDirectory()) &#123; throw new IOException("Hadoop home directory " + homedir + " does not exist, is not a directory, or is not an absolute path."); &#125; home = homedir.getCanonicalPath(); &#125; catch (IOException ioe) &#123; System.out.println("null"); home = null; &#125; return home; &#125; public static void main(String[] args) throws IOException &#123; System.out.println(HADOOP_HOME_DIR); SparkConf conf = new SparkConf().setAppName("SOME APP NAME").setMaster("local[2]").set("spark.executor.memory", "1g"); JavaSparkContext sc = new JavaSparkContext(conf); // Load and parse the data String path = "./sample_lda_data.txt"; JavaRDD&lt;String&gt; data = sc.textFile(path); JavaRDD&lt;Vector&gt; parsedData = data.map(new Function&lt;String, Vector&gt;() &#123; public Vector call(String s) &#123; String[] sarray = s.trim().split(" "); double[] values = new double[sarray.length]; for (int i = 0; i &lt; sarray.length; i++) &#123; values[i] = Double.parseDouble(sarray[i]); &#125; return Vectors.dense(values); &#125; &#125;); // Index documents with unique IDs JavaPairRDD&lt;Long, Vector&gt; corpus = JavaPairRDD .fromJavaRDD(parsedData.zipWithIndex().map(new Function&lt;Tuple2&lt;Vector, Long&gt;, Tuple2&lt;Long, Vector&gt;&gt;() &#123; public Tuple2&lt;Long, Vector&gt; call(Tuple2&lt;Vector, Long&gt; doc_id) &#123; return doc_id.swap(); &#125; &#125;)); corpus.cache(); // Cluster the documents into three topics using LDA DistributedLDAModel ldaModel = (DistributedLDAModel) new LDA().setK(3).run(corpus); // Output topics. Each is a distribution over words (matching word count // vectors) System.out.println("Learned topics (as distributions over vocab of " + ldaModel.vocabSize() + " words):"); Matrix topics = ldaModel.topicsMatrix(); for (int topic = 0; topic &lt; 3; topic++) &#123; System.out.print("Topic " + topic + ":"); for (int word = 0; word &lt; ldaModel.vocabSize(); word++) &#123; System.out.print(" " + topics.apply(word, topic)); &#125; System.out.println(); &#125; sc.stop(); &#125;&#125; sample_lda_data.txt 内容：每行数据解析为Vector的结构。其中原始数据如下，每一行表示一个文档，每一列表示一个单词，每一个元素D(m,w)表示第m篇文档中单词w的词频 1234567891011121 2 6 0 2 3 1 1 0 0 31 3 0 1 3 0 0 2 0 0 11 4 1 0 0 4 9 0 1 2 02 1 0 3 0 0 5 0 2 3 93 1 1 9 3 0 2 0 0 1 34 2 0 3 4 5 1 1 1 4 02 1 0 3 0 0 5 0 2 2 91 1 1 9 2 1 2 0 0 1 34 4 0 3 4 2 1 3 0 0 02 8 2 0 3 0 2 0 2 7 21 1 1 9 0 2 2 0 0 3 34 1 0 0 4 5 1 3 0 1 0 输出： 123456789101112D:\ProgramFiles\hadoop-2.8.1Using Sparks default log4j profile: org/apache/spark/log4j-defaults.properties17/12/05 14:26:07 INFO SparkContext: Running Spark version 2.2.0略....Topic 0: 15.089683953506297 14.750709899385281 1.5312722542015011 1.9508857699006295 15.31823575747685612.59129347897879 7.3969781938882 6.943873430629874 2.4051185864422506 7.614176613626162 2.248599788783381Topic 1: 4.700089259409153 6.789874452485002 1.1186095797841795 9.247805430189871 2.9482721618042618 2.19614141542458 17.961253884900074 1.2321612842281264 4.657846439828521 10.10316570751245 26.544267285383096Topic 2: 6.210226787084551 7.459415648129717 9.350118166014319 28.8013087999095 6.733492080718882 7.21256510559663 5.641767921211726 1.8239652851420005 0.9370349737292286 6.282657678861387 4.20713292583352317/12/05 14:24:09 INFO SparkUI: Stopped Spark web UI at http://172.30.160.143:4040略.... 完结，希望能帮助你！]]></content>
      <categories>
        <category>Apache Spark</category>
      </categories>
      <tags>
        <tag>官方文档</tag>
        <tag>Apache Spark</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Spark 官方文档 编程指南]]></title>
    <url>%2Fp%2Fc3f7.html</url>
    <content type="text"><![CDATA[最近用Apache Spark 处理一些大数据，学了spark官方英文文档，顺便翻译了方便学习。 spark 版本 2.2.0. 翻译官方文档原地址：http://spark.apache.org/docs/latest/rdd-programming-guide.html 概述在较高层次上，每个Spark应用程序都包含一个驱动程序，该程序运行用户的main方法，并在集群上执行各种并行操作。 Spark提供的主要抽象是一个弹性分布式数据集（RDD），它是在集群节点间进行分区的元素集合，可以并行操作。 RDD是通过从Hadoop文件系统（或任何其他Hadoop支持的文件系统）中的文件或驱动程序中现有的Scala集合开始创建的，并对其进行转换。用户也可以要求Spark将RDD保存在内存中，以便在并行操作中有效地重用它。最后，RDD自动从节点故障中恢复。 Spark中的第二个抽象是可用于并行操作的共享变量。默认情况下，Spark在不同节点上并行执行一组任务时，会将该函数中使用的每个变量的副本传送给每个任务。有时候，变量需要在任务之间，或任务与驱动程序之间共享。 Spark支持两种类型的共享变量：广播变量，可用于在所有节点上缓存内存中的值，以及累加器，这些变量只是“添加”到的变量，如计数器和总和。 本指南显示了Spark支持的各种语言中的每个功能。如果您启动Spark的交互式shell（最好是Scala shell的bin / spark-shell或Python的bin / pyspark），最简单的方法就是跟随它。 本翻译文中的代码实现部分仅展示了java部分，scala和python自行官网。 导入 SparkSpark 2.2.0支持简洁地编写函数的lambda表达式，否则您可以使用org.apache.spark.api.java.function包中的类。 请注意，在Spark 2.2.0中删除了对Java 7的支持。 要用Java编写Spark应用程序，您需要在Spark上添加一个依赖项。 Spark可以通过Maven Central获得： 123groupId = org.apache.sparkartifactId = spark-core_2.11version = 2.2.0 另外，如果您想访问一个HDFS集群，您需要为您的HDFS版本添加对hadoop-client的依赖。 123groupId = org.apache.hadoopartifactId = hadoop-clientversion = &lt;your-hdfs-version&gt; 最后您需要导入一些spark的类到您的程序： 123import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.SparkConf; 安装 sparkSpark程序必须做的第一件事是创建一个JavaSparkContext对象，它告诉Spark如何访问一个集群。 要创建一个SparkContext，首先需要构建一个包含有关应用程序信息的SparkConf对象。 12SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);JavaSparkContext sc = new JavaSparkContext(conf); appName参数是您的应用程序在集群UI上显示的名称。 master是Spark，Mesos或YARN群集URL，或者是以本地模式运行的特殊“本地”字符串。 实际上，在群集上运行时，您不希望在程序中硬编码master，而是使用spark-submit启动应用程序，并在那里接收它。 但是，对于本地测试和单元测试，您可以通过“本地”来运行进程中的Spark。 弹性分布式数据集（RDDs）Spark围绕弹性分布式数据集（RDD）的概念展开，RDD是可以并行操作的容错元素集合。 有两种方法可以创建RDD：并行化驱动程序中的现有集合，或 外部存储系统（如共享文件系统，HDFS，HBase或提供Hadoop InputFormat的任何数据源）中引用数据集。 并行集合并行化集合是通过在驱动程序中的现有集合上调用JavaSparkContext的并行化方法来创建的。 集合的元素被复制，以形成可以并行操作的分布式数据集。 例如，下面是如何创建一个包含数字1到5的并行化集合： 12List&lt;Integer&gt; data = Arrays.asList(1, 2, 3, 4, 5);JavaRDD&lt;Integer&gt; distData = sc.parallelize(data); 一旦创建，分布式数据集（distData）可以并行操作。 例如，我们可以调用distData.reduce（（a，b） - &gt; a + b）将列表中的元素相加。 我们稍后介绍分布式数据集上的操作。 并行收集的一个重要参数是要将数据集剪切成的分区数量。 Spark将为群集的每个分区运行一个任务。 通常情况下，您需要为群集中的每个CPU分配2-4个分区。 通常情况下，Spark会根据您的群集自动设置分区数量。 但是，也可以通过将其作为并行化的第二个参数（例如sc.parallelize（data，10））手动设置。 注意：代码中的一些地方使用术语切片（分区的同义词）来维持向后兼容性。 外部数据集Spark可以从Hadoop支持的任何存储源（包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3等）创建分布式数据集.Spark支持文本文件，SequenceFile和任何其他Hadoop InputFormat。 文本文件RDD可以使用SparkContext的textFile方法创建。 这个方法接受一个文件的URI（机器上的一个本地路径，或者一个hdfs：//，s3n：//等URI），并把它作为一个行集合来读取。 这是一个示例调用： 1JavaRDD&lt;String&gt; distFile = sc.textFile("data.txt"); 一旦创建，distFile可以通过数据集操作进行操作。 例如，我们可以使用map来缩小所有行的大小，并reduce操作，如下所示：distFile.map（s - &gt; s.length（））.reduce（（a，b） - &gt; a + b）。 使用Spark读取文件的一些注意事项： 如果在本地文件系统上使用路径，则该文件也必须可以在工作节点上的相同路径上访问。 将文件复制到所有工作人员或使用网络安装的共享文件系统。 Spark的所有基于文件的输入方法（包括textFile）都支持在目录，压缩文件和通配符上运行。 例如，您可以使用textFile（“/ my / directory”），textFile（“/ my / directory / *.txt”）和textFile（“/ my / directory / *.gz”）。 textFile方法还使用可选的第二个参数来控制文件的分区数量。 默认情况下，Spark为文件的每个块创建一个分区（HDFS中的块默认为128MB），但是您也可以通过传递更大的值来请求更多的分区。 请注意，您不能有比块更少的分区 除了文本文件外，Spark的Java API还支持其他几种数据格式： JavaSparkContext.wholeTextFiles允许您读取包含多个小文本文件的目录，并将它们中的每一个都作为（文件名，内容）对返回。这与textFile相反，它将在每个文件中每行返回一个记录。 对于SequenceFiles，使用SparkContext的sequenceFile [K，V]方法，其中K和V是文件中的键和值的类型。这些应该是Hadoop的Writable接口的子类，如IntWritable和Text。 对于其他Hadoop InputFormats，可以使用JavaSparkContext.hadoopRDD方法，该方法采用任意的JobConf和输入格式类，关键类和值类。将它们设置为您使用输入源进行Hadoop作业的方式相同。您还可以使用基于“新”MapReduce API（org.apache.hadoop.mapreduce）的InputSamples的JavaSparkContext.newAPIHadoopRDD。 JavaRDD.saveAsObjectFile和JavaSparkContext.objectFile支持以包含序列化Java对象的简单格式保存RDD。虽然这不像Avro这样的专业格式，但它提供了一种简单的方法来保存任何RDD。 RDD 操作RDD支持两种类型的操作：转换（transformations）从现有数据集创建新数据集 和 行动（actions）在数据集上运行计算后将值返回给驱动程序。例如，map是一个通过函数传递每个数据集元素的转换（transformations），并返回一个代表结果的新RDD。另一方面，reduce是一个行动（actions），它使用某个函数聚合RDD的所有元素，并将最终结果返回给驱动程序（尽管还有一个并行reduceByKey返回一个分布式数据集）。 Spark中的所有转换（transformations）都是懒惰的，因为它们不会马上计算结果。相反，他们只记得应用于某些基础数据集（例如文件）的转换。只有在行动（actions）需要将结果返回给驱动程序时才会计算转换。这种设计使Spark能够更高效地运行。例如，我们可以认识到通过map创建的数据集将被用于reduce，并且只将reduce的结果返回给驱动程序，而不是返回更大的映射数据集。 默认情况下，每次对其执行行动（actions）时，每个已转换的RDD都可能重新计算。但是，您也可以使用持久化（或缓存）方法将RDD保留在内存中，在这种情况下，Spark将保留群集中的元素，以便在下次查询时快速访问。还支持在磁盘上持久化RDD，或在多个节点上复制RDD。 基本为了说明RDD基础知识，请考虑下面的简单程序： 123JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(s -&gt; s.length());int totalLength = lineLengths.reduce((a, b) -&gt; a + b); 第一行定义了来自外部文件的基本RDD。 这个数据集不会被加载到内存中，或者作用于其他行上：lines只是一个指向文件的指针。 第二行将lineLengths定义为map 转换（transformations）的结果。 同样，lineLengths由于懒惰而没有立即计算。 最后，我们运行reduce，这是一个行动(actions)。 在这一点上，Spark将计算分解为在不同机器上运行的任务，每台机器既运行其map部分又运行reduce，之后返回驱动程序的答案。 如果我们还想稍后再使用lineLengths，我们可以添加： 1lineLengths.persist(StorageLevel.MEMORY_ONLY()); reduce之前，这将导致lineLengths被保存在第一次计算后的内存。 将函数传递给SparkSpark的API在很大程度上依赖于将驱动程序中的函数传递到集群上运行。 在Java中，函数由实现org.apache.spark.api.java.function包中的接口的类来表示。 有两种方法来创建这样的功能： 在您自己的类中实现函数接口，或者作为一个匿名的内部类或者一个命名接口，并且将它的一个实例传递给Spark。 使用lambda表达式来简洁地定义一个实现。 虽然本指南的大部分内容都使用lambda语法进行简洁说明，但很容易以长格式使用所有相同的API。 例如，我们可以写上面的代码如下： 1234567JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(new Function&lt;String, Integer&gt;() &#123; public Integer call(String s) &#123; return s.length(); &#125;&#125;);int totalLength = lineLengths.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123; public Integer call(Integer a, Integer b) &#123; return a + b; &#125;&#125;); 或者 12345678910class GetLength implements Function&lt;String, Integer&gt; &#123; public Integer call(String s) &#123; return s.length(); &#125;&#125;class Sum implements Function2&lt;Integer, Integer, Integer&gt; &#123; public Integer call(Integer a, Integer b) &#123; return a + b; &#125;&#125;JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(new GetLength());int totalLength = lineLengths.reduce(new Sum()); 了解关闭Spark的难点之一是在集群中执行代码时理解变量和方法的范围和生命周期。 修改范围之外的变量的RDD操作可能是混淆的常见来源。 在下面的例子中，我们将看看使用foreach（）来增加计数器的代码，但其他操作也会出现类似的问题。 示例 考虑下面的naive的RDD元素总和，根据执行是否发生在同一个JVM中，这可能会有不同的表现。 一个常见的例子就是在本地模式下运行Spark（–master = local [n]）与将Spark应用程序部署到集群（例如，通过spark-submit to YARN）： 1234567int counter = 0;JavaRDD&lt;Integer&gt; rdd = sc.parallelize(data);// Wrong: Don't do this!!rdd.foreach(x -&gt; counter += x);println("Counter value: " + counter); 本地或集群模式 上面的代码的行为(actions)是未定义的，并可能无法正常工作。为了执行作业，Spark将RDD操作的处理分解为任务，每个任务由执行者执行。在执行之前，Spark计算任务的关闭。闭包是执行者在RDD上执行计算（在本例中为foreach（）））时必须可见的那些变量和方法。这个封闭序列化并发送给每个执行者。 发送给每个执行程序的闭包中的变量现在是副本，因此，当在foreach函数中引用计数器时，它不再是驱动程序节点上的计数器。驱动程序节点的内存中还有一个计数器，但执行程序不再可见！执行者只能看到序列化闭包的副本。因此，计数器的最终值仍然是零，因为计数器上的所有操作都引用了序列化闭包内的值。 在本地模式下，在某些情况下，foreach函数实际上将在与驱动程序相同的JVM内执行，并将引用相同的原始计数器，并可能实际更新它。 为了确保在这种情况下明确的行为，应该使用累加器。 Spark中的累加器专门用于提供一种在集群中的工作节点之间执行拆分时安全地更新变量的机制。本指南的“累加器”部分更详细地讨论了这些内容。 一般来说，闭包 - 像循环或本地定义的方法这样的构造不应该被用来改变一些全局状态。 Spark并没有定义或保证对从封闭外引用的对象的突变行为。这样做的一些代码可能在本地模式下工作，但这是偶然的，这样的代码不会按预期在分布式模式下运行。如果需要全局聚合，请使用累加器。 打印RDD的元素 另一个常见的习惯是试图使用rdd.foreach（println）或rdd.map（println）打印RDD的元素。 在单台机器上，这将生成预期的输出并打印所有RDD的元素。 但是，在集群模式下，执行程序调用的stdout输出现在写入执行程序的stdout，而不是驱动程序的stdout，因此驱动程序上的stdout不会显示这些！ 要打印驱动程序中的所有元素，可以使用collect（）方法首先将RDD带到驱动程序节点：rdd.collect（）。foreach（println）。 但是，这可能会导致驱动程序内存不足，因为collect（）会将整个RDD提取到一台计算机; 如果您只需要打印RDD的一些元素，则更安全的方法是使用take（）：rdd.take（100）.foreach（println）。 使用键值对尽管大多数Spark操作在包含任何类型对象的RDD上工作，但是一些特殊操作仅在键 - 值对的RDD上可用。 最常见的是分布式的“随机”操作，如按键分组或聚合元素。 在Java中，键值对使用Scala标准库中的scala.Tuple2类来表示。 您可以简单地调用新的Tuple2（a，b）来创建一个元组，然后使用tuple._1（）和tuple._2（）来访问它的字段。 键值对的RDD由JavaPairRDD类表示。 您可以使用mapToPair和flatMapToPair等特殊版本的map操作从JavaRDDs构建JavaPairRDD。 JavaPairRDD将同时具有标准的RDD功能和特殊的键值。 例如，以下代码使用键值对上的reduceByKey操作来计算文本中每行文本的出现次数： 123JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaPairRDD&lt;String, Integer&gt; pairs = lines.mapToPair(s -&gt; new Tuple2(s, 1));JavaPairRDD&lt;String, Integer&gt; counts = pairs.reduceByKey((a, b) -&gt; a + b); 例如，我们也可以使用counts.sortByKey（）来按字母顺序对这些对进行排序，最后count.collect（）将它们作为一个对象数组返回给驱动程序。 注意：在使用自定义对象作为键值对操作中的键时，必须确保自定义equals（）方法附带有匹配的hashCode（）方法。 有关完整的详细信息，请参阅Object.hashCode（）文档中概述的合同。 转换（Transformations）下表列出了Spark支持的一些常见转换。 有关详细信息，请参阅RDD API文档（Scala，Java，Python，R）和RDD函数doc（Scala，Java）。 转换 含义 map(func) 通过函数func传递源的每个元素来形成一个新的分布式数据集。 filter(func) 通过选择func返回true的源的元素返回一个新的数据集。 flatMap(func) 类似于map，但是每个输入项可以映射到0个或更多个输出项（所以func应该返回一个Seq而不是单个项）。 mapPartitions(func) 与map类似，但是在RDD的每个分区（块）上分别运行，所以当在T型RDD上运行时，func必须是Iterator =&gt; Iterator 类型。 mapPartitionsWithIndex(func) 类似于mapPartitions，但也提供了一个表示分区索引的整数值的func，所以在T类型的RDD上运行时，func的类型必须是（Int，Iterator ）=&gt; Iterator 。 sample(withReplacement, fraction, seed) 使用给定的随机数发生器种子，对数据的一小部分进行采样，有或没有替换。 union(otherDataset) 返回包含源数据集中的元素和参数的联合的新数据集。 intersection(otherDataset) 返回一个新的RDD，其中包含源数据集中的元素和参数的交集。 distinct([numTasks])) 返回包含源数据集的不同元素的新数据集。 groupByKey([numTasks]) 当调用（K，V）对的数据集时，返回（K，Iterable ）对的数据集。 注意：如果您正在对每个键执行聚合（例如总和或平均），则使用reduceByKey或aggregateByKey将会产生更好的性能。 注：默认情况下，输出中的并行级别取决于父RDD的分区数量。 您可以传递一个可选的numTasks参数来设置不同数量的任务。 reduceByKey(func, [numTasks]) 当调用（K，V）对的数据集时，返回（K，V）对的数据集，其中每个键的值使用给定的reduce函数func进行聚合，函数func必须是（V，V）=&gt; V.和groupByKey一样，reduce任务的数量可以通过可选的第二个参数来配置。 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和中性的“零”值来汇总每个键的值。 允许与输入值类型不同的聚合值类型，同时避免不必要的分配。 就像在groupByKey中一样，reduce任务的数量可以通过可选的第二个参数来配置。 sortByKey([ascending], [numTasks]) 当调用K实现Ordered的（K，V）对的数据集时，按照布尔上升参数的指定，按照升序或降序返回按键排序的（K，V）对的数据集。 join(otherDataset, [numTasks]) 当（K，V）和（K，W）类型的数据集被调用时，返回每个键的所有元素对的（K，（V，W））对的数据集。 外连接通过leftOuterJoin，rightOuterJoin和fullOuterJoin来支持。 cogroup(otherDataset, [numTasks]) 当（K，V）和（K，W）类型的数据集被调用时，返回（K，（Iterable ，Iterable ））元组的数据集。 这个操作也被称为groupWith。 cartesian(otherDataset) 当调用类型T和U的数据集时，返回（T，U）对（所有元素对）的数据集。 pipe(command, [envVars]) 通过shell命令管理RDD的每个分区，例如， 一个Perl或bash脚本。 RDD元素被写入进程的stdin，输出到stdout的行被作为字符串的RDD返回。 coalesce(numPartitions) 减少RDD中的分区数量为numPartitions。 用于在过滤大型数据集后更高效地运行操作。 repartition(numPartitions) 随机调整RDD中的数据以创建更多或更少的分区并在其间进行平衡。 这总是通过网络混洗所有数据。 repartitionAndSortWithinPartitions(partitioner) 根据给定的分区程序对RDD进行重新分区，并在每个结果分区中按键分类记录。 这比调用重新分区，然后在每个分区内进行排序更有效率，因为它可以将排序压入洗牌机器。 行动（actions）下表列出了Spark支持的一些常用行动（actions）。 有关详细信息，请参阅RDD API文档（Scala，Java，Python，R）和RDD函数doc（Scala，Java）。| 操作 | 含义 ||———|——-|| reduce(func) | 使用函数func（它接受两个参数并返回一个）聚合数据集的元素。 该函数应该是可交换和关联的，以便它可以被正确地并行计算。|| collect() | 在驱动程序中将数据集的所有元素作为数组返回。 在过滤器或其他操作返回足够小的数据子集之后，这通常很有用。 || count() | 返回数据集中元素的数量。 || first() | 返回数据集的第一个元素（类似于take（1））。 || take(n) | 用数据集的前n个元素返回一个数组。 || takeSample(withReplacement, num, [seed]) | 返回一个数组的随机样本数组，有或没有替换，可以预先指定一个随机数发生器种子。 || takeOrdered(n, [ordering]) | 使用自然顺序或自定义比较器返回RDD的前n个元素。 || saveAsTextFile(path) | 将数据集的元素作为文本文件（或文本文件集）写入本地文件系统，HDFS或任何其他Hadoop支持的文件系统的给定目录中。 Spark将在每个元素上调用toString将其转换为文件中的一行文本。 || saveAsSequenceFile(path) (Java and Scala) | 将数据集的元素作为Hadoop SequenceFile写入本地文件系统，HDFS或任何其他Hadoop支持的文件系统的给定路径中。 这在实现Hadoop的Writable接口的键值对的RDD上是可用的。 在Scala中，它也可用于可隐式转换为Writable的类型（Spark包含Int，Double，String等基本类型的转换）。 || foreach(func) | 在数据集的每个元素上运行函数func。 这通常用于副作用，如更新累加器或与外部存储系统交互。 注意：修改foreach（）之外的累加器以外的变量可能会导致未定义的行为。 请参阅了解更多细节。 || countByKey() | 仅适用于类型（K，V）的RDD。 返回（K，Int）对的hashmap和每个键的计数。 || saveAsObjectFile(path) (Java and Scala) | 使用Java序列化以简单的格式写入数据集的元素，然后可以使用SparkContext.objectFile（）加载。 | Spark RDD API还公开了一些动作的异步版本，例如foreach的foreachAsync，它立即将FutureAction返回给调用者，而不是在完成动作时阻塞。 这可以用来管理或等待操作的异步执行。 洗牌操作(shuffle operations)Spark中的某些操作会触发一个称为shuffle的事件。 洗牌(shuffle)是Spark重新分配数据的机制，以便在不同分区之间进行分组。 这通常涉及在执行者和机器之间复制数据，使得洗牌成为复杂而昂贵的操作。 背景 为了理解shuffle过程中发生了什么，我们可以考虑reduceByKey操作的例子。 reduceByKey操作将生成一个新的RDD，其中单个键的所有值都组合为一个元组 - 键和对与该键相关的所有值执行reduce函数的结果。面临的挑战是，并不是所有的单个密钥的值都必须位于同一个分区，甚至是同一个机器上，但是它们必须位于同一地点才能计算出结果。 在Spark中，数据通常不是跨分区分布，而是在特定操作的必要位置。在计算过程中，单个任务将在单个分区上运行 - 因此，要组织单个reduceByKey reduce任务执行的所有数据，Spark需要执行全部操作。它必须从所有分区中读取所有键的值，然后将各个分区上的值汇总在一起，以计算每个键的最终结果 - 这就是所谓的混洗（shuffle）。 虽然新洗牌数据的每个分区中的元素集合是确定性的，分区本身的排序也是确定性的，但是这些元素的排序并不是这样。如果一个人在随机播放之后需要可预测的有序数据，那么可以使用： mapPartitions使用例如.sorted repartitionAndSortWithinPartitions对每个分区进行排序，从而有效地对分区进行排序 同时对sortBy进行重新分区以制作全局排序的RDD 可能导致混洗的操作包括重新分配操作（如重新分区和合并），“像groupByKey和reduceByKey一样的ByKey操作（除计数），以及像cogroup和join一样的连接操作。 性能影响 Shuffle是一个昂贵的操作，因为它牵涉到磁盘I / O，数据序列化和网络I / O。为了组织数据，Spark生成一组任务 - 映射任务来组织数据，以及一组reduce任务来聚合它。这个术语来自MapReduce，并不直接与Spark的map和reduce操作有关。 在内部，来自个别map任务的结果被保存在内存中，直到它们不适合为止。然后，这些将根据目标分区进行排序并写入单个文件。在reduce方面，任务读取相关的排序块。 某些随机操作会消耗大量的堆内存，因为它们使用内存中的数据结构在传输之前或之后组织记录。具体来说，reduceByKey和aggregateByKey在地图上创建这些结构，ByKey操作在reduce方面生成这些结构。当数据不适合存储在内存中时，Spark会将这些表泄露到磁盘，导致额外的磁盘I / O开销和增加的垃圾回收。 Shuffle也会在磁盘上生成大量的中间文件。从Spark 1.3开始，这些文件将被保留，直到相应的RDD不再使用并被垃圾回收。这样做是为了在重新计算谱系时不需要重新创建洗牌文件。如果应用程序保留对这些RDD的引用，或者GC不经常引入，垃圾收集可能会在很长一段时间后才会发生。这意味着长时间运行的Spark作业可能会消耗大量的磁盘空间。在配置Spark上下文时，临时存储目录由spark.local.dir配置参数指定。 随机行为可以通过调整各种配置参数来调整。请参阅“Spark配置指南”中的“Shuffle Behavior”部分。 RDD 持久化Spark中最重要的功能之一就是在内存中持续（或缓存）一个数据集。当持久化RDD时，每个节点存储它在内存中计算的所有分区，并在该数据集的其他操作（或从中派生的数据集）中重用它们。这使未来的行动（action）更快（通常超过10倍）。缓存是迭代算法和快速交互式使用的关键工具。 您可以使用persist（）或cache（）方法将RDD标记为持久化。第一次在动作(action)中计算时，它将被保存在节点的内存中。 Spark的缓存是容错的 - 如果RDD的任何分区丢失，它将自动使用最初创建它的转换重新计算。 另外，每个持久RDD可以使用不同的存储级别进行存储，例如，允许您将数据集保存在磁盘上，将其保存在内存中，但是作为序列化的Java对象（以节省空间）将其复制到节点上。这些级别通过传递一个StorageLevel对象（Scala，Java，Python）来持久化（）来设置。 cache（）方法是使用默认存储级别的简写，即StorageLevel.MEMORY_ONLY（将反序列化的对象存储在内存中）。全套存储级别是：| 存储级别 | 含义 ||———|——-|| MEMORY_ONLY | 将RDD作为反序列化的Java对象存储在JVM中。 如果RDD不适合内存，某些分区将不会被缓存，并且每次需要时都会重新进行计算。 这是默认级别。 || MEMORY_AND_DISK | 将RDD作为反序列化的Java对象存储在JVM中。 如果RDD不适合内存，请存储不适合磁盘的分区，并在需要时从中读取。 || MEMORY_ONLY_SER (Java and Scala) | 将RDD存储为序列化的Java对象（每个分区一个字节的数组）。 这通常比反序列化的对象更节省空间，特别是在使用快速序列化器的情况下，但需要消耗更多的CPU资源。 || MEMORY_AND_DISK_SER (Java and Scala) | 与MEMORY_ONLY_SER类似，但是将不适合内存的分区溢出到磁盘上，而不是在每次需要时重新计算它们。 || DISK_ONLY | 将RDD分区仅存储在磁盘上。 || MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. | 与上面的级别相同，但复制两个群集节点上的每个分区。 || OFF_HEAP (experimental) | 与MEMORY_ONLY_SER类似，但将数据存储在堆内存中。 这需要启用堆堆内存。 | 注意：在Python中，存储对象将始终与Pickle库序列化，所以选择序列化级别无关紧要。 Python中的可用存储级别包括MEMORY_ONLY，MEMORY_ONLY_2，MEMORY_AND_DISK，MEMORY_AND_DISK_2，DISK_ONLY和DISK_ONLY_2。 Spark也会在shuffle操作（例如reduceByKey）中自动保留一些中间数据，即使没有用户调用persist。 这样做是为了避免在洗牌过程中节点失败时重新输入整个输入。 我们仍然建议用户如果打算重复使用RDD，则调用坚持的RDD。 该选择哪个存储级别？ Spark的存储级别旨在提供内存使用和CPU效率之间的不同折衷。我们建议通过以下过程来选择一个： 如果您的RDD适合默认的存储级别（MEMORY_ONLY），请将其留在原来的位置。这是CPU效率最高的选项，允许RDD上的操作尽可能快地运行。 如果没有，请尝试使用MEMORY_ONLY_SER并选择一个快速序列化库，以使对象更加节省空间，但是访问速度仍然相当快。 （Java和Scala） 除非计算您的数据集的函数是昂贵的，否则它们不会溢出到磁盘上，或者它们会过滤大量的数据。否则，重新计算分区可能与从磁盘读取分区一样快。 如果要快速恢复故障（例如，如果使用Spark来为Web应用程序提供请求），请使用复制的存储级别。所有的存储级别通过重新计算丢失的数据来提供完全的容错能力，但是复制的容量可以让您继续在RDD上运行任务，而无需等待重新计算丢失的分区。 删除数据 Spark会自动监视每个节点上的高速缓存使用情况，并以最近最少使用（LRU）方式删除旧的数据分区。 如果您想要手动删除RDD而不是等待其从缓存中删除，请使用RDD.unpersist（）方法。 共享变量通常，在远程集群节点上执行传递给Spark操作（如map或reduce）的函数时，它将在函数中使用的所有变量的单独副本上运行。 这些变量被复制到每台机器上，远程机器上的变量没有更新传播到驱动程序。 支持通用的，可读写的共享变量将是低效的。 但是，Spark为两种常见的使用模式提供了两种有限类型的共享变量：广播变量和累加器。 广播变量广播变量允许程序员在每台机器上保存一个只读变量，而不是用任务发送一个只读变量的副本。例如，可以使用它们以有效的方式为每个节点提供大型输入数据集的副本。 Spark还试图使用高效的广播算法来分发广播变量，以降低通信成本。 Spark动作（action）是通过一系列阶段执行的，由分散的“随机”操作分开。 Spark会自动播放每个阶段中任务所需的通用数据。以这种方式广播的数据以序列化形式缓存，并在运行每个任务之前反序列化。这意味着只有跨多个阶段的任务需要相同的数据或以反序列化的形式缓存数据时，显式创建广播变量才是有用的。 广播变量是通过调用SparkContext.broadcast（v）从变量v创建的。广播变量是v的一个包装，它的值可以通过调用value方法来访问。下面的代码显示了这一点： 1234Broadcast&lt;int[]&gt; broadcastVar = sc.broadcast(new int[] &#123;1, 2, 3&#125;);broadcastVar.value();// returns [1, 2, 3] 在创建广播变量之后，应该在群集上运行的任何函数中使用值而不是值v，以便v不会多次传送到节点。 另外，对象v在广播之后不应被修改，以确保所有节点获得广播变量的相同值（例如，如果变量稍后被运送到新节点）。 累加器累加器是只能通过关联和交换操作“添加”的变量，因此可以并行有效地支持。 它们可以用来实现计数器（如在MapReduce中）或者和。 Spark本身支持数字类型的累加器，程序员可以添加对新类型的支持。 作为用户，您可以创建名称或未命名的累加器。 如下图所示，一个命名的累加器（在这种情况下计数器）将显示在修改该累加器的阶段的Web UI中。 Spark显示由“任务”表中的任务修改的每个累加器的值。 跟踪用户界面中的累加器对于理解运行阶段的进度非常有用（注意：Python尚不支持）。 可以通过调用SparkContext.longAccumulator（）或SparkContext.doubleAccumulator（）来分别累积Long或Double类型的值来创建数字累加器。 在群集上运行的任务可以使用add方法添加到它。 但是，他们无法读懂它的价值。 只有驱动程序可以使用其值方法读取累加器的值。 下面的代码显示了一个累加器被用来加总一个数组的元素： 12345678LongAccumulator accum = jsc.sc().longAccumulator();sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -&gt; accum.add(x));// ...// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 saccum.value();// returns 10 虽然这段代码使用了对Long类型的累加器的内置支持，程序员也可以通过继承AccumulatorV2来创建它们自己的类型。 AccumulatorV2抽象类有几个方法必须重写：复位重置累加器为零，添加用于向累加器中添加另一个值，合并合并另一个相同类型的累加器到这个累加器中。 其他必须被覆盖的方法包含在API文档中。 例如，假设我们有一个表示数学向量的MyVector类，我们可以这样写： 123456789101112131415161718class VectorAccumulatorV2 implements AccumulatorV2&lt;MyVector, MyVector&gt; &#123; private MyVector myVector = MyVector.createZeroVector(); public void reset() &#123; myVector.reset(); &#125; public void add(MyVector v) &#123; myVector.add(v); &#125; ...&#125;// Then, create an Accumulator of this type:VectorAccumulatorV2 myVectorAcc = new VectorAccumulatorV2();// Then, register it into spark context:jsc.sc().register(myVectorAcc, "MyVectorAcc1"); 请注意，当程序员定义自己的AccumulatorV2类型时，结果类型可能与添加元素的类型不同。 对于仅在动作内执行的累加器更新，Spark保证每个任务对累加器的更新只会应用一次，即重新启动的任务不会更新该值。 在转换中，用户应该意识到如果任务或作业阶段被重新执行，每个任务的更新可能会被应用多次。 累加器不会改变Spark的懒惰评估模型。 如果它们在RDD上的操作中被更新，则其值仅在RDD作为动作的一部分计算之后才被更新。 因此，在像map（）这样的惰性转换中进行累加器更新并不能保证执行。 下面的代码片段演示了这个属性： 123LongAccumulator accum = jsc.sc().longAccumulator();data.map(x -&gt; &#123; accum.add(x); return f(x); &#125;);// Here, accum is still 0 because no actions have caused the `map` to be computed. 部署到群集应用程序提交指南介绍了如何将应用程序提交到集群。 简而言之，一旦将应用程序打包成JAR（用于Java / Scala）或一组.py或.zip文件（用于Python），bin / spark-submit脚本就可以将其提交给任何受支持的集群管理器。 从Java / Scala启动Spark作业org.apache.spark.launcher包提供了使用简单的Java API作为子进程启动Spark作业的类。 单元测试Spark对任何流行的单元测试框架的单元测试都很友好。 只需在主URL设置为本地的情况下在测试中创建一个SparkContext，运行您的操作，然后调用SparkContext.stop（）将其拆除。 确保停止finally块或测试框架的tearDown方法中的上下文，因为Spark不支持在同一个程序中同时运行的两个上下文。 下一步该怎么办您可以在Spark网站上看到一些Spark程序示例。 另外，Spark在示例目录（Scala，Java，Python，R）中包含了几个示例。 您可以通过将类名传递给Spark的bin / run-example脚本来运行Java和Scala示例; 例如： 1./bin/run-example SparkPi 对于Python示例，请使用spark-submit代替： 1./bin/spark-submit examples / src / main / python / pi.py 对于R示例，请使用spark-submit代替： 1./ bin / spark-submit examples / src / main / r / dataframe.R 有关优化程序的帮助，配置和调优指南提供有关最佳做法的信息。 它们对于确保您的数据以有效的格式存储在内存中尤其重要。 有关部署的帮助，群集模式概述描述了分布式操作中涉及的组件以及支持的群集管理器。 最后，完整的API文档可以在Scala，Java，Python和R. 完结，希望能帮助您！]]></content>
      <categories>
        <category>Apache Spark</category>
      </categories>
      <tags>
        <tag>官方文档</tag>
        <tag>Apache Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[thrift 服务端返回null]]></title>
    <url>%2Fp%2Fa947.html</url>
    <content type="text"><![CDATA[强大、牛X的RPC框架 - thrift，就没怂过！ 问题在用thrift做RPC框架存在服务端有时需要返回null 的情况，这时客户端就报 org.apache.thrift.TApplicationException: detectEvent failed: unknown result 异常。 原因点击去，查看service类时发现thrift 服务端返回null时，会抛出异常，也就是不接受null的情况。 123456789public java.util.Set&lt;Bean&gt; recv_detectWord() throws org.apache.thrift.TException &#123; detectWord_result result = new detectWord_result(); receiveBase(result, "detectWord"); if (result.isSetSuccess()) &#123; return result.success; &#125; throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "detectWord failed: unknown result"); &#125; 但是这个异常出现有两个可能的原因是： 服务器抛出一个（未捕获）的异常。 你试图返回一个空结果，这是Thrift非法的。 解决问题找到后，就简单了。客户端代码抓取TApplicationException异常，再处理你的事即可。 12345678910try &#123; Bean response = client.detectEvent(titleSplit, contentSplit, domains); if (response != null) &#123; System.out.println(response.toString()); &#125; &#125; catch (org.apache.thrift.TApplicationException e) &#123; System.out.println("response is null"); &#125; 完结，希望能帮助你！]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>Apache Thrift</tag>
        <tag>官方文档</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jar包详情]]></title>
    <url>%2Fp%2F314b.html</url>
    <content type="text"><![CDATA[java jar包 常用来运行java程序的压缩包。今天就来看看内幕。 简介维基百科 在软件领域，JAR文件（Java归档，英语：Java ARchive）是一种软件包文件格式，通常用于聚合大量的Java类文件、相关的元数据和资源（文本、图片等）文件到一个文件，以便分发Java平台应用软件或库。JAR文件是一种归档文件，以ZIP格式构建，以.jar为文件扩展名。用户可以使用JDK自带的jar命令创建或提取JAR文件。也可以使用其他zip压缩工具，不过压缩时zip文件头里的条目顺序很重要，因为Manifest资源配置文件常需放在首位。JAR文件内的文件名是Unicode文本。 JAR 文件可以用于： 用于发布和使用类库 作为应用程序和扩展的构建单元 作为组件、applet或者插件程序的部署单位 用于打包与组件有关联的辅助资源 JAR 文件格式的优势和功能： 安全性：可以对 JAR 文件内容加上数字化签名。这样，能够识别签名的工具就可以有选择地为您授予软件安全特权，这是其他文件做不到的，它还可以检测代码是否被篡改过。 减少下载时间：如果一个 applet 捆绑到一个 JAR 文件中，那么浏览器就可以在一个 HTTP 事务中下载这个 applet 的类文件和相关的资源，而不是对每一个文件打开一个新连接。 压缩：JAR 格式允许您压缩文件以提高存储效率。 传输平台扩展：Java 扩展框架 (Java Extensions Framework) 提供了向 Java 核心平台添加功能的方法，这些扩展是用 JAR 文件打包的 (Java 3D 和 JavaMail 就是由 Sun 开发的扩展例子 )。 包密封：存储在 JAR 文件中的包可以选择进行 密封，以增强版本一致性和安全性。密封一个包意味着包中的所有类都必须在同一 JAR 文件中找到。 包版本控制：一个 JAR 文件可以包含有关它所包含的文件的数据，如厂商和版本信息。 可移植性：处理 JAR 文件的机制是 Java 平台核心 API 的标准部分。 JAR包结构解压JAR包之后除了编译后.class文件以外，还有META-INF 文件夹。该文件夹下面包含了MANFEST.MF文件。 MANIFET.MF 属性一般属性： Manifest-Version ： 用来定义Manifest文件的版本。 Created-By ：该文件的生成者，由jar命令行工具生成。 Signature-Version ： jar文件签名版本。 Class-Path ： 应用程序或者类装载器使用该值来构建内部的类搜索路径。 应用程序相关： Main-Class ： jar文件的入口类，该文件必须是一个可执行的类，包含main方法。 小程序（Applet）相关属性：现在还有用applet的情况吗？没有了吧！？用不到，不记了。 扩展标识属性： Extension-Name ： 该文件定义了jar文件的标识。 包扩展属性： Implementation-Title ：扩展实现的标题 Implementation-Version ：扩展实现的版本 Implementation-Vendor ：扩展实现的组织 Implementation-Vendor-Id ：扩展实现的组织的标识 Implementation-URL : 该扩展包的下载地址(URL) Specification-Title ：扩展规范的标题 Specification-Version ：扩展规范的版本 Specification-Vendor ：声明了维护该规范的组织 Sealed ：jar文件是否封存，值可以是true或者false 签名相关属性：一般eclipse导出的可执行jar包默认不包含签名属性。 JAR包命令jar命令格式：jar {c t x u f}[v m e 0 m i][-C 目录]文件名 其中{ctvu}这四个参数必须选其一，[v m e 0 m i] 是可选参数，文件名也必须。 -c 创建一个jar包-t 显示jar中的内容列表-x 解压jar包-u 添加文件到jar包中-f 指定jar包的文件名 -v 生成详细的报造，并输出至标准设备-m 指定manifest.mf文件.(manifest.mf文件中可以对jar包及其中的内容作一些一设置)-0 产生jar包时不对其中的内容进行压缩处理-M 不产生所有文件的清单文件(Manifest.mf)。这个参数与忽略掉-m参数的设置-i 为指定的jar文件创建索引文件-C 表示转到相应的目录下执行jar命令,相当于cd到那个目录，然后不带-C执行jar命令 可执行JAR文件一个可执行Java程序以及其使用的库文件可以打包在一个JAR文件中。 可执行的JAR文件中的Manifest文件用代码Main-Class: myPrograms.MyClass指定了入口点类，注意要指明该类的路径（-cp参数将被忽略）。有些操作系统可以在点击后直接运行可执行JAR文件。而更典型的调用则是通过命令行执行“java -jar foo.jar”。 在多数平台上可以使用封装器封装可执行JAR文件。例如，对于更喜欢使用Windows EXE的Microsoft Windows用户而言，可以使用工具（如JSmooth、Launch4J、WinRun4J、Nullsoft脚本安装系统等），将单个JAR文件转换为可执行文件。 JAR包使用案例创建jar包 jar cf hello.jar hello 创建并显示打包过程 jar cvf hello.jar hello 利用hello目录创建hello.jar包,并显示创建过程 显示jar包 jar tvf hello.jar 查看hello.jar包的内容 解压jar包 jar xvf hello.jar 解压hello.jar至当前目录 jar中添加文件 jar uf hello.jar HelloWorld.java 将HelloWorld.java添加到hello.jar包中 创建不压缩内容jar包 jar cvf0 hello.jar *.class 利用当前目录中所有的.class文件生成一个不压缩jar包 创建带manifest.mf文件的jar包 jar cvfm hello.jar manifest.mf hello创建的jar包多了一个META-INF目录,META-INF止录下多了一个manifest.mf文件,至于manifest.mf的作用,后面会提到. 忽略manifest.mf文件 jar cvfm hello.jar mymanifest.mf -C hello/表示在切换到hello目录下然后再执行jar命令 加-C应用 jar cvfm hello.jar mymanifest.mf -C hello/表示在切换到hello目录下然后再执行jar命令 -i为jar文件生成索引列表 当一个jar包中的内容很好的时候，你可以给它生成一个索引文件，这样看起来很省事。jar i hello.jar 执行完这条命令后，它会在hello.jar包的META-INF文件夹下生成一个名为INDEX.LIST的索引文件，它会生成一个列表，最上边为jar包名。 tar包 和 jar包 和 war包 的区别？某乎上的回答https://www.zhihu.com/question/22129866/answer/101446363。 tar：tar是*nix下的打包工具，生成的包通常也用tar作为扩展名，其实tar只是负责打包，不一定有压缩，事实上可以压缩，也可以不压缩，通常你看到xxxx.tar.gz，就表示这个tar包是压缩的，并且使用的压缩算法是GNU ZIP，而xxxx.tar.bz2就表示这个包使用了bzip2算法进行压缩，当然这样的命名只是一种惯例，并非强制。简单地说，tar就仅是打包。 jar：即Java Archive，Java的包，Java编译好之后生成class文件，但如果直接发布这些class文件的话会很不方便，所以就把许多的class文件打包成一个jar，jar中除了class文件还可以包括一些资源和配置文件，通常一个jar包就是一个java程序或者一个java库。 war：Web application Archive，与jar基本相同，但它通常表示这是一个Java的Web应用程序的包，tomcat这种Servlet容器会认出war包并自动部署。 完结，对你有用！]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jar包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于SimpleDateFormat 线程不安全]]></title>
    <url>%2Fp%2Ff2d.html</url>
    <content type="text"><![CDATA[问题java 程序都常用SimpleDateFormat 和 DateFormat 工具类，用来字符串和时间对象的相互转换，多线程环境下共享使用SimpleDateFormat 时，format() 和 parese() 方法会出下面出错。实际开发中遇到的错误： 1234567891011121314151617181920java.lang.NumberFormatException: For input string: "" at java.lang.NumberFormatException.forInputString(Unknown Source) at java.lang.Long.parseLong(Unknown Source) at java.lang.Long.parseLong(Unknown Source) at java.text.DigitList.getLong(Unknown Source) at java.text.DecimalFormat.parse(Unknown Source) at java.text.SimpleDateFormat.subParse(Unknown Source) at java.text.SimpleDateFormat.parse(Unknown Source) at java.text.DateFormat.parse(Unknown Source) ... at java.lang.Thread.run(Unknown Source)java.lang.NumberFormatException: multiple points at sun.misc.FloatingDecimal.readJavaFormatString(Unknown Source) at sun.misc.FloatingDecimal.parseDouble(Unknown Source) at java.lang.Double.parseDouble(Unknown Source) at java.text.DigitList.getDouble(Unknown Source) at java.text.DecimalFormat.parse(Unknown Source) at java.text.SimpleDateFormat.subParse(Unknown Source) at java.text.SimpleDateFormat.parse(Unknown Source) at java.text.DateFormat.parse(Unknown Source) 原因主要原因是 SimpleDateFormat 和 DateFormat 是线程不安全导致的。 解决解决方法有很多，但是效率、简洁、美观上我推荐最后一个方法。对每个方法使用SimpleDateFormat对象时新建对象，使用局部变量解决问题，但是创建对象空间开销较大，不推荐。还有就是使用同步代码块synchronized，把 parse() 和 format() 分别同步起来，这个可以采用，但是总觉得想到同步就会想到慢，所以我没采纳。 使用ThreadLocal, 也是将共享变量变为独享，线程独享肯定能比方法独享在并发环境中能减少不少创建对象的开销。如果对性能要求比较高的情况下，一般推荐使用这种方法。 使用线程局部变量 ThreadLocal 来解决，以下是我采用的解决方案： 123456789101112131415161718192021222324/*** 很重要* * SimpleDateFormat 和 DateFormat 不是线程安全，需要加锁，或者设置为局部线程安全*/private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() &#123;@Overrideprotected DateFormat initialValue() &#123; return new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); &#125;&#125;;public static Date parse(String dateStr) &#123; try &#123; return threadLocal.get().parse(dateStr); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; return null;&#125;public static String format(Date date) &#123; return threadLocal.get().format(date);&#125; 完结，希望能帮助你！]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Thrift 官方 java教程]]></title>
    <url>%2Fp%2F413a.html</url>
    <content type="text"><![CDATA[Apache Thrift 维基百科Thrift是一种接口描述语言和二进制通讯协议，它被用来定义和创建跨语言的服务。它被当作一个远程过程调用（RPC）框架来使用，是由Facebook为“大规模跨语言服务开发”而开发的。 最近用 thrift 做微服务 ， 抽空写了通俗易懂 apache thrift 官方java 教程 中文版。 准备 添加依赖 官网有复杂的安装thrift和编译的过程，复杂多余，最终目的还是要转成jar包，添加maven依赖。不过现在已经有maven依赖的，省去了编译打包jar包过程。一个maven依赖就可以加载进来所有需要的jar包。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt; &lt;artifactId&gt;libthrift&lt;/artifactId&gt; &lt;version&gt;0.10.0&lt;/version&gt;&lt;/dependency&gt; 接口语言 转 java 工具 除了maven依赖，还需要一个从接口描述语言（thrift）转换成 java 的工具，从这里下载.exe文件 http://thrift.apache.org/download 。 下载thrift-0.10.0.exe文件，把他移到自己喜欢的目录下D:\code\thrift,并改名 thrift.exe,并且把路径添加到环境变量path。 WIN+R , cmd进去输入命令thrift会打印一下则，环境添加成功。 编写接口从 http://thrift.apache.org/tutorial/java ， 下载 tutorial.thrift 和 shared.thrift ，控制台 cd 到它俩所在的目录 1thrift -r --gen java tutorial.thrift 打开目录会多出来以下文件： 1234567891011gen-java ├─shared │ SharedService.java │ SharedStruct.java │ └─tutorial Calculator.java InvalidOperation.java Operation.java tutorialConstants.java Work.java 把 shared 和 tutorial 俩文件夹拷到 maven 项目里（我个人用eclipse建的maven，直接拷到了eclipse，src/main/java 下）。 特殊说明 —爬坑 这里可能会出现拷进去的文件的@Override注解报错问题，这是因为什么呢？ 原因是我用eclipse建的maven 的jre system libraries 默认使用了 jre1.5 。 这是jdk的问题，@Override是JDK5就已经有了，但是不支持对接口的实现，认为这不是Override而报错。JDK6修正了这个Bug，无论是对父类的方法覆盖还是对接口的实现都可以加上@Override。 解决问题：我把jre system libraries 1.5 删除，并且新添了 jre1.8 就可以了。 实现先贴出项目整个图： 这里有一个CalculatorHandler.java 文件， 也是可以从官网找到的 http://thrift.apache.org/ 。 其它client 和 server 都可以下载 http://thrift.apache.org/tutorial/java 。 官方教程中启动服务器提供了两种方式，一般方式(simple)和TSSL方式(secure)。 simple方式很简单就能运行，但是TSSL方式需要用到安全证书。 官方教程源码，服务端代码里有一段： 1params.setKeyStore("../../lib/java/test/.keystore", "thrift", null, null); 这里的.keystore是私钥，”thrift”是私钥的口令。 在客户端里有： 1params.setTrustStore("../../lib/java/test/.truststore", "thrift", "SunX509", "JKS"); 这里的.truststore是公钥，”SunX509”是公钥的口令，我们需要通过key-tool工具来生成私钥和公钥。 需要用 java keytool 工具来生成TSSL的私钥和公钥： 这里所有需要输入口令的地方我都输入了 thrift , 其实不合理，由于是测试使用，keytool详情自行“某度” 或 “某狗”。 输入下面的命令生成私钥： 1keytool -genkeypair -alias certificatekey -keyalg RSA -validity 365 -keystore .keystore 输入下面的命令生成server.cer证书： 1keytool -export -alias certificatekey -keystore .keystore -rfc -file server.cer 输入下面的命令生成.truststore： 1keytool -import -alias certificatekey -file server.cer -keystore .truststore 最后我生成.keystore和.truststore文件放到了项目根目录下，并把client 和 server 路径改了 12345// clinetparams.setTrustStore("./.truststore", "thrift", "SunX509", "JKS");//serverparams.setKeyStore("./.keystore", "thrift", null, null); 运行这个是client 控制台 ： 12345678910Received 1ping()Received 21+1=2Received 3Invalid operation: Cannot divide by 0Received 415-10=5Received 5Check log: 5 这个是server 控制台 : 1234567Starting the simple server...Starting the secure server...ping()add(1,1)calculate(1, &#123;DIVIDE,1,0&#125;)calculate(1, &#123;SUBTRACT,15,10&#125;)getStruct(1) 参考http://thrift.apache.org/tutorial/javahttp://thrift.apache.org/ 完结，希望能帮助你！]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>Apache Thrift</tag>
        <tag>官方文档</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计日志里某时间段内访问最的ip]]></title>
    <url>%2Fp%2F7dd5.html</url>
    <content type="text"><![CDATA[今天某时间段服务出现异常，需要统计某段时间内请求最多的ip，查看是否有人添加了服务，学习了运维大牛，并且在此记录分享。 命令： 12#按时间段统计sed -n &apos;/10:35:00/,/11:11:00/p&apos; access.log | awk &apos;&#123;print $1&#125;&apos;|sort|uniq -c|sort -n -r |head -n 20 日志格式： 110.90.11.10 - 5000 - [27/Nov/2017:17:45:55 +0800] awk ‘ { print $1} ‘ : 取日志每行第一列。也即是10.90.11.10。 sort : 对ip部分进行排序，如果ip后跟着端口（10.90.11.10:8080）,则sort 替换成 cut -d : -f 1 | sort 。 uniq -c : 打印每重复行出现的次数，并去掉重复。 sort -n -r ：按照重复行出现的次序倒序排序。 head -n 20 : 取前20位ip。 完结，希望能帮助你！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell 脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 内存模型]]></title>
    <url>%2Fp%2Fcd69.html</url>
    <content type="text"><![CDATA[学习程晓明的《深入理解Java内存模型》。 摘要文章内容有并发、内存模型、重排序、内存屏障、happens-before规则、as-if-serial语义、顺序一致性内存模型、volatile、锁、final。 并发并发需要处理的两个关键问题：线程之间通信和同步 通信：是指线程之间以何种机制来交换信息；在命令式编程中，通信机制有两种，共享内存和消息传递。 同步：是指程序用于控制不同线程之间操作发生相对顺序的机制。 Java的并发采用的是共享内存模型，线程之间的通信总是隐式的进行。 java 内存模型Java 线程之间的通信由Java 内存模型（简称JMM）控制。 主内存（main memory）：存储了线程之间共享变量；是指堆、方法区。 本地内存（local memory）：存储了该线程以读/写共享的副本；是一个抽象概念（并不真实存在），它涵盖了缓存、写缓冲区、寄存器以及其它的硬件和编译器优化。 重排序重排序分三种类型： 编译器优化的重排序：编译器在不改变单线程程序语义的前提下，可以重新安排语义的执行顺序。 指令级并行的重排序：现代处理器采用了指令级并行技术（ILP）来将多条指令重叠执行。如果不存在数据依赖，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序：由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 内存屏障指令内存屏障指令：为了保证内存可见性，java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。 内存屏障指令分四类： happens-before规则happens-before：在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这俩操作之间必须存在happens-before关系。 happens-before规则： 程序顺序规则：一个线程中的每一个操作，happens-before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器的解锁，happens-before 于任意后续对监视器的加锁。 volatile变量规则：对于一个volatile域的写，happens-before 于任意后续对这个volatile域的读。 传递性：如果A happens-before B, 且 B happens-before C ，那么A happens-before C。 注意，两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。 数据依赖性数据依赖性：如果两个操作访问同一变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。 数据依赖性分三类： 写后读：a=1,b=a; 写后写：a=1,a=2; 读后写：a=b,b=1; 编译器和处理器在重排序时，会遵守数据依赖性。 as-if-serial 语义as-if-serial语义：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守as-if-serial语义。 顺序一致性内存模型顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 volatilevolatile变量自身特性： 可进行：对于一个volatile变量的读，总能看到（任意线程）对于这个volatile变量的最后写入。 原子性（微弱）：对于任意的单个volatile变量的du/写具有原子性，但是类似volatile++这种复合操作（两个操作，读取再写入）不具有原子性的。（文章中也举例测试过） 禁止重排序：为了实现volatile内存语义，JMM会分别限制编译器重排序和处理器重排序。 volatile内存语义： volatile写：当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。 volatile读：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 下面是JMM针对编译器制定的volatile重排序规则表： 基于保守策略的JMM内存屏障插入策略： 在每个volatile写操作的前面插入一个StoreStore屏障 在每个volatile写操作的后面插入StoreLoad屏障 在每个volatile读操作的后面插入一个LoadLoad屏障 在每个volatile读操作的后面插入一个LoadStore屏障 volatile 和 synchronize对比 在功能上，监视器锁比volatile更强大；在可伸缩性和执行性能上，volatile更有优势。 volatile仅仅保证对单个volatile变量的读/写具有原子性；而synchronize锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性。 锁锁是java 并发编程中最重要的同步机制 锁的内存语义： 线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。 线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享变量所做修改的）消息。 线程A释放锁，随后线程B获取这个锁，这个过程实质上是线程A通过主内存向线程B发送消息。 ReetrantLock锁：它的实现依赖于java同步器AbstractQueueSynchronizer（简称AQS）,而AQS使用一个整型的volatile变量（命名为state）来维护同步状态。 ReetrantLock分为公平锁 和 非公平锁： 公平锁：是通过volatile实现同步的。公平锁在释放锁的最后写volatile变量state；在获取锁时首先读这个volatile变量。根据volatile的happens-before规则，释放锁的线程在写volatile变量之前可见的共享变量，在获取锁的线程读取同一个volatile变量后将立即变的对获取锁的线程可见。 非公平锁；通过CAS实现的，CAS是compareAndSet()方法的简称，意思是compare and swap。CAS实际上调用的JNI函数，也就是CAS依赖于本地方法实现。以Intel来说，对于CAS的JNI实现函数，它保证：（1）禁止该CAS之前和之后的读和写指令重排序。（2）把写缓冲区中的所有数据刷新到内存中。这两点具有内存屏障效果，实现了同volatile读和volatile写的内存语义一样的效果。 final对于final域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 final域是基础数据类型时的重排序规则： 写final域：JMM禁止编译器把final域的写重排序到构造函数之外。编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障禁止处理器把final域的写重排序到构造函数之外。 读final域：在一个线程中，初次读对象引用与初次读该对象包含的final域，JMM禁止处理器重排序这两个操作（注意，这个规则仅仅针对处理器）。编译器会在读final域操作的前面插入一个LoadLoad屏障。 final域是引用类型时的重排序规则： 写final域：构造函数内对一个final引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 为什么final引用不能从构造方法内”逸出“？ 前面我们提到过，写final域的重排序规则可以确保：在引用变量为任意线程可见之前，该引用变量指向的对象的final域已经在构造函数中被正确初始化过了。其实要得到这个效果，还需要一个保证：在构造函数内部，不能让这个被构造对象的引用为其他线程可见，也就是对象引用不能在构造函数中“逸出”。 内存模型总结JMM保证：如果程序是正确同步的，程序的执行将具有顺序一致性 。 JMM设计 从JMM设计者的角度来说，在设计JMM时，需要考虑两个关键因素： 程序员对内存模型的使用。程序员希望内存模型易于理解，易于编程。程序员希望基于一个强内存模型(程序尽可能的顺序执行)来编写代码。 编译器和处理器对内存模型的实现。编译器和处理器希望内存模型对它们的束缚越少越好，这样它们就可以做尽可能多的优化(对程序重排序，做尽可能多的并发)来提高性能。编译器和处理器希望实现一个弱内存模型。 JMM设计就需要在这两者之间作出协调。JMM对程序采取了不同的策略： 对于会改变程序执行结果的重排序，JMM要求编译器和处理器必须禁止这种重排序。 对于不会改变程序执行结果的重排序，JMM对编译器和处理器不作要求（JMM允许这种重排序）。 吐槽，64位的long/double型具有原子性，真的这样吗？！JMM不保证对64位的long型 和 double型变量读/写 操作具有原子性,是这样的吗？！ 对于32位机器：当JVM在32位处理器上运行时，会把一个64位long/double型变量的写操作分成两个32位的写操作来执行。这两个32位的写操作可能会被分配到不同的总线事务中执行，此时对于64位变量的写不具有原子性。 对于64位机器：（对于这个我查阅了资料）在一些32位机器上，如果要求对64位数据的写具有原子性，会有较大的开销，为了照顾这种机器，java语言规范鼓励但不强求JVM对64位long/double变量的写具有原子性。但是在查阅的过程中各有己见，我个人采纳了（https://www.zhihu.com/question/38816432 中 码农甲 的回答，也是最近回答2017-09-09），他的结论是在目前intel平台的x64 hotspot jvm中long/double型的访问是原子的(这正是我的环境)，他在回答里链接Value integrity guarantee for concurrent long writes in 64-bit OpenJDK 7/8 和 All Accesses Are Atomic都详细地讨论和解释了64位读写long/double是原子的。 参考文献http://www.infoq.com/cn/profile/%E7%A8%8B%E6%99%93%E6%98%8Ehttps://www.zhihu.com/question/38816432https://stackoverflow.com/questions/25173208/value-integrity-guarantee-for-concurrent-long-writes-in-64-bit-openjdk-7-8https://shipilev.net/blog/2014/all-accesses-are-atomic/http://www.cnblogs.com/skywang12345/p/3447546.html 完结。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK1.8 String源码解析]]></title>
    <url>%2Fp%2F447c.html</url>
    <content type="text"><![CDATA[JDK源码深揪，JDK1.8版本。 摘要经常用到的String类，不可变字符串，看看它是如何高效的处理字符串操作的。 == 和 equals()== ，判断的是对象的内存起始地址是否相同；equals ，判断字符串内容是否相同。 常量池 在class文件中存在一个常量池，里面主要放字面量和符号引用，就是String s=”123”作为字符串字面量就在里面；而方法区中有一个常量池 叫做 运行时常量池，方法去外有一个常量池叫做字符串常量池（与方法区平级），字符串常量池是全局共享，类似缓存区。当加载class文件时，class文件中的常量池中大部分进入运行时常量池，但是 new String（”123“），中的”123“，是”进入”字符串常量池，这个进入为什么要加引号呢，最上面已经说了，字符串本身是在堆中（和其他一般对象一样，刚出来的时候很大可能是在eden中），然后在字符串常量池中有指向它的引用。 String.intern()会把String放在运行时的常量池。 为什么要有 常量池 字符串的分配，和其它的对象分配一样，消费高昂的时间和空间代价，作为最基础的数据类型，大量的频繁的创建字符串，极大程度影响程序的性能。 JVM 为了提高性能和减少内存的开销，在实例化字符串常量的时候进行了一些优化 为字符串开辟一个字符串常量池，类似于缓存区 创建字符串常量时，首先判断字符串常量池是否存在该字符串 存在的话，返回引用实例，不存在，实例化该字符串并放入池中 实现的基础 实现该优化的基础是因为字符串不可变，可以不用担心数据冲突进行共享 运行时实例创建的全局字符串常量池中有一个表，总是为池中每个唯一的字符串对象维护一个引用,这就意味着它们一直引用着字符串常量池中的对象，所以，在常量池中的这些字符串不会被垃圾收集器回收。 在哪里 常量池 堆 存储的是对象，每个对象都包含一个与之对应的class。 JVM只有一个堆区(heap)被所有线程共享，堆中不存放基本类型和对象引用，只存放对象本身对象的由垃圾回收器负责回收，因此大小和生命周期不需要确定。 栈 每个线程包含一个栈区，栈中只保存基础数据类型的对象和自定义对象的引用(不是对象)。 每个栈中的数据(原始类型和对象引用)都是私有的。 栈分为3个部分：基本类型变量区、执行环境上下文、操作指令区(存放操作指令)。 数据大小和生命周期是可以确定的，当没有引用指向数据时，这个数据就会自动消失。 方法区 静态区，跟堆一样，被所有的线程共享。 方法区中包含的都是在整个程序中永远唯一的元素，如class，static变量。 运行时常量池存在于方法区，而字符串常量池在非堆空间（但不是静态区） 123456String str1 = “abc”;String str2 = “abc”;String str3 = “abc”;String str4 = new String(“abc”);String str5 = new String(“abc”);String str5 = new String(“bc11”); 这个，这样理解：对于new String(&quot;abc&quot;)，在执行new时，字符串常量池如果存在abc，则在堆创建对象new String(),并且把字符串常量池”abc”的引用返回给堆的对象， 不存在”bc11”，则在堆空间创建完对象再把内容写进字符串常量池。 String.intern()当一个String实例str调用intern()方法时，Java查找运行时常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在运行时常量池中增加一个Unicode等于str的字符串并返回它的引用。 12345678910String s0 = "k先生";String s1 = new String("k先生");String s2 = new String("k先生");System.out.println(s0 == s1);System.out.println("----------------------------------");s1.intern();s2 = s2.intern(); // 把常量池中“k先生”的引用赋给s2System.out.println(s0 == s1);System.out.println(s0 == s1.intern());System.out.println(s0 == s2); 输出 12345false----------------------------------falsetruetrue 源码解析声明声明为final，可序列化，可比较，不可变，不能被继承，实现了Serializable，Comparable，CharSequence接口。 12public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; 内部使用了final字符数组进行存储，涉及value数组的操作都使用了拷贝数组元素的方法，保证了不能在外部修改字符数组，String重写了Object的hashCode()方法使hash值基于字符数组内容，但是由于String缓存了hash值，所以即使通过反射改变了字符数组内容，hashCode()返回值不会自动更新。 123456private final char value[];private int hash; // Default to 0/** use serialVersionUID from JDK 1.0.2 for interoperability */private static final long serialVersionUID = -6849794470754667710L; 构造方法基本有14个公共构造方法，重载了很多： String类主要提供了通过String，StringBuilder，char数组，int数组，byte数组（需要指定编码）进行初始化。 通过字符数组，StringBuffer，StringBuilder进行初始化时，就要执行value数组元素的拷贝，创建新数组，防止外部对value内容的改变。 通过byte数组进行初始化，需要指定编码，或使用默认编码（ISO-8859-1），否则无法正确解释字节内容。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public String() &#123; this.value = new char[0];&#125;public String(String original) &#123; this.value = original.value; this.hash = original.hash;&#125;public String(char value[]) &#123; this.value = Arrays.copyOf(value, value.length);&#125;public String(char value[], int offset, int count) &#123; if (offset &lt; 0) &#123; throw new StringIndexOutOfBoundsException(offset); &#125; if (count &lt; 0) &#123; throw new StringIndexOutOfBoundsException(count); &#125; // Note: offset or count might be near -1&gt;&gt;&gt;1. if (offset &gt; value.length - count) &#123; throw new StringIndexOutOfBoundsException(offset + count); &#125; this.value = Arrays.copyOfRange(value, offset, offset+count);&#125;public String(int[] codePoints, int offset, int count) &#123;//略 @Deprecatedpublic String(byte ascii[], int hibyte, int offset, int count) &#123;//略// 检查越界private static void checkBounds(byte[] bytes, int offset, int length) &#123; if (length &lt; 0) throw new StringIndexOutOfBoundsException(length); if (offset &lt; 0) throw new StringIndexOutOfBoundsException(offset); if (offset &gt; bytes.length - length) throw new StringIndexOutOfBoundsException(offset + length);&#125;public String(byte bytes[], int offset, int length, String charsetName) throws UnsupportedEncodingException &#123; if (charsetName == null) throw new NullPointerException("charsetName"); checkBounds(bytes, offset, length); this.value = StringCoding.decode(charsetName, bytes, offset, length);&#125;public String(StringBuffer buffer) &#123; synchronized(buffer) &#123; this.value = Arrays.copyOf(buffer.getValue(), buffer.length()); &#125;&#125;public String(StringBuilder builder) &#123; this.value = Arrays.copyOf(builder.getValue(), builder.length());&#125;//内部构造方法 ， 能修改value，外部无法访问String(char[] value, boolean share) &#123; // assert share : "unshared not supported"; this.value = value;&#125; 主要方法下面这下方法实现简单： public int length() public boolean isEmpty() public char charAt(int index) public boolean startsWith() //是否已前缀开头，加上偏移量 public boolean endsWith() public int indexOf() public int lastIndexOf() public String substring() //数学里的[1,5) public String concat() public String replace() public boolean matches() //内部调用了Pattern.matches(regex, this) public boolean contains() public String replaceAll(String regex, String replacement) public static String join( ) //静态连接方法 public String toLowerCase(Locale locale) //小写 public String toUpperCase(Locale locale) public String trim() //去掉开头和结尾的空格 public char[] toCharArray() //直接内用了 System.arraycopy（） public static String valueOf() //通过构造方法，新创建一个字符串返回，或者使用参数的toString()方法 public native String intern() //跟常量池 有关的 本地方法 这些方法深究深究： public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) ，把String拷贝到特定的char数组中，内部用了System.arraycopy()方法来拷贝。 编码问题getBytes() 当需要处理各种各样的编码问题，在处理之前，必须明确“源”的编码，然后用指定的编码方式正确读取到内存中。 如不指定编码，此方法使用平台默认的字符集来获取字符串对应的字节数组。首先会使用JVM默认编码，而JVM则取操作系统默认编码（我的win10系统是GBK编码）进行编码。 public byte[] getBytes(String charsetName) ，使用指定的字符集将此字符串编码为一个字节序列，并将结果存储到新的字节数组中。 1234567891011121314151617181920String str = "中文";// 获取JVM默认字符集System.out.println("defaultCharset:" + Charset.defaultCharset());System.out.println("##字符串转换成byte数组");byte[] defaultByteArray = str.getBytes();byte[] gbkByteArray = str.getBytes("GBK");byte[] utfByteArray = str.getBytes("UTF-8");System.out.println("defaultByteArray:" + Arrays.toString(defaultByteArray));System.out.println("gbkByteArray:" + Arrays.toString(gbkByteArray));System.out.println("utfByteArray:" + Arrays.toString(utfByteArray));System.out.println("##byte数组转换成字符串");String defaultStr = new String(defaultByteArray);String gbkStr = new String(defaultByteArray, "GBK");String utfStr = new String(defaultByteArray, "UTF-8");System.out.println("defaultStr:" + defaultStr);System.out.println("gbkStr:" + gbkStr);// 因为utf-8是变长编码，没有跟[-42, -48, -50, -60]对应的用utf-8字符集的字符串，所以会乱码System.out.println("utfStr:" + utfStr); 输出 123456789defaultCharset:GBK##字符串转换成byte数组defaultByteArray:[-42, -48, -50, -60]gbkByteArray:[-42, -48, -50, -60]utfByteArray:[-28, -72, -83, -26, -106, -121]##byte数组转换成字符串defaultStr:中文gbkStr:中文utfStr:???? 比较方法equals()12345678910111213141516171819202122public boolean equals(Object anObject) &#123; if (this == anObject) &#123; return true; &#125; //判断是否属于一个类，也是String类对象 if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false;&#125; public boolean equalsIgnoreCase(String anotherString) ：将这个字符串与另一个字符串进行比较，忽略大小写的考虑。 如果两个字符串的长度相同，两个字符串被认为是相等的，忽略大小写，两个字符串中的相应字符相等则忽略大小写。 比较方法 compareTo()123456789101112131415161718public int compareTo(String anotherString) &#123; int len1 = value.length; int len2 = anotherString.value.length; int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; while (k &lt; lim) &#123; char c1 = v1[k]; char c2 = v2[k]; if (c1 != c2) &#123; return c1 - c2; &#125; k++; &#125; return len1 - len2;&#125; public int compareToIgnoreCase(String str) ,比较大小忽略大小写。 hashCode方法12345678910111213public int hashCode() &#123; int h = hash; //空字符串 hash是0 if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; 为什么 是乘 31 ？在 布洛赫的约书亚 Effective Java 中提及：值 31被选中，因为它是一个奇数。 如果它甚至是乘法溢出，信息会丢失，因为乘以 2等于移位。 使用素数的优势不明显，但它是传统的。 31的一个不错的属性是乘法可以通过移位和减法替换，以获得更好的性能： 31 * i == (i &lt;&lt;5) - i 现代j JVM 自动执行这种优化。 切割方法，split()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public String[] split(String regex, int limit) &#123; /* fastpath if the regex is a (1)one-char String and this character is not one of the RegEx's meta characters ".$|()[&#123;^?*+\\", or (2)two-char String and the first char is the backslash and the second is not the ascii digit or ascii letter. */ char ch = 0; if (((regex.value.length == 1 &amp;&amp; ".$|()[&#123;^?*+\\".indexOf(ch = regex.charAt(0)) == -1) || (regex.length() == 2 &amp;&amp; regex.charAt(0) == '\\' &amp;&amp; (((ch = regex.charAt(1))-'0')|('9'-ch)) &lt; 0 &amp;&amp; ((ch-'a')|('z'-ch)) &lt; 0 &amp;&amp; ((ch-'A')|('Z'-ch)) &lt; 0)) &amp;&amp; (ch &lt; Character.MIN_HIGH_SURROGATE || ch &gt; Character.MAX_LOW_SURROGATE)) &#123; int off = 0; int next = 0; boolean limited = limit &gt; 0; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); while ((next = indexOf(ch, off)) != -1) &#123; if (!limited || list.size() &lt; limit - 1) &#123; list.add(substring(off, next)); off = next + 1; &#125; else &#123; // last one //assert (list.size() == limit - 1); list.add(substring(off, value.length)); off = value.length; break; &#125; &#125; // If no match was found, return this if (off == 0) return new String[]&#123;this&#125;; // Add remaining segment if (!limited || list.size() &lt; limit) list.add(substring(off, value.length)); // Construct result int resultSize = list.size(); if (limit == 0) &#123; while (resultSize &gt; 0 &amp;&amp; list.get(resultSize - 1).length() == 0) &#123; resultSize--; &#125; &#125; String[] result = new String[resultSize]; return list.subList(0, resultSize).toArray(result); &#125; return Pattern.compile(regex).split(this, limit);&#125; 参考文献JDK1.8 源码http://blog.csdn.net/stubbornant/article/details/51535946http://www.jianshu.com/p/69ad183fefc4https://segmentfault.com/a/1190000009888357http://www.importnew.com/18167.htmlhttps://www.zhihu.com/question/55328596 完结。]]></content>
      <categories>
        <category>JDK 源码解析</category>
      </categories>
      <tags>
        <tag>JDK1.8</tag>
        <tag>源码解析</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK1.8 HashMap源码解析]]></title>
    <url>%2Fp%2F79a4.html</url>
    <content type="text"><![CDATA[JDK源码深揪，JDK1.8版本。 摘要HashMap 经常用到的 java 内置的数据结构，今天就深入源码瞧瞧,刨一刨。 HashMap 是对 Map 接口的一种哈希表的实现，是 key-value（键-值对），通过 key 可以常数时间内找到 value 。它是非线程安全，如果需要满足线程安全，可以用 Collections 的 synchronizedMap 方法使 HashMap 具有线程安全的能力，或者使用 ConcurrentHashMap。但是 HashMap 不是 Collection 的子集，但是 HashMap 的 key 和value 都可以单独的转换成 Collection子集，比如转换成 Set。HashMap 允许存在 null的，最多只允许一条记录的键为 null，允许多条记录的值为 null。 那我就简单的看一看 HashMap 源码，也是我第一次看整个类，以前都是需要哪个方法 就跳进去 看看，过程又惊喜又痛苦。 源码解析HashMap继承自AbstractMap,并且实现了Map,Cloneable,Serializable接口。 12public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; 常量HashMap 的初始容量为16，最大的容量为1073741824（1&lt;&lt;30），当构造函数中没有指定参数时，使用的默认负载因子为0.75f , 使用了基于数组哈希表的结构，数组中每一个元素都是一个链表，把数组中的每一格称为一个桶(bin或bucket)。当HashMap中节点数量超过容量和装填因子的积，会进行扩容操作，扩容后的HashMap容量是之前容量的两倍（左位移操作）。 123static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 初始容量static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 最大的容量static final float DEFAULT_LOAD_FACTOR = 0.75f; // 负载因子 ， 加载因子 ， 装填因子 HashMap 的扩容及树化过程中会涉及下列常量，当好多的节点（node）被映射在同一个桶（bin）中的时，如果这个桶（bin）的数量小于TREEIFY_THRESHOLD，不会转换成树性结构；如果这个桶（bin）的节点（node）数量大于TREEIFY_THRESHOLD,但是容量（总节点数）却小于MIN_TREEIFY_CAPACITY,则依然使用链表结构进行存储，此时会对HashMap进行扩容，如果容量大于MIN_TREEIFY_CAPACITY就开始进行树形化。 123static final int TREEIFY_THRESHOLD = 8; static final int UNTREEIFY_THRESHOLD = 6; static final int MIN_TREEIFY_CAPACITY = 64; 成员变量来看看成员变量， transient 此成员变量不可序列化。 都知道 HashMap 是数组链表（+红黑树），Node&lt;K,V&gt;[] table 是哈希桶数组。对于红黑树抽空搞一搞。 123456transient Node&lt;K,V&gt;[] table;transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; // 保持缓存的entrySet()transient int size; // 实际HashMap拥有key-value数 transient int modCount; // 此字段用于使HashMap的集合视图上的迭代器失效int threshold; // 下一次调整大小的阈值（capacity * load factor）。final float loadFactor; // 负载因子 节点HashMap 的节点 ， 第一个是链表节点，第二个是红黑树节点。 1234567891011121314151617181920212223242526272829303132333435//链表节点/*** Basic hash bin node, used for most entries. (See below for* TreeNode subclass, and in LinkedHashMap for its Entry subclass.)*/static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;//详情略。。。//红黑树节点/*** Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn* extends Node) so can be used as extension of either regular or* linked node.*/static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125;//详情略。。。 构造函数HashMap 有四个构造函数，主要涉及两个成员变量loadFactor（负载因子）threshold（下一次调整大小的阈值（capacity * load factor））。构造函数并不会创建空间，第一次put操作才会创建空间，懒加载过程。 12345678910111213141516171819202122public HashMap(int initialCapacity, float loadFactor) &#123; public HashMap(int initialCapacity) &#123;public HashMap() &#123; public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; //保证容量是2的幂 static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; Hash 算法 和 扩容过程Hash 算法hash算法三个步骤： 用 key 的 hashCode() 获取 hash 值。 无符号右移16位（无符号，空位补0，一共32位），然后异或(^)运算。这样可以避免只靠低位数据来计算哈希时导致的冲突，计算结果由高低位结合决定，可以避免哈希值分布不均匀。 最后用table的长度求模（h%table.length）。比如，在获取和插入方法getNode(),putVal() 都有求模运算，只不过用与(&amp;)运算来代替模运算来提高效率。 1234567891011//hash 方法static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;//插入方法 和 查询方法 内部小块代码 ，table是node数组Node&lt;K,V&gt;[] tab;if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length;if ((p = tab[i = (n - 1) &amp; hash]) == null) 图解，图片地址http://blog.csdn.net/u011240877/article/details/53351188 扩容过程这篇文章（http://blog.csdn.net/fan2012huan/article/details/51088211）中有笔者详细实验，详细记录了扩容过程，但是我这里就刨一刨代码是怎么实现的。 直接看代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/*** Initializes or doubles table size. If null, allocates in* accord with initial capacity target held in field threshold.* Otherwise, because we are using power-of-two expansion, the* elements from each bin must either stay at same index, or move* with a power of two offset in the new table.** @return the table*/final Node&lt;K,V&gt;[] resize() &#123; //复制一份 数据链表数据 Node&lt;K,V&gt;[] oldTab = table; //保存旧的数据 阈值 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //新增容量为以前的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp;oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果旧的容量为0，旧的阈值&gt;0，说明创建了hashtable 但是没有添加数据， //初始化容量等于阈值 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults //都是0，还没创建hashtable，所以初始化 ，构造函数里也用了扩容 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //如果新的阈值 为 0 ，就得重新 计算一次 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 遍历复制 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 公共方法//返回size。public int size() // 是否为空。public boolean isEmpty() // 获取value，里面调用了final getNode（int hash, Object key）方法。public V get(Object key) // 是否存在key，里面调用了final getNode（int hash, Object key）方法。public boolean containsKey(Object key) // 与key相关联的上一个value，如果不存在key，是新加key-value，则返回null。public V put(K key, V value) // 将指定map的所有映射复制到此map。这个跟用map构造函数一样。public void putAll(Map&lt;? extends K, ? extends V&gt; m) // 删除key对应的key-value，删除成功返回上一个value值。public V remove(Object key) // 删除所有节点public void clear() //value 是否存在public boolean containsValue(Object value) // 返回key的set集合public Set keySet() // 返回value的Colection集合 ， 跟AbstractMap 里变量 values 有关。public Collection values() // 返回key-value 的MapEntrypublic Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() jdk1.8 扩展的方法，都是重写了Map 接口 // 重写Map接口的方法，获取value ，不存在则得到默认值public V getOrDefault(Object key, V defaultValue) // 重写Map接口的方法，如果指定的key尚未与value相关联（或映射到{@code null}），则将其与给定value相关联并返回{@code null}，否则返回当前value。public V putIfAbsent(K key, V value) // 删除节点 ， 里面返回了节点并且判断了是否为空。public boolean remove(Object key, Object value) // 仅当当前映射到指定的oldValue时，才能替换成newValue。public boolean replace(K key, V oldValue, V newValue) // 替换key 对应得value，返回以前的value 或 null。public V replace(K key, V value) // 如果指定的key尚未与value相关联（或映射到null），则尝试使用给定的映射函数计算其value，并将其输入到此映射中，除非为null。 返回与指定key相关联的当前（现有或计算）value，如果计算值为空，则为null。public V computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction) 后面是一些 三个迭代器（key,value,entry） 和 TreeNode 相关代码，不看了，目前还没兴趣往下看。 参考文献JDK1.8 源码https://tech.meituan.com/java-hashmap.htmlhttp://blog.csdn.net/fan2012huan/article/details/51088211http://blog.jrwang.me/2016/java-collections-hashmap/http://blog.csdn.net/u011240877/article/details/53351188 完结。]]></content>
      <categories>
        <category>JDK 源码解析</category>
      </categories>
      <tags>
        <tag>JDK1.8</tag>
        <tag>HashMap</tag>
        <tag>源码解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用 linux 命令]]></title>
    <url>%2Fp%2Fbc7.html</url>
    <content type="text"><![CDATA[个人日常使用linux命令笔记，详情用man 命令 来查看即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#管道符号，是unix一个很强大的功能,符号为一条竖线:&quot;|&quot;。#用法: command 1 | command 2 他的功能是把第一个命令command 1执行的结果作为command 2的输入传给command 2，# 运行jar 包nohup java -jar solr.jar &gt;solr.log &amp; # 动态输出日志文件tail -f solr.log #解压unzip solr.zip #解压 tartar -zxvf # CRT 上传rz -be # CRT 下载sz# 查看当前文件权限ls -l learn.sh -rwxr-xr-x 1 root root 34 Sep 29 14:05 learn.sh # 给文件加权限chmod [-cfvR] [--help] [--version] mode learn.shmode : [ugoa][+-=][rwxX] # 查看当前路径 pwd# 在 Linux 控制台生成ps -ef | grep java kill -3 PID(不过会输出到system.out) # 使用 jstack 获取 Thread Dumpjps -Vjstack -f 5824# 使用可视化用具 VisualVM Profiler# java 8 自带的工具 jcmd PID Thread.print # 用于连接文件并打印到标准输出设备上cat file1 &gt; file2#用vim查看大容量日志时会爆掉#用lessless filename# tree命令，树形打印目录下文件tree 目录名# 拷贝cp file1 file2# 删除rm -rf file# 剪切，重命名mv file1 file2#linux定时任务crontab -e#基本格式 :#* * * * * command#分 时 日 月 周 命令#命令用来在指定目录下查找文件find . -name &quot;*.sh&quot;#查看文件行数wc -l file#ll不是命令，是ls -l的别名ll#显示进程详情ps -A -opid,stime,etime,argsps -opid,args#显示目前所有文件系统的可用空间及使用情形df -h#查询文件或文件夹的磁盘使用空间du -h du -sm * 未完，待续。。。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 多线程 生产者-消费者模式 爬取数据]]></title>
    <url>%2Fp%2F854d.html</url>
    <content type="text"><![CDATA[摘要最近收集了数据供个人实验使用，后期再展示实验结果。 用利用java 多线程，生产者和消费者模式一周爬取了内涵段子1300w+条数据，写了三个文件，每个文件都有1.5G左右。 程序的整体思路： 一个线程（生产者）爬取json数据并格式化成我想要的json格式（去掉一些无用信息），然后三个线程（消费者）各自写进自己的文件，每行是一条json。还有把所有的任务交给线程池ExecutorService去执行多线程，生产者和消费者的公共容器使用的是LinkedBlockingQueue。 这里解释一下，为什么是一个生产者？ 原因是爬取的json数据里有请求下一个json数据的max_time变量，这导致无法使用多个生产者。 完整实现从服务端获取json数据时中文unicode编码用fastjson解决的。我在上一篇文章中提到了解决方案。 Collect类下面是Collect类，生产者和消费者都要操作的类，这里公用容器用LinkedBlockingQueue，每次查询都会返回20条数据，所以我把每次的20条数据写进list，容器能存储100个list。 日志里保存了获取下一条json的max_time变量，供观察运行情况。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198package com.im_k.collectSet;import java.io.BufferedReader;import java.io.BufferedWriter;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.io.UnsupportedEncodingException;import java.net.MalformedURLException;import java.net.URL;import java.util.ArrayList;import java.util.List;import java.util.concurrent.LinkedBlockingQueue;import java.util.regex.Matcher;import java.util.regex.Pattern;import org.apache.log4j.Logger;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.JSONObject;/** * @author dalai * @date 2017-10-20 * */public class Collect &#123; /* * 本程序内容： * 从http://neihanshequ.com/joke/?is_json=1&amp;app_name=neihanshequ_web&amp;max_time= * 1508400711 获取json，解析出用户id、用户名、和内容，并且构造成json写到文件，每个段子写一行。 */ static String homeUrl = "http://neihanshequ.com/joke/?is_json=1&amp;app_name=neihanshequ_web&amp;max_time="; static String max_time = "1508400711"; static Logger logger = Logger.getLogger(Collect.class); private final int MAX_SIZE = 100; private LinkedBlockingQueue&lt;List&lt;String&gt;&gt; lbq = new LinkedBlockingQueue&lt;List&lt;String&gt;&gt;(MAX_SIZE); public Collect() &#123; &#125; // 从服务端获取json private List&lt;String&gt; getContents() &#123; InputStream in = null; InputStreamReader reader = null; BufferedReader bufferedReader = null; List&lt;String&gt; jsonList = new ArrayList&lt;String&gt;(); try &#123; URL url = new URL(homeUrl + max_time); in = url.openStream(); if (in != null) &#123; reader = new InputStreamReader(in, "utf-8"); &#125; else &#123; return jsonList; &#125; bufferedReader = new BufferedReader(reader); String temp = null; String data = ""; while ((temp = bufferedReader.readLine()) != null) &#123; data += temp; &#125; Pattern pattern = Pattern.compile("(\\\\u(\\p&#123;XDigit&#125;&#123;4&#125;))"); Matcher matcher = pattern.matcher(data); char ch; while (matcher.find()) &#123; ch = (char) Integer.parseInt(matcher.group(2), 16); data = data.replace(matcher.group(1), ch + ""); &#125; JSONObject jsonObject = JSON.parseObject(data); jsonList = dataOperation(jsonObject); &#125; catch (MalformedURLException e) &#123; e.printStackTrace(); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (in != null) &#123; in.close(); reader.close(); bufferedReader.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return jsonList; &#125; // 解析，构造json格式 private List&lt;String&gt; dataOperation(JSONObject jsonObject) &#123; List&lt;String&gt; jsonList = new ArrayList&lt;String&gt;(); if (jsonObject == null) return jsonList; max_time = ((JSONObject) jsonObject.get("data")).getString("max_time"); logger.info("max_time=" + max_time); // System.out.println("max_time=" + max_time); JSONArray jsonArray = (JSONArray) ((JSONObject) jsonObject.get("data")).get("data"); for (int idx = 0, len = jsonArray.size(); idx &lt; len; idx++) &#123; String name = (String) ((JSONObject) ((JSONObject) ((JSONObject) jsonArray.get(idx)).get("group")) .get("user")).getString("name"); String id = (String) ((JSONObject) ((JSONObject) ((JSONObject) jsonArray.get(idx)).get("group")) .get("user")).getString("user_id"); String content = (String) ((JSONObject) ((JSONObject) jsonArray.get(idx)).get("group")).getString("text"); String jsonStr = "&#123;\"name\":\"" + name + "\",\"id\":\"" + id + "\",\"content\":\"" + content + "\"&#125;"; jsonList.add(jsonStr); &#125; return jsonList; &#125; // 生产者 public void produce() &#123; List&lt;String&gt; jsonList = getContents(); // if (lbq.size() == MAX_SIZE) &#123; // System.out.println("缓冲区容量：" + MAX_SIZE + "\t 暂时不能执行任务!"); // &#125; try &#123; lbq.put(jsonList); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; // 消费者 public void consume(BufferedWriter bw) &#123; // if (lbq.size() == 0) &#123; // System.out.println("缓冲区容量：0 \t 暂时不能执行任务!"); // &#125; List&lt;String&gt; jsonList = null; for (int i = 0; i &lt; lbq.size(); i++) &#123; try &#123; jsonList = lbq.take(); writeToFile(jsonList, bw); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 写进文件中 private void writeToFile(List&lt;String&gt; jsonList, BufferedWriter bw) &#123; if (jsonList == null || jsonList.size() &lt;= 0) return; try &#123; for (int i = 0, len = jsonList.size(); i &lt; len; i++) &#123; bw.write(jsonList.get(i)); bw.newLine(); bw.flush(); &#125; &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // test public static void main(String[] args) throws UnsupportedEncodingException &#123; Collect collect = new Collect(); collect.getContents(); &#125;&#125; 多线程 生产者-消费者每个线程都是死循环的，我写死了，直到我手动杀掉进程。 还有把所有线程提交给线程池去运行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package com.im_k.collectSet;import java.io.BufferedWriter;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.OutputStreamWriter;import java.io.UnsupportedEncodingException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @auther dalai * @date 2017-10-20 * *///生产者class Producer implements Runnable &#123; private Collect collect; public Producer(Collect collect) &#123; this.collect = collect; &#125; public void run() &#123; while (true) &#123; collect.produce(); &#125; &#125;&#125;//消费者class Consumer implements Runnable &#123; private Collect collect; public Consumer(Collect collect) &#123; this.collect = collect; &#125; public void run() &#123; BufferedWriter bw = null; String fileName = Thread.currentThread().getName(); fileName = "./samples." + fileName + ".txt"; System.out.println(fileName); try &#123; bw = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(fileName), "UTF-8")); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; while (true) &#123; collect.consume(bw); &#125; &#125;&#125;//多线程类public class MultiThread &#123; public static void main(String[] args) &#123; Collect collect = new Collect(); // 一个生产者 Producer p1 = new Producer(collect); // 三个消费者 Consumer c1 = new Consumer(collect); Consumer c2 = new Consumer(collect); Consumer c3 = new Consumer(collect); ExecutorService executorService = Executors.newFixedThreadPool(4); executorService.execute(p1); executorService.execute(c1); executorService.execute(c2); executorService.execute(c3); executorService.shutdown(); &#125;&#125; 完结。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从服务端获取json串中文unicode编码问题]]></title>
    <url>%2Fp%2Fc94.html</url>
    <content type="text"><![CDATA[今天遇到从服务端获取json数据打印到控制台后发现json中包含的中文全部是unicode编码显示状态，经过查阅解决问题如下。 服务端返回的json中输出到控制台的内容有一段是这样的： 1"text": "\u6709\u53ea\u82cd\u8747\u98de\u8fdb" 用getBytes() (不可用)直接截取内容用getBytes()转能转成中文，但是整个json内容转的时候确实无效的。 1234String str = "\u6709\u53ea\u82cd\u8747\u98de\u8fdb";byte[] bt = str.getBytes("utf-8");String ret = new String(bt, "utf-8");System.out.println(ret); 输出如下： 1有只苍蝇飞进着 用 regex 的 pattern 和 matcher (可用)用正则把unicode编码转成16进制，再用char显示就成中文了。 char类型是一个单一的 16 位 Unicode 字符，最小值是 \u0000（即为0），最大值是 \uffff（即为65535）,可以储存任何字符。 这里的data是直接服务端获取的json串。 12345678Pattern pattern = Pattern.compile("(\\\\u(\\p&#123;XDigit&#125;&#123;4&#125;))");Matcher matcher = pattern.matcher(data);char ch;while (matcher.find()) &#123; ch = (char) Integer.parseInt(matcher.group(2), 16); data = data.replace(matcher.group(1), ch + "");&#125;System.out.println(data); 从代码上看，Unicode转中文关键是：(char)Integer.parseInt(matcher.group(2), 16)，其中matcher.group(2)表示匹配到的Unicode编码格式。比如：4e2d转成16进制为200013，即：(char)200013为汉字：中。 java底层如何表示字符型数据？java中的String类型是采用UTF-16编码实现的，也就是不管在源码的编码如何，在Java虚拟机中的字符串都是使用UTF-16编码实现。 我们要知道String其实就是char[]。也就是说，java底层保存字符型数据是使用16位整数数字。 这意味着我们可以这样：(char)200013结果：中 。 用 fastjson 包 (可用)还有更简单的方法，fastjson 包是阿里的一个解析json的jar 包。 这里的data是直接服务端获取的json串。 123JSONObject jsonObject = JSON.parseObject(data);System.out.println(jsonObject.toString()); 这是fastjson的github wiki：https://github.com/alibaba/fastjson/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98 学习文献【1】http://blog.csdn.net/u013066244/article/details/54708179【2】https://segmentfault.com/q/1010000005710620?_ea=943588 完结。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>json</tag>
        <tag>fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本 定期删除日志文件]]></title>
    <url>%2Fp%2F8a28.html</url>
    <content type="text"><![CDATA[编写定期删除日志文件的shell脚本程序。 实现代码创建脚本del_log.sh 1vim del_log.sh 编写,用find命令简洁 123456789#!/bin/bashlocation=&quot;/home/dl/code/logs&quot;find $location -mtime +4 -type f | xargs rm -f#location 是删除文件所在目录#-mtime +4 是设置删除多少天以前的文件#-type f 删除的类型为文件 给文件执行权限 1chmod +x del_log.sh 查看当前用户是否存在定时执行任务 1crontab -l 注意： 如果提示crontab命令不存在，那可能需要安装crontab命令。 添加定时执行任务 1crontab -e 进入到文件/tmp/crontab.4rh7ec(文件后缀不一样没关系),添加 110 4 * * 1 /bin/bash /home/dl/code/del_log.sh 保存后提示 12&quot;/tmp/crontab.4rh7ec&quot; 3L, 143C writtencrontab: installing new crontab 查看当前用户下定时任务列表 1crontab -l 到这就完成了定时删除日志文件的shell脚本。 命令crontab 时间表达式 12* * * * * command分 时 日 月 周 命令 例子： 123456789101112#每分钟执行一次* * * * *#每隔一小时执行一次 00 * * * * #or* */1 * * * #(/表示频率) #每小时的15和30分各执行一次 15,45 * * * * #（,表示并列） 命令find例子： 12#将目前目录及其子目录下所有延伸档名是 sh 的文件列出来。find . -name &quot;*.sh&quot; find命令详情请查看 完结。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell 脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot 使用]]></title>
    <url>%2Fp%2Fab2f.html</url>
    <content type="text"><![CDATA[本节将详细介绍如何使用Spring Boot。 它涵盖了诸如构建系统，自动配置以及如何运行应用程序等主题。 我们还介绍了一些Spring Boot的最佳做法。 虽然Spring Boot没有什么特别的特点（它只是另一个可以消耗的lib），但是有一些建议可以让您的开发过程更容易一些。 构建系统构建典型的系统，在另一篇 spring boot 快速入门中有maven 和 gradle 配置方式。 典型的pom.xml里有这个parent里加了org.springframework.boot,如果pom.xml已经有其它的parent，这里也有不用继承，也能配置maven的方式。 parent 是 maven 的一个元素，可以继承pom.xml中依赖，不用重复配置pom。 spring boot 不使用 parent 来继承依赖如果您不想使用spring-boot-starter-parent，则仍然可以通过使用scope = import依赖关系来保持依赖关系管理（但不是插件管理）的好处： 123456789101112&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!-- Import dependency management from Spring Boot --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 有parent元素用了spring boot 时 覆盖单挑依赖直接后面加就行。 但是现在，要实现相同的结果，您需要在spring-boot-dependencies条目之前在项目的dependencyManagement中添加一个条目。 例如，要升级到另一个Spring Data发行列车，您需要将以下内容添加到pom.xml中。 12345678910111213141516171819&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Override Spring Data release train provided by Spring Boot --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-releasetrain&lt;/artifactId&gt; &lt;version&gt;Fowler-SR2&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 除了maven 和 gradle 外 ， 官方还有ant 构建系统的例子，ant 构建spring boot 详情 spring boot Startersstarters是一组方便的依赖关系描述符，可以包含在应用程序中。 您可以获得所需的所有Spring和相关技术的一站式服务，而无需通过示例代码搜索和复制粘贴相关性描述符的负载。 例如，如果要开始使用Spring和JPA进行数据库访问，那么只需在项目中包含spring-boot-starter-data-jpa依赖项，就可以了。 starters包含许多依赖关系，您需要使项目快速启动并运行，并具有一致的受支持的托管传输依赖关系。 Spring Boot在org.springframework.boot组下提供了以下应用程序starters，是时候展现spring的强大之处了： spring-boot-starter spring-boot-starter-activemq spring-boot-starter-amqp spring-boot-starter-aop spring-boot-starter-artemis spring-boot-starter-batch spring-boot-starter-cache spring-boot-starter-cloud-connectors spring-boot-starter-data-cassandra spring-boot-starter-data-couchbase spring-boot-starter-data-elasticsearch spring-boot-starter-data-gemfire spring-boot-starter-data-jpa spring-boot-starter-data-ldap spring-boot-starter-data-mongodb spring-boot-starter-data-neo4j spring-boot-starter-data-redis spring-boot-starter-data-rest spring-boot-starter-data-solr spring-boot-starter-freemarker spring-boot-starter-groovy-templates spring-boot-starter-hateoas spring-boot-starter-integration spring-boot-starter-jdbc spring-boot-starter-jersey spring-boot-starter-jooq spring-boot-starter-jta-atomikos spring-boot-starter-jta-bitronix spring-boot-starter-jta-narayana spring-boot-starter-mail spring-boot-starter-mobile spring-boot-starter-mustache spring-boot-starter-security spring-boot-starter-social-facebook spring-boot-starter-social-linkedin spring-boot-starter-social-twitter spring-boot-starter-test spring-boot-starter-thymeleaf spring-boot-starter-validation spring-boot-starter-web spring-boot-starter-web-services spring-boot-starter-websocket spring-boot-starter-actuator spring-boot-starter-remote-shell spring-boot-starter-jetty spring-boot-starter-log4j2 spring-boot-starter-logging spring-boot-starter-tomcat spring-boot-starter-undertow 获取这些starters的pom.xml 依赖配置地址获取pom.xml依赖项。 编写良好的代码编写良好的代码，避免一些低级的坑。 使用”default“包当类不包括包声明时，它被认为是在“默认包”中。 官方不推荐使用“默认包”，应该避免使用。 对于使用@ComponentScan，@EntityScan或@SpringBootApplication注释的Spring Boot应用程序，可能会导致特殊的问题，因为每个jar的每个类都将被读取。 应用程序主类我们通常建议您将主应用程序类定位到其他类之上的根包中。 @EnableAutoConfiguration注释通常放置在您的主类上，并且它隐式定义了某些项目的基本“搜索包”。 例如，如果您正在编写JPA应用程序，则@EnableAutoConfiguration注释类的包将用于搜索@Entity项。 使用根包也可以使用@ComponentScan注释，而不需要指定basePackage属性。 如果主类位于根包中，也可以使用@SpringBootApplication注释。 良好的布局： 1234567891011121314com +- example +- myproject +- Application.java | +- domain | +- Customer.java | +- CustomerRepository.java | +- service | +- CustomerService.java | +- web +- CustomerController.java Application.java文件将声明main方法以及基本的@Configuration。 1234567891011121314151617package com.example.myproject;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;@Configuration@EnableAutoConfiguration@ComponentScanpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 配置 类Spring Boot支持基于Java的配置。 虽然可以使用XML源调用SpringApplication.run（），但我们通常建议您的主源是一个@Configuration类。 通常，定义main方法的类也是一个很好的候选者，作为主要的@Configuration。 导入其他配置类您不需要将所有的@Configuration放在一个类中。 @Import注释可用于导入其他配置类。 或者，您可以使用@ComponentScan自动拾取所有Spring组件，包括@Configuration类。 导入xml 配置如果您绝对必须使用基于XML的配置，我们建议您仍然从@Configuration类开始。 然后可以使用额外的@ImportResource注释来加载XML配置文件。 auto-configurationSpring Boot自动配置尝试根据您添加的jar依赖关系自动配置您的Spring应用程序。 例如，如果HSQLDB在您的类路径上，并且您没有手动配置任何数据库连接bean，那么我们将自动配置内存数据库。 您需要通过将@EnableAutoConfiguration或@SpringBootApplication注释添加到您的一个@Configuration类中来选择自动配置。 替换auto-configuration自动配置是无创的，您可以随时开始定义自己的配置来替换自动配置的特定部分。 例如，如果添加自己的DataSource bean，则默认的嵌入式数据库支持将会退回。 如果您需要了解当前正在应用的自动配置，以及为什么要使用–debug开关启动应用程序。 这将启用调试日志以选择核心日志记录器，并将自动配置报告记录到控制台。 取消特殊auto-configuration如果有特殊配置不想使用，可以用@EnableAutoConfiguration来取消。 12345678import org.springframework.boot.autoconfigure.*;import org.springframework.boot.autoconfigure.jdbc.*;import org.springframework.context.annotation.*;@Configuration@EnableAutoConfiguration(exclude=&#123;DataSourceAutoConfiguration.class&#125;)public class MyConfiguration &#123;&#125; spring beans 和 依赖注入您可以自由使用任何标准的Spring Framework技术来定义您的bean及其注入的依赖关系。 为了简单起见，我们经常发现使用@ComponentScan找到你的bean，结合@Autowired构造函数注入效果很好。 如果您按照上述建议（将应用程序类定位在根包中）构建代码，则可以添加@ComponentScan而不使用任何参数。 所有应用程序组件（@Component，@Service，@Repository，@Controller等）将自动注册为Spring Bean。 这是一个使用构造函数注入获取所需的RiskAssessor bean的@Service Bean的例子。 123456789101112131415161718package com.example.service;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class DatabaseAccountService implements AccountService &#123; private final RiskAssessor riskAssessor; @Autowired public DatabaseAccountService(RiskAssessor riskAssessor) &#123; this.riskAssessor = riskAssessor; &#125; // ...&#125; 使用@SpringBootApplication注解使用@SpringBootApplication 注解跟使用@Configuration, @EnableAutoConfiguration 和 @ComponentScan 三个效果是一样的。 12345678910111213package com.example.myproject;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplication // same as @Configuration @EnableAutoConfiguration @ComponentScanpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 完结。]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot 快速入门]]></title>
    <url>%2Fp%2F7dd9.html</url>
    <content type="text"><![CDATA[介绍spring bootspring boot 就是给使用spring的人带来方便，不在需要繁杂的xml配置。 Spring Boot可以轻松创建可以“运行”的独立的，生产级的基于Spring的应用程序。 大多数Spring Boot应用程序需要很少的Spring配置。 您可以使用Spring Boot创建可以使用java -jar或更多传统war部署启动的Java应用程序。 我们还提供运行“spring script”的命令行工具。spring boot的主要目标是： 为所有Spring开发提供一个更快，更广泛的入门体验。 被要求开箱即用，但随着需求开始偏离默认值而迅速脱离出来。 提供大量项目（例如嵌入式服务器，安全性，指标，运行状况检查，外部化配置）通用的一系列非功能特性。 绝对没有代码生成，也不需要XML配置。 系统环境 我这里学习的版本，spring boot 1.5.7 release , 要求java 7+，spring framework 4.3.11 + release , maven 3.2+ 和 gradle2.9 + ，如果使用这些注意版本。 serlvet 容器默认是 tomcat 8 ，可以换成 jetty。 安装 spring bootspring boot 还提供了一个客户端来操作，还需要用到groovy来写脚本，个人觉得直接用java传统的方式运行run as -&gt; java application就行。 配置尽管您可以复制Spring Boot jar到项目中，但是官方推荐您使用支持依赖关系管理的构建工具（如Maven或Gradle）。 maven 配置 spring bootSpring bbot依赖使用org.springframework.boot groupId。 通常，您的Maven POM文件将从spring-boot-starter-parent项目继承，并声明一个或多个“starters”的依赖关系。 Spring Boot还提供了一个可选的Maven插件来创建可执行的jar。 这里列出典型的pom.xml 文件： 1234567891011121314151617181920212223242526272829303132333435&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;myproject&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;!-- Inherit defaults from Spring Boot --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;!-- Add typical dependencies for a web application --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- Package as an executable jar --&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; gradle 配置 spring boot可以使用org.springframework.boot组声明Spring boot依赖项。 通常，您的项目将声明一个或多个“gradle”的依赖关系。 Spring Boot提供了一个有用的Gradle插件，可用于简化依赖关系声明并创建可执行的jar。 列出典型的build.gradle 文件： 12345678910111213141516171819plugins &#123; id &apos;org.springframework.boot&apos; version &apos;1.5.7.RELEASE&apos; id &apos;java&apos;&#125;jar &#123; baseName = &apos;myproject&apos; version = &apos;0.0.1-SNAPSHOT&apos;&#125;repositories &#123; jcenter()&#125;dependencies &#123; compile(&quot;org.springframework.boot:spring-boot-starter-web&quot;) testCompile(&quot;org.springframework.boot:spring-boot-starter-test&quot;)&#125; 安装 spring boot CLISpring Boot CLI是一个命令行工具，可以使用，如果你想用Spring快速原型。 它允许您运行Groovy脚本，这意味着您具有熟悉的类似Java的语法，没有太多的样板代码。 您不需要使用CLI来使用Spring Boot，但它绝对是将Spring应用程序从本地开始的最快方式。 但是我推荐使用，用ide比较好，毕竟项目大了好管理。 spring boot 的简单实现用ide（eclipse 或 IntelliJ IDEA）创建一个maven项目，直接添加pom.xml文件如下： 12345678910111213&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建个HelloSBoot类： 123456789101112131415161718192021import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.*;import org.springframework.stereotype.*;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controller@EnableAutoConfigurationpublic class HelloSboot &#123; @RequestMapping("/") @ResponseBody String home() &#123; return "you did it"; &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(HelloSboot.class, args); &#125;&#125; 这就是最简单的spring boot程序了写完了。run as -&gt; java application即可，运行后显示如下，并打开浏览器输入：http://localhost:8080/，会显示you did it。 console显示： 12345678910 . ____ _ __ _ _ /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.5.7.RELEASE)//后面有很多，在这略。。。 @Controller 和 @RequestMapping 注解这些都是spring 的注解， 了结果spring 或者 spring mvc 的很容易理解。 我们的Example类的第一个注释是@Controller。 这被称steretype 注释。 它为人们阅读代码提供了一些提示，对于Spring来说，这个类具有特定的作用。 在这种情况下，我们的类是一个web @Controller，所以Spring在处理传入的Web请求时会考虑它。 @RequestMapping注释提供“路由”信息。 告诉Spring，任何具有路径“/”的HTTP请求都应映射到home方法。 @ResponseBody注释告诉Spring将生成的字符串直接返回给调用者。 @EnableAutoContiguration 注解第二个类级别的注释是@EnableAutoConfiguration。 这个注释告诉Spring Boot根据你添加的jar依赖关系来“猜”你将如何配置Spring。 由于spring-boot-starter-web添加了Tomcat和Spring MVC，自动配置将假定您正在开发Web应用程序并相应地设置Spring。 只想说就是怎么智能！。 创建可运行jar包为了创建可运行jar包，需要添加pom.xml文件（上面典型pom.xml里有完整文件）： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 导出jar包即可。 完结。]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot 完整学习目录]]></title>
    <url>%2Fp%2Fc697.html</url>
    <content type="text"><![CDATA[摘要每次必推，官方文档spring boot 官方在线文档 以前用spring mvc来搭建web应用程序，每次都要配置好久xml，有时忘了还要查看以前的项目配置。最近使用spring boot 简单实现后我竟然开心的笑了，自带servlet容器，直接像java应用程序一样运行，自觉捡到了宝，深入的学习并作笔记来此分享。 Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Boot致力于在蓬勃发展的快速应用开发领域（rapid application development）成为领导者。 可以去各大论坛看看，spring boot的优缺点，但是我个人觉得，既然这东西能火起来，必定有他的道理，更何况给我带来方便为什么不用呢！ spring boot 完整学习目录 spring boot 快速入门spring boot 使用 参考文献深入学习微框架嘟嘟MD spring boot 干货系列未完，待续。。。]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本 快速入门]]></title>
    <url>%2Fp%2Fc96a.html</url>
    <content type="text"><![CDATA[你会linux是吗？一点点。那会shell脚本吗？不会。(就尴尬了不是) 所以抽空学习了一波shell脚本，这个语法跟php相似，但是呢有些格式要求，比如if语句，像python一样严谨。所以呢，还是要特别注意一些的。 下面就直接附上本人的学习的脚本代码，基本内容都涵盖了，有些细节问题就去查查linux命令或者看看其它教程网站都能查到。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240#!/bin/bash# author:dalai# www.imdalai.com# learn bash# 调试bash脚本# 将 -xv 参数添加到 #!/bin/bash后##这一行说明要使用的 shell。##!/bin/bash 表示脚本使用 /bin/bash。##对于 python 脚本，就是 #!/usr/bin/python。# 变量name=&quot;linux&quot;echo $&#123;name&#125;echo $name# 拼接字符串greeting=&quot;hello , $&#123;name&#125; !&quot;s1=&quot;good&quot;s2=&quot;game&quot;s3=$&#123;s1&#125;$&#123;s2&#125;echo $s3echo $greeting# 循环for skill in Ada Coffe Action Java;do echo &quot;$&#123;skill&#125;script&quot;donefor file in `ls /home`;do echo $filedone# 传递参数echo &quot;参数个数：$#&quot;echo &quot;文件名：$0&quot;echo &quot;如何检查之前的命令是否运行成功：$?&quot;for i in &quot;$@&quot;;do echo $idone# 数组my_arr=(a b c d 1 2 3)for i in $&#123;my_arr[*]&#125;;do echo $idoneecho $&#123;my_arr[*]&#125;;# 基本运算符# + - * / % = == !=val=`expr 2 + 2`echo &quot;两个数和为：$val&quot;;a=10b=20let c=$a + $b #方法1echo $cA=5 B=6 echo $(($A+$B)) # 方法 2 echo $[$A+$B] # 方法 3 expr $A + $B # 方法 4 echo $A+$B | bc # 方法 5awk &apos;BEGIN&#123;print &apos;&quot;$A&quot;&apos;+&apos;&quot;$B&quot;&apos;&#125;&apos; # 方法 6#awk是linux命令，是一个强大的文本分析工具if [ $a == $b ]then echo &quot;a==b&quot;fiif [ $a != $b ]then echo &quot;a!=b&quot;fi# 关系运算符# -eq -ne -gt -lt -ge -leif [ $a -eq $b ]then echo &quot;$a -eq $b : a 等于 b &quot;else echo &quot;$a -eq $b : a 不等于 b&quot;fi# 布尔运算# ！ -o -aif [ $a -lt 100 -o $b -gt 100 ]then echo &quot;$a 小于 100 或者 $b 大于 100： 返回： true&quot;else echo &quot;$a 小于 100 或者 $b 大于100 ；返回 false&quot;fi# 逻辑运算符# &amp;&amp; ||# 跟-a 和 -o 一样# 字符串运算符# = != -z -n strif [ $a = $b ]then echo &quot;$a = $b : a 等于 b&quot;else echo &quot;$a =$b : a 不等于 b&quot;fi# 文件检测运算符# 可以检测文件各种权限和属性if [ -f /var/log/messages]then echo &quot;file exists&quot;fi# 输出echo it is a testecho &quot;\&quot;it is a test\&quot;&quot;# 输出至文件echo &quot;it is a test&quot; &gt; testecho `date`# printf 命令# 一些格式化输出方式# test 命令# 检查 数值 字符 和文件if test $[a] -eq $[num2]then echo &apos;两个数相等&apos;else echo &apos;两个数不想等&apos;fi# 流程控制# 和java php 不一样，sh的流程控制不可为空if [ $a == $b ]then echo &quot;a等于b&quot;elif [ $a -gt $b ]then echo &quot;a大于b&quot;elif [ $a -lt $b ]then echo &quot;a小于b&quot;else echo &quot;没有符合条件&quot;fi# while 语句int=1while(( $int&lt;=5 ))do echo $int let &quot;int++&quot;done # until 循环# 很少用# 无限循环# while:# do# command# done# caseecho &quot;输入1到4之间的数字&quot;echo &apos;你输入的数字为：&apos;read numcase $num in 1) echo &apos;你选择了 1&apos; ;; 2) echo &apos;你选择了 2&apos; ;; 3) echo &apos;你选择了 3&apos; ;; 4) echo &apos;你选择了 4&apos; ;; *) echo &apos;你没有输入 1 到 4 之间的数字&apos; ;;esac# break 和 continue# 只能在 循环中使用 ，case 不可以，case的两个分号就是起到了break的作用# 函数# 可带function fun() 也可 fun()demoFun()&#123; echo &apos;第一个函数&apos;&#125;demoFun# 函数参数# 函数体内部，通过$n的形式获取参数，$1表示第一个参数，# $# $* $$ $! $@ $- $?# 输出重定向echo &apos;imdalai.com&apos; &gt;&gt; test# 重定向将后面的输出的标准输出和标准错误到某文件exec &gt;log.txt 2&gt;&amp;1# 文件包含# . filename 或者 source filename# ---. ./test# ---被包含的文件test不需要可执行权限# export# 使变量在子shell中可用 完结。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell 脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置良好的solr实例]]></title>
    <url>%2Fp%2Fee07.html</url>
    <content type="text"><![CDATA[配置solrconfig.xmlsolrconfig.xml文件是具有影响Solr本身的最多参数的配置文件。 在solrconfig.xml中，您可以配置重要功能，例如： requestHandlers(请求处理程序)，它将请求处理到Solr，例如向索引添加文档的请求或为查询返回结果的请求。 listeners(侦听器)，“侦听”特定的进程 查询相关事件; 侦听器可用于触发特殊代码的执行，例如调用一些常见查询来预热高速缓存。 用于管理HTTP通信的请求分派器。 管理Web界面。 与复制和复制相关的参数（这些参数详细介绍 在传统的规模和分配）。 在Solr配置文件中替换属性JVM系统属性 任何JVM系统属性，通常在启动JVM时使用-D标志指定，可以用作Solr中任何XML配置文件中的变量。 例如，在示例solrconfig.xml文件中，您将看到此值定义要使用的lockType： 1&lt;lockType&gt;$&#123;solr.lock.type:native&#125;&lt;/lockType&gt; 这意味着锁定类型默认为“native”，但是当启动Solr时，您可以使用JVM系统属性重新启动它，方法是启动Solr它： 1bin/solr start -Dsolr.lock.type=name Config API 可以通过config API去修改solr配置文件，用API修改的文件被命名成configoverlay.json。这个文件可以被API编辑，看起来像这样： 1234&#123;&quot;userProps&quot;:&#123; &quot;dih.db.url&quot;:&quot;jdbc:oracle:thin:@localhost:1521&quot;, &quot;dih.db.user&quot;:&quot;username&quot;, &quot;dih.db.pass&quot;:&quot;password&quot;&#125;&#125; core.properties 每个core下的文件夹都有这个文件（core.properties），文件可以使用Java标准属性文件格式包含任意用户定义的属性名称和值,并且这些属性可以用作Solr核心的XML配置文件中的变量。 core.properties文件： 123#core.propertiesname=gettingstartedmy.custom.prop=edismax 在solrconfig.xml文件中使用例子: 12345&lt;requestHandler name=&quot;/select&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;defType&quot;&gt;$&#123;my.custom.prop&#125;&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; core隐藏属性 Solr核心的几个属性可用作“隐含”属性，可用于变量替换，独立于基础值初始化的位置或方式。 例如：无论特定Solr核心的名称是否在core.properties中显式配置，或者从实例目录的名称推断，隐式属性solr.core.name可用作该核心配置文件中的变量。所有隐式属性都使用solr.core. 名称前缀，并反映等效的core.properties属性的运行时值： solr.core.name solr.core.config solr.core.schema solr.core.dataDir solr.core.transient solr.core.loadOnStartup DataDir and DirectoryFactory in SolrConfig Solr存储其索引的位置和方式是可配置的选项。 用参数dataDir指定一个索引数据的位置，例如： 1&lt;dataDir&gt;$&#123;solr.data.dir:&#125;&lt;/dataDir&gt; 为索引指定DirectoryFactory solr.StandardDirectoryFactory是文件系统并尝试挑选当前的最佳实现JVM和平台。 solr.NRTCachingDirectoryFactory，默认，包装solr.StandardDirectoryFactory并将小文件缓存在内存中为了更好的NRT性能。 solr.MMapDirectoryFactory,solr.NIOFSDirectoryFactory或solr.SimpleFSDirectoryFactory可以通过强制实现特定的实现。 solr.RAMDirectoryFactory是基于内存的，而不是持久性，并且不能与复制配合使用。例子： 1234&lt;directoryFactory name=&quot;DirectoryFactory&quot; class=&quot;solr.MMapDirectoryFactory&quot;&gt; &lt;bool name=&quot;preload&quot;&gt;true&lt;/bool&gt;&lt;/directoryFactory&gt;/ Lib Directives in SolrConfig Solr允许通过在solrconfig.xml中定义指令来加载插件。 插件按照它们在solrconfig.xml中显示的顺序加载。如果有依赖关系，首先列出最低级别的依赖项。 正则表达式可用于为同一目录中的其他jar提供依赖的控件加载jar。 所有目录都相对于Solr instanceDir解析。 1&lt;lib dir=&quot;$&#123;solr.install.dir:../../../..&#125;/contrib/extraction/lib&quot; regex=&quot;.*\.jar&quot; /&gt; Schema Factory Definition in SolrConfig Solr 默认使用manage-shcema 当在solrconfig.xml文件中未明确声明时，Solr会隐式使用ManagedIndexSchemaFactory，默认情况下为“mutable”，并将模式信息保存在managedschema文件中。一个Solr隐式默认行为的例子，如果没有明确定义schemaFactory。 1234&lt;schemaFactory class=&quot;ManagedIndexSchemaFactory&quot;&gt; &lt;bool name=&quot;mutable&quot;&gt;true&lt;/bool&gt; &lt;str name=&quot;managedSchemaResourceName&quot;&gt;managed-schema&lt;/str&gt;&lt;/schemaFactory&gt; 如果要明确配置ManagedIndexSchemaFactory，可以使用以下选项： mutable - 控制是否可以对架构数据进行更改。 这必须设置为true，以允许使用Schema API进行编辑。 managedSchemaResourceName是一个默认为“managed-schema”的可选参数，并为架构文件定义了一个新的名称，该名称可以是除“schema.xml”以外的其他文件。使用上面显示的默认配置，您可以使用Schema API根据需要修改模式，然后如果希望将模式“锁定”到位并防止将来更改，那么稍后将mutable的值更改为false。 schema.xml文件 使用managed-schema的另一种方法是显式配置一个ClassicIndexSchemaFactory。 ClassicIndexSchemaFactory需要使用schema.xml配置文件，并且不允许在运行时对模式进行任何程序更改。 必须手动编辑schema.xml文件，仅在加载集合时加载。 1&lt;schemaFactory class=&quot;ClassIndexSchemaFactory&quot;/&gt; 配置模式managed-schema换成schema.xml 如果solr core使用ClassicIndexSchemaFactory，并且配置模式换成schema-managed模式，只要简单的吧solrconfig.xml文件中指定为ManagedIndexSchemaFactory即可。一旦Solr重新启动并且它检测到schema.xml文件存在，但managedSchemaResourceName文件（即：“managed-schema”）不存在，现有的schema.xml文件将被重命名为schema.xml.bak和内容 被重写到managed-schema文件。 如果您查看生成的文件，您将在页面顶部看到它： 1&lt;!-- Solr managed schema - automatically generated - DO NOT EDIT --&gt; 现在，您可以随意使用Schema API进行更改，并删除schema.xml.bak。 配置模式从managed-schema切换到手动编辑的schema.xml 如果已启动managed-schema的Solr，并且要切换到手动编辑schema.xml文件，则应执行以下步骤： 将managed-schema文件重命名为schema.xml。 修改solrconfig.xml以替换schemaFactory类。 删除任何ManagedIndexSchemaFactory定义（如果存在）。 添加一个ClassicIndexSchemaFactory定义，如上所示，重新加载内核。如果您使用SolrCloud，您可能需要通过ZooKeeper修改文件。 bin / solr脚本提供了一种从ZooKeeper下载文件并在编辑后上传它们的简单方法。 有关详细信息，请参阅ZooKeeper操作部分。 IndexConfig in SolrConfigsolrconfig.xml的部分定义了Lucene索引编写器的低级行为。默认情况下，Solr中包含的示例solrconfig.xml中的设置被注释掉，这意味着使用默认值。 在大多数情况下，默认值是正确的。 123&lt;indexConfig&gt; ...&lt;/indexConfig&gt; 1&lt;ramBufferSizeMB&gt;100&lt;ramBufferSizeMB&gt; 一旦累积的文档更新超过了这么多的内存空间（以MB定义），则刷新挂起的更新。 这也可以创建新的段或触发一个合并。 使用此设置通常比maxBufferedDocs更好。 如果在solrconfig.xml中都设置了maxBufferedDocs和ramBufferSizeMB，则在达到任一限制时将会发生刷新。 默认值为100Mb。 1&lt;maxBufferedDocs&gt;1000&lt;/maxBufferedDocs&gt; 将文件更新的数量设置为内存中缓冲区的数量，然后再将其刷新为新的段。 这也可能触发一个合并。 默认的Solr配置设置为通过RAM使用进行刷新（ramBufferSizeMB）。 1&lt;useCompoundFile&gt;false&lt;/useCompoundFile&gt; 控制新写入（尚未合并）索引片段是否应使用复合文件段格式。 默认值为false。 合并索引片段 Solr中的默认值是使用TieredMergePolicy，它合并大小相等的段，但必须符合每层允许的段数。其他可用的策略是LogByteSizeMergePolicy和LogDocMergePolicy。 1234&lt;mergePolicyFactory class=&quot;org.apache.solr.index.TieredMergePolicyFactory&quot;&gt; &lt;int name=&quot;maxMergeAtOnce&quot;&gt;10&lt;/int&gt; &lt;int name=&quot;segmentsPerTier&quot;&gt;10&lt;/int&gt;&lt;/mergePolicyFactory&gt; 对于TieredMergePolicy，这是通过设置和选项来控制的，而LogByteSizeMergePolicy有一个选项。 要了解为什么这些选项很重要，请考虑使用LogByteSizeMergePolicy对索引进行更新时会发生什么：文档总是添加到最近打开的段中。当片段填满时，将创建一个新的片段，并将其后续的更新放在那里。 如果创建新段将导致最低级别段的数量超过mergeFactor值，则所有这些段都将合并在一起以形成单个大段。因此，如果合并因子为10，则每个合并导致创建比其十个组成部分中的每一个大约十倍的单个分段。当这些较大的段中有10个时，它们又被合并成一个更大的单个段。这个过程可以无限期地继续下去。当使用TieredMergePolicy时，该过程是相同的，但是不使用单个mergeFactor值，将segmentPerTier设置用作阈值来决定是否应该进行合并，并且maxMergeAtOnce设置确定合并中应包含多少段。 选择最佳合并因子通常是索引速度与搜索速度的折衷。索引中较少的片段通常会加快搜索速度，因为找不到要查找的地方。它还可以减少磁盘上的物理文件。但是为了保持段数低，合并将更频繁地出现，这可能会增加系统的负载，并减缓对索引的更新。 相反，保留更多细分可以加快索引，因为并发发生的频率较低，使更新不太可能触发合并。但搜索变得更加计算昂贵，可能会更慢，因为搜索字词必须在更多的索引片段中查找。更快的索引更新也意味着更短的提交周转时间，这意味着更及时的搜索结果。 自定义合并策略 如果内置合并策略的配置选项不完全适合您的用例，则可以自定义它们：通过创建在配置中指定的自定义合并策略工厂，或者通过配置使用包装的合并策略wrapped.prefix配置选项来控制如何将其包装的工厂配置： 1234567&lt;mergePolicyFactory class=&quot;org.apache.solr.index.SortingMergePolicyFactory&quot;&gt; &lt;str name=&quot;sort&quot;&gt;timestamp desc&lt;/str&gt; &lt;str name=&quot;wrapped.prefix&quot;&gt;inner&lt;/str&gt; &lt;str name=&quot;inner.class&quot;&gt;org.apache.solr.index.TieredMergePolicyFactory&lt;/str&gt; &lt;int name=&quot;inner.maxMergeAtOnce&quot;&gt;10&lt;/int&gt; &lt;int name=&quot;inner.segmentsPerTier&quot;&gt;10&lt;/int&gt;&lt;/mergePolicyFactory&gt; 上面的示例显示了Solr的SortingMergePolicyFactory被配置为通过“timestamp desc”对合并的段中的文档进行排序，并且包裹在配置为通过SortingMergePolicyFactory的“wrapped.prefix”选项定义的内部前缀maxMergeAtOnce = 10和segmentsPerTier = 10的TieredMergePolicyFactory。 有关使用SortingMergePolicyFactory的更多信息，请参阅segmentTerminateEarly参数。 mergeScheduler 合并调度程序控制如何执行合并。 默认ConcurrentMergeScheduler使用单独的线程在后台执行合并。 另一种方法是SerialMergeScheduler，不会与单独的线程执行合并。 1&lt;mergeScheduler class=&quot;org.apache.lucene.index.ConcurrentMergeScheduler&quot;/&gt; mergedSegmentWarmer 当使用Solr进行近实时搜索合并的分段加热器可以配置为在合并提交之前对新合并的分段进行加热。 这对于近实时搜索来说不是必需的，但是在合并完成之后会打开一个新的近实时阅读器会减少搜索延迟。 1&lt;mergedSegmentWarmer class=&quot;org.apache.lucene.index.SimpleMergedSegmentWarmer&quot;/&gt; 复合文件段 每个Lucene段通常由十几个文件组成。 Lucene可以配置为使用.cfs文件扩展名将段的所有文件捆绑到单个复合文件中。 它是复合文件段的缩写。 索引锁 1&lt;lockType&gt;native&lt;/lockType&gt; LockFactory选项指定要使用的锁定实现。 该组有效的锁类型选项取决于您配置的DirectoryFactory。 下面列出的值由StandardDirectoryFactory（默认值）支持： native（默认）使用NativeFSLockFactory来指定本地操作系统文件锁定。如果第二个Solr进程尝试访问目录，它将失败。 当多个Solr Web应用程序试图共享一个索引时，不要使用。 简单的使用SimpleFSLockFactory指定一个普通文件进行锁定。 single（专家）使用SingleInstanceLockFactory。 用于只读索引目录的特殊情况，或者不可能有多个进程尝试修改索引（甚至顺序）。 此类型将防止同一个JVM中的多个内核尝试访问相同的索引。 警告！ 如果不同JVM中的多个Solr实例修改索引，则此类型将不会防止索引损坏。 hdfs使用HdfsLockFactory来支持将索引和事务日志文件读取和写入HDFS文件系统。 有关使用此功能的详细信息，请参阅在HDFS上运行Solr的部分。 1&lt;writeLockTimeout&gt;1000&lt;/writeLockTimeout&gt; 等待IndexWriter写入锁定的最长时间。 默认值为1000，以毫秒表示。 其它索引设置 还有一些其他参数可能对您的实现进行配置很重要。 这些设置会影响对索引进行更新的方式或时间。 reopenReaders：控制是否重新打开IndexReaders，而不是关闭然后打开，这通常效率较低。 默认值为true。 deletionPolicy：控制在回滚情况下保留提交的方式。 默认值为SolrDeletionPolicy，它具有要保留的最大提交数（MAXCommitsToKeep）的子参数，要保留的优化提交的最大数量（maxOptimizedCommitsToKeep）以及要保留的任何提交的最大时间（maxCommitAge），它支持DateMathParser 句法。 infoStream：InfoStream设置指示底层的Lucene类将索引过程中的详细调试信息写入Solr日志消息。 1234567&lt;reopenReaders&gt;true&lt;/reopenReaders&gt;&lt;deletionPolicy class=&quot;solr.SolrDeletionPolicy&quot;&gt; &lt;str name=&quot;maxCommitsToKeep&quot;&gt;1&lt;/str&gt; &lt;str name=&quot;maxOptimizedCommitsToKeep&quot;&gt;0&lt;/str&gt; &lt;str name=&quot;maxCommitAge&quot;&gt;1DAY&lt;/str&gt;&lt;/deletionPolicy&gt;&lt;infoStream&gt;false&lt;/infoStream&gt; SolrConfig中的RequestHandlers和SearchComponentRequestHandlers处理来到Solr的请求。 这些可能是查询请求或索引更新请求。 您可能需要几个这些定义，这取决于您希望Solr如何处理您将要做的各种请求。SearchComponent是搜索的特征，例如突出显示或刻面。SearchComponent在solrconfig.xml中定义，与RequestHandlers分开，然后根据需要向RequestHandlers注册。 RequestHandlers SearchHandlers 默认情况下用Solr定义的主RequestHandlers是“SearchHandler”，它处理搜索查询。 定义RequestHandlers，然后默认使用一个列表定义RequestHandlers的默认列表。 123456&lt;requestHandler name=&quot;/select&quot; class=&quot;solr.SearchHandler&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;echoParams&quot;&gt;explicit&lt;/str&gt; &lt;int name=&quot;rows&quot;&gt;10&lt;/int&gt; &lt;/lst&gt;&lt;/requestHandler&gt; 此示例将rows参数定义为要返回的搜索结果的数量为“10”。 echoParams参数定义当返回调试信息时，应该返回查询中定义的参数。 还要注意，如果参数是字符串，整数或其他类型，则在列表中定义默认值的方式会有所不同。 搜索部分中描述的所有参数可以被定义为任何SearchHandler的默认值。 UpdateRequestHandlers ShardHandlers Search Components(搜索组件) 默认组件 如果没有定义任何组件（除了第一个组件和最后一个组件 - 见下文），默认情况下将按以下顺序执行：|组件名称|类名||—-|—||query|solr.QueryComponent||facet|solr.FacetComponent||mit|solr.MoreLinkeThisCmponent||highlight|solr.HighlightComponent||stats|solr.StatsComponent||debug|solr.DebugComponent||expand|solr.ExpandComponent|如果您使用这些默认名称之一注册新的搜索组件，则新定义的组件将将替换默认组件执行。 第一组件贺最后组件 可以将某些组件定义为之前（使用第一个组件）或之后（使用最后组件）上面列出的默认组件。 123456&lt;arr name=&quot;first-components&quot;&gt; &lt;str&gt;mycomponent&lt;/str&gt;&lt;/arr&gt;&lt;arr name=&quot;last-components&quot;&gt; &lt;str&gt;spellcheck&lt;/str&gt;&lt;/ar 其它有用的组件 SpellCheckComponent TermVectorComponent QueryElevationComponent TermsComponent InitParams在SolrConfigsolrconfig.xml的部分允许您在处理程序配置之外定义请求处理程序参数。 例如，如果您希望多个搜索处理程序返回相同的字段列表，则可以创建一个部分，而无需在每个请求处理程序定义中定义相同的参数集。 如果您有一个请求处理程序返回不同的字段，那么您可以像往常一样在单个部分中定义覆盖参数。例如，这里是data_driven_config示例中默认定义的部分之一： 12345&lt;initParams path=&quot;/update/**,/query,/select,/tvrh,/elevate,/spell,/browse&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;df&quot;&gt;_text_&lt;/str&gt; &lt;/lst&gt;&lt;/initParams&gt; UpdateHandlers in SolrConfig本节中的设置在solrconfig.xml中的元素中进行配置，可能会影响索引更新的性能。 这些设置会影响内部更新的完成。 配置不影响处理客户端更新请求的RequestHandler的更高级别配置。 Query Settings in SolrConfig本节中的设置会影响Solr将处理和响应查询的方式。 这些设置都在solrconfig.xml中的元素的子元素中进行配置。 123&lt;query&gt; ...&lt;/query&gt; 高速缓存(Caches) 未完，持续更新。。。]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr 文档、字段和schema设计]]></title>
    <url>%2Fp%2F889e.html</url>
    <content type="text"><![CDATA[概述文档，字段和schema设计Solr的基本前提很简单。 你给它很多信息，然后你可以问问题，找到你想要的信息。 您在所有信息中提供的部分称为indexing(索引)或updating(更新)。 当您提出问题时，称为query(查询)。 schema是告诉Solr如何从输入文档构建索引的地方。 Solr的Schema 文件 Solr存储有关在Schema文件中理解的字段类型和字段的详细信息。 此文件的名称和位置可能会有所不同，具体取决于您最初配置Solr的方式或稍后修改它。 managed-schema是Solr默认使用的模式文件的名称，用于支持在运行时通过Schema API或Schemaess Mode功能进行Schema更改。 您可以显式配置托管架构功能以使用替代文件名，如果您选择，但文件的内容仍然由Solr自动更新。 schema.xml是模式文件的传统名称，可以由使用ClassicIndexSchemaFactory的用户手动编辑。 如果使用SolrCloud，您可能无法在本地文件系统上找到这些名称的任何文件。 您只能通过Schema API（如果启用）或通过Solr Admin UI的“云端屏幕”来查看模式。 Solr字段类型 字段类型定义了Solr应如何解释字段中的数据以及如何查询字段。 默认情况下，Solr包含许多字段类型，也可以在本地定义。 字段类型定义和属性字段类型定义当文档被索引或查询发送到索引时将在字段上发生的分析。 字段类型定义可以包括四种类型的信息： 字段类型的名称（必需）。 实现类名称（强制性）。 如果字段类型为TextField，则说明字段类型的字段分析。 字段类型属性，根据实现类，某些属性可能是必需的。 schema.xml中的字段类型定义字段类型定义在schema.xml中。 每个字段类型都在fieldType元素之间定义。 它们可以可选地分组在一个类型元素中。 以下是一个名为text_general的类型的字段类型定义的示例： 12345678910111213&lt;fieldType name=&quot;text_general&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot; multiValued=&quot;true&quot;&gt; &lt;analyzer type=&quot;index&quot;&gt; &lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot;/&gt; &lt;filter class=&quot;solr.StopFilterFactory&quot; words=&quot;stopwords.txt&quot; ignoreCase=&quot;true&quot;/&gt; &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt; &lt;/analyzer&gt; &lt;analyzer type=&quot;query&quot;&gt; &lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot;/&gt; &lt;filter class=&quot;solr.StopFilterFactory&quot; words=&quot;stopwords.txt&quot; ignoreCase=&quot;true&quot;/&gt; &lt;filter class=&quot;solr.SynonymGraphFilterFactory&quot; expand=&quot;true&quot; ignoreCase=&quot;true&quot; synonyms=&quot;synonyms.txt&quot;/&gt; &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; 上面示例中的第一行包含字段类型名称text_general和实现类solr.TextField的名称。定义的其余部分是关于现场分析，在了解分析仪，令牌和过滤器中描述。 实现类负责确保字段处理正确。 在schema.xml中的类名中，字符串solr是org.apache.solr.schema或org.apache.solr.analysis的缩写。 因此，solr.TextField实际上是org.apache.solr.schema.TextField. 字段类型属性字段类型类确定字段类型的大部分行为，但也可以定义可选属性。 例如，日期字段类型的以下定义定义了两个属性sortMissingLast和omitNorms。 12&lt;fieldType name=&quot;date&quot; class=&quot;solr.TrieDateField&quot; sortMissingLast=&quot;true&quot; omitNorms=&quot;true&quot;/&gt; 可以为给定字段类型指定的属性分为三大类： 特定于字段类型类的属性。 常规属性Solr支持任何字段类型。 字段默认属性，可以在字段类型上指定，该类型将由使用此类型而不是默认行为的字段继承。 一般属性 属性 描述 值 name fieldType的名称。 该值在“类型”属性中用于字段定义。强烈建议，名称仅由字母数字或下划线字符组成，而不是以数字开头。 目前尚未严格执行。 class 用于存储和索引此类型的数据的类名称。 请注意，您可以将包含的类名称与“solr”进行前缀。 而Solr会自动找出哪些包来搜索该类-所以solr.TextField将会工作。 如果您正在使用第三方类，则可能需要具有完全限定的类名。 solr.TextField的完全限定等价物是org.apache.solr.schema.TextField。 positionIncrementGap 对于多值字段，指定多个值之间的距离，以防止虚假的短语匹配. integer autoGeneratePhraseQueries 对于text fields, 如果为true，Solr会自动生成相邻项的短语查询。 如果是虚假的，术语必须用双引号括起来作为短语处理。 true or false enableGraphQueries 对于text fields，当使用sow = false进行查询时适用。 例如,对于具有查询分析器的字段类型，使用true（默认值），包括图形感知过滤器。 同义词图形滤波器和字分隔符图形滤波器。 对于具有查询分析器的字段类型，使用false，包括当缺少某些令牌时可以匹配文档的过滤器，例如Shingle Filter。 true or false docValuesFormat 定义用于此类型的字段的自定义DocValuesFormat。 这要求在solrconfig.xml中配置了一个支持架构的编解码器，如SchemaCodecFactory。 n/a postingsFormat 定义用于此类型字段的自定义PostingsFormat。 这要求在solrconfig.xml中配置了一个支持架构的编解码器，如SchemaCodecFactory。 n/a 字段默认属性 这些属性可以在字段类型或各个字段上指定，以覆盖字段类型提供的值。 每个属性的默认值取决于底层的FieldType类，它们又可能取决于的版本属性。 下表包含由Solr提供的大多数FieldType实现的默认值，假定一个schema.xml声明版本=“1.6”。 属性 描述 值 隐含默认值 indexed 如果为true，则该字段的值可用于查询匹配文档的查询。 true or false true stored 如果为ture，则可以通过查询来检索该字段的实际值。 true or false true docValues 如果为true，则该字段的值将被放置在面向列的DocValues结构中。 true or false false sortMissingFirstsortMissingLast 当排序字段不存在时控制文档的放置。 true or false false multiValued 如果为true，则表示单个文档可能包含此字段类型的多个值 true or false flase omitNorms 如果为true，则省略与此字段相关联的规范（这将禁用字段的长度归一化，并节省一些内存）。 所有原始（未分析）字段类型的默认值为true，例如int，float，data，bool和string。 只有全文字段或字段需要规范。 true or false * omitTermFreqAndPositions 如果为真，则从该字段的帖子中省略术语频率，位置和有效载荷。 这可以是不需要该信息的字段的性能提升。 它还减少了索引所需的存储空间。 依赖于使用此选项的字段上发布的位置的查询将无法找到文档。 对于不是文本字段的所有字段类型，此属性默认为true。 true or false * omitPositions 类似于omitTermFreqAndPositions，但保留术语频率信息。 true or false * termVectorstermPositionstermOffsetstermPayloads 这些选项指示Solr维护每个文档的全字段向量，可选地包括这些向量中每个词出现的位置，偏移量和有效载荷信息。 这些可用于加速突出显示和其他辅助功能，但在索引大小方面施加了相当大的成本。 它们对Solr的典型用途不是必需的。 true or false false required 指示Solr拒绝添加不具有该字段值的文档的任何尝试。 此属性默认为false。 true or false false useDocValuesAsStored 如果该字段启用了docValues，则将其设置为true将允许返回该字段，就像在fl参数中匹配“*”时一样，它是一个存储字段（即使它具有存储= false）. true or false true large large字段总是懒惰加载，如果实际值&lt;512KB，将只占用文档缓存中的空间。 此选项需要stored =“true”和multiValued =“false”。 它适用于可能具有非常大的值的字段，以便它们不会缓存在内存中。 true or false false Solr包含的字段类型下表列出了Solr中可用的字段类型。 org.apache.solr.schema包中包含此表中列出的所有类。 类 描述 BinaryField 二进制数据。 BoolField 包含true或false。 第一个字符中的“1”，“t”或“T”的值被解释为真。 第一个字符中的任何其他值都被解释为false。 CollationField 支持排序和范围查询的Unicode归类。 如果可以使用ICU4J，ICUCollationField是一个更好的选择。 请参阅“Unicode排序规则”一节。 CurrencyField 支持货币和汇率。 请参阅“使用货币和汇率”一节。 DateRangeField 支持索引日期范围，包括时间点日期实例（单毫秒持续时间）。 有关使用此字段类型的更多详细信息，请参阅“使用日期”部分。 考虑使用此字段类型，即使它仅适用于日期实例，特别是当查询通常落在UTC年/月/日/小时等边界时。 ExternalFileField 从磁盘上的文件中拉取值。 请参阅使用外部文件和进程一节。 EnumField 允许定义一个枚举的值集合，这些值可能不容易按字母或数字顺序排序（例如，严重性列表）。 此字段类型接受配置文件，其中列出了字段值的正确顺序。 有关详细信息，请参阅使用枚举字段部分。 ICUCollationField 支持排序和范围查询的Unicode归类。 请参阅“Unicode排序规则”一节。 LatLonPointSpatialField 空间搜索：纬度/经度坐标对; 可能多值多点。 通常用逗号指定为“lat，lon”。 LatLonType （已弃用）空间搜索：单值纬度/经度坐标对。 通常用逗号指定为“lat，lon”。 PointType 空间搜索：单值n维点。 它既用于排序不是lat-lon的空间数据，也用于一些更罕见的用例。 （注意：这与“基于点”的数字字段无关） PreAnalyzedField 提供一种发送到Solr序列化令牌流的方式，可选地使用字段的独立存储值，并将此信息存储和编入索引，而不进行任何其他文本处理。 PreAnalyzedField的配置和使用记录在“使用外部文件和进程”页面上。 RandomSortField 不包含值。 对此字段类型进行排序的查询将以随机顺序返回结果。 使用动态字段来使用此功能。 SpatialRecursivePrefixTreeFieldType （简称RPT）空间搜索：以WKT格式接受纬度逗号经度字符串或其他形状。 StrField 字符串（UTF-8编码字符串或Unicode）。 字符串用于小字段，不以任何方式进行标记化或分析。 他们的限制稍低于32K。 TextField 文字，通常是多个单词或令牌。 TrieDateField 日期字段。 以毫秒精度表示一个时间点。 请参阅“使用日期”部分。 precisionStep =“0”最小化索引大小; precisionStep =“8”（默认值）可实现更有效的范围查询。 对于单值字段，请使用docValues =“true”进行高效排序。 TrieDoubleField 双字段（64位IEEE浮点）。 precisionStep =“0”最小化索引大小; precisionStep =“8”（默认值）可实现更有效的范围查询。 对于单值字段，请使用docValues =“true”进行高效排序。 TrieFloatField 浮点字段（32位IEEE浮点）。 precisionStep =“0”可以进行有效的数字排序并最小化索引大小; precisionStep =“8”（默认值）可实现有效的范围查询。 使用docValues =“true”进行有效的排序。 对于单值字段，请使用docValues =“true”进行高效排序。 TrieIntField 整数字段（32位有符号整数）。 precisionStep =“0”可以进行有效的数字排序并最小化索引大小; precisionStep =“8”（默认值）可实现有效的范围查询。 对于单值字段，请使用docValues =“true”进行高效排序。 TrieLongField 长字段（64位有符号整数）。 precisionStep =“0”最小化索引大小; precisionStep =“8”（默认值）可实现更有效的范围查询。 对于单值字段，请使用docValues =“true”进行高效排序。 TrieField 如果使用此字段类型，还必须指定“类型”属性，有效值为：integer，long，float，double，date。 使用此字段与使用上述任何Trie字段相同 DatePointField 日期字段。 以毫秒精度表示一个时间点。 请参阅“使用日期”部分。 该类与TrieDateField类似，但是使用“维度点”的数据结构而不是索引条款，并且不需要配置精确步骤。 对于单值字段，必须使用docValues =“true”来启用排序。 DoublePointField 双字段（64位IEEE浮点）。 该类与TrieDoubleField类似，但是使用基于“维度点”的数据结构而不是索引条款，并且不需要配置精度步骤。 对于单值字段，必须使用docValues =“true”来启用排序。 FloatPointField 浮点字段（32位IEEE浮点）。 该类的功能类似于TrieFloatField，但是使用基于“维度点”的数据结构而不是索引项，并且不需要配置精确步骤。 对于单值字段，必须使用docValues =“true”来启用排序。 IntPointField 整数字段（32位有符号整数）。 该类的功能类似于TrieIntField，但是使用基于“维度点”的数据结构而不是索引项，并且不需要配置精确步骤。 对于单值字段，必须使用docValues =“true”来启用排序。 LongPointField 长字段（64位有符号整数）。 该类与TrieLongField类似，但使用基于“维度点”的数据结构而不是索引条款，并且不需要配置精确步骤。 对于单值字段，必须使用docValues =“true”来启用排序。 UUIDField Universally Unique Identifier (UUID). 传递值为“NEW”，Solr将创建一个新的UUID。 注意：在使用SolrCloud时，大多数用户不建议使用默认值“NEW”的UUIDField实例（如果UUID值被配置为唯一键字段则不可行），因为结果将是每个文档的每个副本 将获得唯一的UUID值。 建议使用UUIDUpdateProcessorFactory在添加文档时生成UUID值。 定义字段字段在schema.xml的fields元素中定义。一旦设置了字段类型，定义字段本身就很简单。示例以下示例定义一个名为price的名为float的字段，默认值为0.0; 索引和存储的属性显式设置为true，而在float字段类型上指定的任何其他属性都将被继承。 1&lt;field name=&quot;price&quot; type=&quot;float&quot; default=&quot;0.0&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; 复制字段您可能想要以多种方式解释一些文档字段。 Solr具有制作字段副本的机制，以便可以将多个不同的字段类型应用于单个传入信息。 您要复制的字段的名称是源，并且副本的名称是目标。 在schema.xml中，复制字段非常简单： 1&lt;copyField source=&quot;cat&quot; dest=&quot;text&quot; maxChars=&quot;30000&quot; /&gt; 动态字段动态字段允许Solr索引您在模式中未明确定义的字段。 如果您发现自己忘记定义一个或多个字段，这将非常有用。 通过在可以添加到Solr的文档中提供一些灵活性，动态字段可以使您的应用程序变得更不易碎。 动态字段就像一个常规字段，除了它有一个带有通配符的名称。 当您索引文档时，与任何明确定义的字段不匹配的字段可以与动态字段匹配。 例如，假设您的架构包含名称为* _i的动态字段。 如果您尝试使用cost_i字段索引文档，但在模式中未定义明确的cost_i字段，那么cost_i字段将为* _i定义字段类型和分析。 像常规字段一样，动态字段具有名称，字段类型和选项。 1&lt;dynamicField name=&quot;*_i&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; schema其它元素Unique KeyuniqueKey元素指定哪个字段是文档的唯一标识符。 虽然uniqueKey不是必需的，但几乎总是由您的应用程序设计保证。 例如，如果您将更新索引中的文档，则应使用uniqueKey。 1&lt;uniqueKey&gt;id&lt;/uniqueKey&gt; Schema的默认值和copyFields不能用于填充uniqueKey字段。 uniqueKey的fieldType不能被分析。 您可以使用UUIDUpdateProcessorFactory来自动生成uniqueKey值。 此外，如果使用uniqueKey字段，但是多值（或从fieldtype继承多值），则操作将失败。 但是，唯一的键将继续工作，只要该字段正确使用。 默认搜索字段和查询运算符虽然它们已经被弃用了相当长的一段时间，但是Solr仍然支持基于Schema的配置，一个（被df参数取代）和（被q取代 .op参数。 Similarity相似性(Similarity)是用于在搜索中得分文档的Lucene类。 每个集合都有一个“全局”相似性，默认情况下，Solr使用一个隐式SchemaSimilarityFactory，它允许使用“每类型”特定的相似性来配置各个字段类型，并且对于没有显式相似性的任何字段类型隐式使用BM25相似性。 通过声明schema.xml中的顶级元素，可以覆盖此默认行为，不在任何单个字段类型之外。这种相似性声明可以直接引用具有无参数构造函数的类的名称，例如在本示例中显示BM25Similarity： 1234567&lt;similarity class=&quot;solr.BM25SimilarityFactory&quot;/&gt;&lt;similarity class=&quot;solr.DFRSimilarityFactory&quot;&gt; &lt;str name=&quot;basicModel&quot;&gt;P&lt;/str&gt; &lt;str name=&quot;afterEffect&quot;&gt;L&lt;/str&gt; &lt;str name=&quot;normalization&quot;&gt;H2&lt;/str&gt; &lt;float name=&quot;c&quot;&gt;7&lt;/float&gt;&lt;/similarity&gt; Schema API作为维护暂时没用到就不写了。 完结。]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr 快速入门]]></title>
    <url>%2Fp%2F2bc5.html</url>
    <content type="text"><![CDATA[Solr 简介维基百科的简介： Solr（读作“solar”）是Apache Lucene项目的开源企业搜索平台。其主要功能包括全文检索、命中标示、分面搜索、动态聚类、数据库集成，以及富文本（如Word、PDF）的处理。Solr是高度可扩展的，并提供了分布式搜索和索引复制。Solr是最流行的企业级搜索引擎，Solr 4还增加了NoSQL支持。Solr是用Java编写、运行在Servlet容器（如Apache Tomcat或Jetty）的一个独立的全文搜索服务器。 Solr采用了Lucene Java搜索库为核心的全文索引和搜索，并具有类似REST的HTTP/XML和JSON的API。 Solr强大的外部配置功能使得无需进行Java编码，便可对其进行调整以适应多种类型的应用程序。Solr有一个插件架构，以支持更多的高级定制。因为2010年Apache Lucene和Apache Solr项目合并，两个项目是由同一个Apache软件基金会开发团队制作实现的。提到技术或产品时，Lucene/Solr或Solr/Lucene是一样的。 其实说白了，Solr跟数据库服务器真的很像。 以下是Solr可能集成到应用服务器的示例(来自官网) 在上述情况下，Solr沿其他服务器应用程序运行。 例如，在线商店应用将提供用户界面，购物车以及为最终用户购买的方式; 而库存管理应用程序将允许商店员工编辑产品信息。 产品元数据将保存在某种数据库以及Solr中。 Solr可以轻松添加通过以下步骤在线商店中搜索的功能： 定义schema： 该schema告诉Solr关于它将被索引的文档的内容（原文易理解： The schema tells Solr about the contents of documents it will be indexing.）。 在在线商店示例中，模式将定义产品名称，描述，价格，制造商等的字段。 Solr的模式是强大而灵活的，可以让您根据自己的应用定制Solr的行为。 有关详细信息，请参阅文档，字段和schema设计。 部署Solr。 您的用户将搜索的Feed Solr文档。 在您的应用程序中公开搜索功能。 安装 SolrJava环境Windows和Linux 都需要先安装Java环境，自行google、baidu. windows10 安装 SolrSolr6.6.0官方下载地址 Windows10 用户下载solr-6.6.0.zip版本,然后解压到想放置solr的文件夹。 解压完就已经好了。 centos7 安装 Solr通过wget命令下载solr-6.6.0.tgz版本的压缩包： 12cd /homewget https://mirrors.tuna.tsinghua.edu.cn/apache/lucene/solr/6.6.0/solr-6.6.0.tgz 下载完创建文件夹并压缩： 12cd /hometar zxf solr-6.6.0.tgz 解压完就可以了。 运行 Solrwindows10 运行Solr 服务器命令行里cd到solr6.6.0当前目录下，输入bin\solr.cmd start,服务器就运行了。 centos7 运行Solr 服务器命令行里cd到solr6.6.0当前目录下，输入bin/solr start(如果是root用户需要后面家 -force 命令),服务器就运行了。 Solr 脚本选项下面只展示centos7.2的命令(windows10很大差别会说明) 查看solr 命令 1bin/solr -help 运行/停止在不同的端口，默认端口是8983 12bin/solr start -p 8984bin/solr stop -p 8983 windows10 停止可能后面加-all。 捆绑core列子来启动服务器 1bin/solr -e techproducts 查看Solr状态 1bin/solr status 浏览器中输入http://localhost:8983/solr/来查看web管理界面。 创建一个core 1bin/solr create -c &lt;name&gt; 查看帮助同样 1bin/solr create -help Solr上传文档centos7环境下post提交，gettingstarted 换成想要上传的core的名字 1bin/post -c gettingstarted example/exampledocs/*.xml window环境下的post提交，gettingstarted 换成想要上传的core的名字 1java -Dc=gettingstarted -jar example/exampledocs/post.jar .\example\exampledocs\*.xml 下面是我的截图： 现在上传完文档就有索引文档了，可以通过GET请求来访问，会返回xml文档。 1http://localhost:8983/solr/gettingstarted/select?q=video gettingstarted替换成自己的core名字。 进一步介绍 Solr配置这部分主要介绍主目录和其它的配置文件。 Solr有两种模式该示例显示了Solr的主目录的关键目录。 独立模式（就是单独solr服务器） 12345678&lt;solr-home-directory&gt;/(在solr-6.6.0/server/solr/) solr.xml core_name1/ core.properties conf/ solrconfig.xml managed-schema data/ SolrCloud模式（分布式，集群） 12345&lt;solr-home-directory&gt;/(在solr-6.6.0/server/solr/) solr.xml core_name1/ core.properties data/ 你可能见到了其它的目录，但是你需要知道的主要目录是： solr.xml : 是solr服务器实例的配置文件。 对于每个solr core： core.properties: 定义每个核心的特定属性，如其名称，核心所属的集合，模式的位置以及其他参数。 solrconfig.xml ：控制高级别的行为。 例如，您可以指定的data目录的备用位置。 managed-schema(或schema.xml) : 描述了您要求Solr索引的文档。 schema将文档定义为字段的集合。 您可以自定义字段类型和字段。 字段类型定义很强大，包括有关Solr如何处理传入字段值和查询值的信息。 data/ : 该目录包含低级索引文件。 请注意SolrCloud示例不包含每个Solr Core的conf目录（因此没有solrconfig.xml或Schema文件）。 这是因为通常在conf目录中找到的配置文件存储在ZooKeeper中，以便它们可以跨集群传播。 Solr 脚本详细介绍以下全部centos7下的命令，Windows10的差异不大。 启动/停止Solr启动/重新启动Solr1234bin/solr start [options]bin/solr start -helpbin/solr restart [options]bin/solr restart -help bin / solr脚本提供了许多选项，允许您以常用方式定制服务器，例如更改侦听端口。 然而，大多数默认值对于大多数Solr安装是足够的，特别是刚开始使用时。 可选参数（这里直接附上官方文档） 设置Java系统属性bin / solr脚本将以-D开头的任何其他参数传递给JVM，这允许您设置任意的Java系统属性。 1bin/solr start -Dsolr.autoSoftCommit.maxTime=3000 SolrCloud模式 12bin/solr start -cbin/solr start -cloud -c 和- cloud一样。如果指定ZooKeeper连接字符串，例如-z 192.168.1.4:2181，则Solr将连接到ZooKeeper并加入集群。如果在Cloud模式下启动Solr时不指定-z选项，则Solr将启动一个嵌入式的ZooKeeper服务器，监听Solr端口+ 1000，即，如果Solr在端口8983上运行，则嵌入的ZooKeeper将监听 9983。 用示例启动Solr 示例配置允许您使用镜像，您希望使用Solr完成的配置进行快速入门。 1bin/solr start -e &lt;name&gt; 提供以下示例： cloud ：此示例在单个机器上启动1-4节点SolrCloud群集。选择时，交互式会话将开始引导您选择要使用的初始配置集，示例集群的节点数，要使用的端口以及要创建的集合的名称。使用此示例时，可以从$ SOLR_HOME / server / solr / configsets中找到的任何可用配置中进行选择。 techproducts ：此示例以独立模式启动Solr，并为包含在$ SOLR_HOME / example / exampledocs目录中的示例文档设计的模式启动。所使用的配置集可以在$ SOLR_HOME / server / solr / configsets / sample_techproducts_configs中找到。 dih：该示例以独立模式启动DataImportHandler（DIH）启用，并为DIH支持的不同类型的数据预先配置了几个示例dataconfig.xml文件（例如数据库内容，电子邮件，RSS源等） 。所使用的配置集定制为DIH，并在$ SOLR_HOME / example / example-DIH / solr / conf中找到。有关DIH的更多信息，请参阅使用数据导入处理程序上传结构化数据存储数据一节。 schemaless：此示例使用托管模式在独立模式下启动Solr，如SolrConfig中的“模式工厂定义”一节所述，并提供了非常小的预定义模式。 Solr将以这种配置运行在Schemaless模式中，Solr将立即在架构中创建字段，并且将猜测在传入文档中使用的字段类型。所使用的配置集可以在$ SOLR_HOME / server / solr / configsets / data_driven_schema_configs中找到。 停止Solr停止正在运行的Solr服务器 12bin/solr stop [options]bin/solr stop -help 可选参数 Solr系统信息Solr版本1bin/solr version Solr状态1bin/solr status 5.2.3 Solr检查12bin/solr healthcheck [options]bin/solr healthcheck -help 可选参数 Collections and Cores(集合和核心)创建Collections and Cores12bin/solr create [options]bin/solr create -help 可选参数 删除12bin/solr delete [options]bin/solr delete -help 可选参数 完结。]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr 完整学习目录]]></title>
    <url>%2Fp%2F96dd.html</url>
    <content type="text"><![CDATA[官方文档Apache Solr Reference Guide看他人多少博客都不如看官方文档。 环境介绍本人搭建用到了两种环境(在快速入门中介绍)： windows10 + solr6.6.0 centos7.2 + solr6.6.0 Solr6.6.0 学习目录 快速入门 升级solr 使用Solr Web管理用户界面。 文档，字段和Schema设计 了解Analyzers, Tokenizers, and Filters 索引和基本数据操作 搜索 配置良好的Solr实例 管理Solr SolrCloud 传统规模和分布 客户端APIs 从Solr 5 到Solr 6的重大变化 升级Solr群集 遇到的问题列表[问题]solr6 error opening new searcher [问题]solr6 没有schema.xml只有managed-schema文件 [问题]windows环境下solr post命令提交 到这里就全部介绍完了，有什么错误、不完善之处请指教，共同进步！ 持续更新。。。]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es java client 教程]]></title>
    <url>%2Fp%2F5875.html</url>
    <content type="text"><![CDATA[个人学习elasticSearch 5 的过程分享，至于实现代码后期有空再整理，很多东西可以从文档中找到的，拿过来用即可。 学习elasticSearch官方文档elasticSearch权威指南 中文版 elasticSearch java客户端 elasticSearch英文文档 说明：我用的是ES 5.0.0版本，以上文档除了权威指南，其它是ES 5.0.0版本，但是进去后可以选择切换成其它版本。elasticSearch权威指南(中文版)是1.4.X版本的，读着了解基本流程、操作很有用，有什么疑惑就查看对应的英文版。还有1.4.X、2.X和5.X的连接方式都不同，查看对应的版本文档，并最好使用对应的jar包。 特别声明 如果ELK加了search guard，那连接ES时步骤将不一样，需要验证ssl。 es java client简单实现用search guard 连接 es 的简单的es java client实现代码分享。node-1-keystore.jks，truststore.jks这些文件从部署elk的管理员那儿索要。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122import java.net.InetAddress;import java.util.ArrayList;import org.apache.log4j.Logger;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.InetSocketTransportAddress;import org.elasticsearch.search.sort.FieldSortBuilder;import org.elasticsearch.search.sort.SortBuilders;import org.elasticsearch.search.sort.SortOrder;import org.elasticsearch.transport.client.PreBuiltTransportClient;import com.floragunn.searchguard.ssl.SearchGuardSSLPlugin;import com.floragunn.searchguard.ssl.util.SSLConfigConstants;public class ESClientHelper &#123; static Logger LOG = Logger.getLogger(ESClientHelper.class); static final String INDEX = "index-*"; static final int SIZE = 1000; private Settings.Builder settingsBuilder; private Settings settings; private TransportClient client; @SuppressWarnings(&#123; "resource", "unchecked" &#125;) public ESClientHelper() &#123; try &#123; settingsBuilder = Settings.builder().put("path.home", ".").put("cluster.name", "elasticSearch") .put("client.transport.sniff", true) .put(SSLConfigConstants.SEARCHGUARD_SSL_HTTP_ENABLE_OPENSSL_IF_AVAILABLE, true) .put(SSLConfigConstants.SEARCHGUARD_SSL_TRANSPORT_ENABLE_OPENSSL_IF_AVAILABLE, true) // .put(SSLConfigConstants.SEARCHGUARD_SSL_TRANSPORT_ENABLE_OPENSSL_IF_AVAILABLE,true) // .put(SSLConfigConstants.SEARCHGUARD_SSL_TRANSPORT_KEYSTORE_ALIAS, "node-1") .put("searchguard.ssl.transport.enabled", true) // .put("searchguard.ssl.transport.keystore_filepath", // "/data/hexl/logstash/deleteIndex/conf/node-1-keystore.jks") // .put("searchguard.ssl.transport.truststore_filepath", // "/data/hexl/logstash/deleteIndex/conf/truststore.jks") .put("searchguard.ssl.transport.keystore_filepath", "D:/code/node-1-keystore.jks") //.put("searchguard.ssl.transport.keystore_filepath", "D:/code/node-2-keystore.jks") .put("searchguard.ssl.transport.truststore_filepath", "D:/code/truststore.jks") .put("searchguard.ssl.transport.keystore_password", "password") .put("searchguard.ssl.transport.truststore_password", "password") .put("searchguard.ssl.transport.enforce_hostname_verification", false); settings = settingsBuilder.build(); client = new PreBuiltTransportClient(settings, SearchGuardSSLPlugin.class) .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("127.0.0.1"), 9300)); //这里默认客户端端口是9300，网页默认端口是9200 &#125; catch (Exception e) &#123; LOG.error("create ESClientHelper failed!"); &#125; &#125; /** * 获取数据，分页操作 * * @param indexStr * @param fromN * @param sizeN */ public ArrayList&lt;String&gt; getResponse(int fromN, int sizeN) &#123; try &#123; FieldSortBuilder sortBuilders = SortBuilders.fieldSort("@timestamp").order(SortOrder.ASC); SearchResponse response = client.prepareSearch(INDEX).addSort(sortBuilders).setFrom(fromN).setSize(sizeN) .execute().actionGet(); System.out.println(response.getHits().getHits().length); return getContentList(response); &#125; catch (Exception e) &#123; LOG.error("getResponse failed, the parameter: " + "," + fromN + "," + sizeN); &#125; return null; &#125; /** *获取某个字段，我的是content字段 * * @param response * @return */ private ArrayList&lt;String&gt; getContentList(SearchResponse response) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); try &#123; if (response == null) return null; for (int idx = 0, len = response.getHits().getHits().length; idx &lt; len; idx++) &#123; list.add(response.getHits().getAt(idx).getSource().get("content").toString()); &#125; &#125; catch (Exception e) &#123; LOG.error("response transport to list failed!"); &#125; return list; &#125; public void closeClient() &#123; try &#123; client.close(); &#125; catch (Exception e) &#123; LOG.error("close ES java client failed!"); &#125; &#125; //test main public static void main(String[] args) &#123; System.out.println(new ESClientHelper().getResponse(0, 1000).toString()); &#125;&#125; 完结。]]></content>
      <categories>
        <category>elasticSearch</category>
      </categories>
      <tags>
        <tag>官方文档</tag>
        <tag>java client</tag>
        <tag>elasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUnit 单元测试]]></title>
    <url>%2Fp%2F591c.html</url>
    <content type="text"><![CDATA[所有代码上线之前开发人员都要做单元测试的，很重要！！！本文就来学习学习使用JUnit编写单元测试。 概述文中例子用的是Eclipse，对于IntelliJ IDEA也一样，万变不离其宗。 JUnit下载 JUnit 是一个 Java 编程语言的单元测试框架。JUnit 在测试驱动的开发方面有很重要的发展，是起源于 JUnit 的一个统称为 xUnit 的单元测试框架之一。 JUnit 促进了“先测试后编码”的理念，强调建立测试数据的一段代码，可以先测试，然后再应用。 特点： JUnit 是一个开放的资源框架，用于编写和运行测试。 提供注释来识别测试方法。 提供断言来测试预期结果。 提供测试运行来运行测试。 JUnit 测试允许你编写代码更快，并能提高质量。 JUnit 优雅简洁。没那么复杂，花费时间较少。 JUnit 测试可以自动运行并且检查自身结果并提供即时反馈。所以也没有必要人工梳理测试结果的报告。 JUnit 测试可以被组织为测试套件，包含测试用例，甚至其他的测试套件。JUnit 测试分类 测试分类是在编写和测试 JUnit 的重要分类。几种重要的分类如下： 包含一套断言方法的测试断言 包含规定运行多重测试工具的测试用例 包含收集执行测试用例结果的方法的测试结果 基本用法（简单示例）需要测试的类需要测试的类（LengthOfLongestSubstring），是LeetCode一个算法实现类。后面的所有用例都会用这个类来进行测试。 123456789101112131415161718192021222324252627282930313233343536//3. Longest Substring Without Repeating Characterspublic class LengthOfLongestSubstring &#123; /* * Given a string, find the length of the longest substring without repeating * characters. * * Examples: * * Given "abcabcbb", the answer is "abc", which the length is 3. * * Given "bbbbb", the answer is "b", with the length of 1. * * Given "pwwkew", the answer is "wke", with the length of 3. Note that the * answer must be a substring, "pwke" is a subsequence and not a substring. */ public int lengthOfLongestSubstring(String s) &#123; if (s == null || s.length() == 0) return 0; LinkedList&lt;Integer&gt; linkedList = new LinkedList&lt;&gt;(); int length = 0; for (int i = 0, len = s.length(); i &lt; len; i++) &#123; int number = Integer.valueOf(s.charAt(i)); if (linkedList.contains(number)) &#123; length = linkedList.size() &gt; length ? linkedList.size() : length; linkedList.subList(0, linkedList.indexOf(number) + 1).clear(); &#125; linkedList.addLast(number); &#125; return linkedList.size() &gt; length ? linkedList.size() : length; &#125;&#125; 编写测试用例File-&gt;New-&gt;JUnit Test Case新建后会显示下面窗口点击Next。里面实现算法的一个方法和main方法，如果有很多方法需要测试，直接勾上就行。点击Finish完成。编写测试用例如下： 12345678910111213141516171819202122import static org.junit.Assert.*;import org.junit.Test;import org.junit.runner.JUnitCore;import org.junit.runner.Result;import org.junit.runner.notification.Failure;import junit.TestLeetCodeItem;public class LengthOfLongestSubstringTest &#123; @Test public void testLengthOfLongestSubstring() &#123; assertEquals(3, new LengthOfLongestSubstring().lengthOfLongestSubstring("abcabcbb")); &#125; public static void main(String[] args) &#123; Result result = JUnitCore.runClasses(LengthOfLongestSubstringTest.class); for (Failure failure : result.getFailures()) &#123; System.out.println(failure.toString()); &#125; System.out.println(result.wasSuccessful()); &#125;&#125; 这里用到了断言和JUnitCore测试器。断言可以输入预期结果跟真实结果做比较来测试，JUnitCore。runClasses方法会执行传参进去的LengthOfLongestSubstringTest.class类的所有测试类。更多详情看API部分。 JUnit中的重要的 APIJUnit 中的最重要的程序包是 junit.framework 它包含了所有的核心类。一些重要的类列示如下： Assert : assert方法的集合 TestCase : 一个定义了运行多重测试的固定装置 TestResult : TestResult集合了执行测试样例的所有结果 TestSuite : TestSuite 是测试的集合 Assert类下面介绍的是 org.junit.Assert 类： 1public class Assert extends java.lang.Object 这个类提供了一系列的编写测试的有用的声明方法。只有失败的声明方法才会被记录。Assert 类的重要方法列式如下： method desc void assertEquals(boolean expected, boolean actual) 检查两个变量或者等式是否平衡 void assertFalse(boolean condition) 检查条件是假的 void assertNotNull(Object object) 检查对象不是空的 void assertNull(Object object) 检查对象不是空的 void assertTrue(boolean condition) 检查条件为真 void assertNotNull(Object object) 检查对象不是空的 void fail() 在没有报告的情况下使测试不通过 TestCase 类下面介绍的是 org.junit.TestCaset 类： 1public abstract class TestCase extends Assert implements Test 测试样例定义了运行多重测试的固定格式。TestCase 类的一些重要方法列式如下： method desc int countTestCases() 创建一个默认的 TestResult 对象 TestResult createResult() 创建一个默认的 TestResult 对象 String getName() 获取 TestCase 的名称 TestResult run() 一个运行这个测试的方便的方法，收集由TestResult 对象产生的结果 void run(TestResult result) 在 TestResult 中运行测试案例并收集结果 void setName(String name) 设置 TestCase 的名称 void setUp() 创建固定装置，例如，打开一个网络连接 void tearDown() 拆除固定装置，例如，关闭一个网络连接 String toString() 返回测试案例的一个字符串表示 附上一个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import org.junit.Before;import org.junit.Test;import org.junit.runner.JUnitCore;import org.junit.runner.Result;import org.junit.runner.notification.Failure;import junit.framework.TestCase;public class MyTestCase extends TestCase &#123; protected double fValue1; protected double fValue2; @Before public void setUp() &#123; fValue1 = 2.0; fValue2 = 3.0; &#125; @Test public void testAdd() &#123; // count the number of test cases System.out.println("No of Test Case = " + this.countTestCases()); // test getName String name = this.getName(); System.out.println("Test Case Name = " + name); // test setName this.setName("testNewAdd"); String newName = this.getName(); System.out.println("Updated Test Case Name = " + newName); //print fValue1 System.out.println(fValue1); &#125; // tearDown used to close the connection or clean up activities public void tearDown() &#123; &#125; public static void main(String[] args) &#123; Result result = JUnitCore.runClasses(MyTestCase.class); for (Failure failure : result.getFailures()) &#123; System.out.println(failure.toString()); &#125; System.out.println(result.wasSuccessful()); &#125;&#125; 输出结果： 1234No of Test Case = 1Test Case Name = testAddUpdated Test Case Name = testNewAdd2.0 TestResult 类下面定义的是 org.junit.TestResult 类： 1public class TestResult extends Object TestResult 类收集所有执行测试案例的结果。它是收集参数层面的一个实例。这个实验框架区分失败和错误。失败是可以预料的并且可以通过假设来检查。错误是不可预料的问题就像 ArrayIndexOutOfBoundsException。TestResult 类的一些重要方法列式如下：| method | desc || ——————————————————– |:————————-:|| void addError(Test test, Throwable t) | 在错误列表中加入一个错误 || void addFailure(Test test, AssertionFailedError t) | 在失败列表中加入一个失败 || void endTest(Test test) | 显示测试被编译的这个结果 || int errorCount() | 获取被检测出错误的数量 || Enumeration errors() | 返回错误的详细信息 || int failureCount() | 获取被检测出的失败的数量 || void run(TestCase test) | 运行 TestCase || int int runCount() | 获得运行测试的数量 || void startTest(Test test) | 声明一个测试即将开始 || void stop() | 标明测试必须停止 | TestSuite 类下面定义的是 org.junit.TestSuite 类： 1public class TestSuite extends Object implements Test TestSuite 类是测试的组成部分。它运行了很多的测试案例。TestSuite 类的一些重要方法列式如下：|方法和描述||—||void addTest(Test test)|在套中加入测试。||void addTestSuite(Class&lt;? extends TestCase&gt; testClass)|将已经给定的类中的测试加到套中。||int countTestCases()|对这个测试即将运行的测试案例进行计数。||String getName()|返回套的名称。||void run(TestResult result)|在 TestResult 中运行测试并收集结果。||void setName(String name)|设置套的名称。||Test testAt(int index)|在给定的目录中返回测试。||int testCount()|返回套中测试的数量。||static Test warning(String message)|返回会失败的测试并且记录警告信息。| 更多API请看JUnit API在线查看 注解注释就好像你可以在你的代码中添加并且在方法或者类中应用的元标签。JUnit 中的这些注释为我们提供了测试方法的相关信息，哪些方法将会在测试方法前后应用，哪些方法将会在所有方法前后应用，哪些方法将会在执行中被忽略。JUnit 中的注释的列表以及他们的含义：|注解和描述||—||@Test|这个注释说明依附在 JUnit 的 public void 方法可以作为一个测试案例。||@Before|有些测试在运行前需要创造几个相似的对象。在 public void 方法加该注释是因为该方法需要在 test 方法前运行||@After|如果你将外部资源在 Before 方法中分配，那么你需要在测试运行后释放他们。在 public void 方法加该注释是因为该方法需要在 test 方法后运行。||@BeforeClass|在 public void 方法加该注释是因为该方法需要在类中所有方法前运行。||@AfterClass|它将会使方法在所有测试结束后执行。这个可以用来进行清理活动。||@Ignore|这个注释是用来忽略有关不需要执行的测试的。| 执行过程新建一个JUnit Test Case,如图： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import static org.junit.Assert.*;import org.junit.After;import org.junit.AfterClass;import org.junit.Before;import org.junit.BeforeClass;import org.junit.Ignore;import org.junit.Test;import org.junit.runner.JUnitCore;import org.junit.runner.Result;import org.junit.runner.notification.Failure;import com.dalai.pra.LengthOfLongestSubstring;public class LengthOfLongestSubstringTest2 &#123; @Test public void test_LengthOfLongestSubstring() &#123; assertEquals(3, new LengthOfLongestSubstring().lengthOfLongestSubstring("abcabcbb")); System.out.println("in test01"); &#125; @Test public void test() &#123; System.out.println("in test02"); &#125; // execute before class @BeforeClass public static void beforeClass() &#123; System.out.println("in before class"); &#125; // execute after class @AfterClass public static void afterClass() &#123; System.out.println("in after class"); &#125; // execute before test @Before public void before() &#123; System.out.println("in before"); &#125; // execute after test @After public void after() &#123; System.out.println("in after"); &#125; // test case ignore and will not execute @Ignore public void ignoreTest() &#123; System.out.println("in ignore test"); &#125; public static void main(String[] args) &#123; Result result = JUnitCore.runClasses(LengthOfLongestSubstringTest2.class); for (Failure failure : result.getFailures()) &#123; System.out.println(failure.toString()); &#125; System.out.println(result.wasSuccessful()); &#125;&#125; 执行结果： 12345678in before classin beforein test01in afterin beforein test02in afterin after class 可以在需要测试方法前后都可以执行一些方法。 时间测试只要在写注解Test时： 12345@Test(timeout=1000)public void testPrintMessage() &#123; System.out.println("Inside testPrintMessage()"); messageUtil.printMessage();&#125; 异常测试12345@Test(expected = ArithmeticException.class)public void testPrintMessage() &#123; System.out.println("Inside testPrintMessage()"); messageUtil.printMessage();&#125; 参数化测试Junit 4 引入了一个新的功能参数化测试。参数化测试允许开发人员使用不同的值反复运行同一个测试。你将遵循5个步骤来创建参数化测试： 用 @RunWith(Parameterized.class) 来注释 test 类。 创建一个由 @Parameters 注释的公共的静态方法，它返回一个对象的集合(数组)来作为测试数据集合。 创建一个公共的构造函数，它接受和一行测试数据相等同的东西。 为每一列测试数据创建一个实例变量。 用实例变量作为测试数据的来源来创建你的测试用例。 一旦每一行数据出现测试用例将被调用。让我们看看活动中的参数化测试。 1234567891011121314151617181920212223242526272829303132333435363738394041import static org.junit.Assert.*;import java.util.Arrays;import java.util.Collection;import org.junit.Test;import org.junit.runner.JUnitCore;import org.junit.runner.Result;import org.junit.runner.RunWith;import org.junit.runner.notification.Failure;import org.junit.runners.Parameterized;import com.dalai.pra.LengthOfLongestSubstring;@RunWith(Parameterized.class)public class LengthOfLongestSubstringTest &#123; private String inputString; private int expectedResult; @Test public void testLengthOfLongestSubstring() &#123; assertEquals(expectedResult, new LengthOfLongestSubstring().lengthOfLongestSubstring(inputString)); &#125; public LengthOfLongestSubstringTest(String inputStr, int expectedResult) &#123; this.inputString = inputStr; this.expectedResult = expectedResult; &#125; @Parameterized.Parameters public static Collection primeNumbers() &#123; return Arrays.asList(new Object[][] &#123; &#123; "abcabcbb", 3 &#125;, &#123; "bbbbbb", 1 &#125;, &#123; "asdfga", 5 &#125; &#125;); &#125; public static void main(String[] args) &#123; Result result = JUnitCore.runClasses(LengthOfLongestSubstringTest.class); for (Failure failure : result.getFailures()) &#123; System.out.println(failure.toString()); &#125; System.out.println(result.wasSuccessful()); &#125;&#125; 其它问题StackOverFlow上JUnit问题排行]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jUnit</tag>
        <tag>单元测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer java实现]]></title>
    <url>%2Fp%2F279d.html</url>
    <content type="text"><![CDATA[剑指offer 66道题实现全部代码地址 涉及到的数据结构与算法数据结构：数组、字符串、哈希表、二叉树、栈、队列、链表、堆 算法：归并、动态规划、深度优先搜索（DFS）、广度优先搜索（BFS）、回溯、递归、二分查找 目录 二维数组中的查找问题替换空格从尾到头打印链表重建二叉树用两个栈实现队列旋转数组的最小数字斐波那契数列跳台阶变态跳台阶矩阵覆盖二进制中的1的个数数值的整数次方调整数组顺序使奇数位于偶数前面链表中倒数第k个节点反转链表合并两个排序的链表树的子结构二叉树的镜像顺时针打印矩阵包含min函数的栈栈的压入、弹出序列从上往下打印二叉树二叉搜索树的后序遍历序列二叉树中和为某一值的路径复杂链表复制二叉搜索树与双向链表字符串的排列数组中出现次数超过一半的数字最小的k个数连续子数组的最大和整数中一出现的次数把数组排成最小的数丑数第一个只出现一次的字符数组中的逆序对两个链表第一个公共节点数字在排序数组中出现的次数二叉树的深度平衡二叉树数字中只出现一次的数字和为S的连续整数序列和为S的两个数组左旋转字符串反转单词顺序序列扑克牌顺子圆圈中最后剩下的数(约瑟夫环)求1+2+3+…+n要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句不加减乘除做加法字符串转换成整数数组中重复的数字构建乘积数组正则表达式表示数值的字符串字符流中第一个不重复的字符链表中环的入口结点删除链表中重复的结点二叉树的下一个结点对称二叉树按之字形式顺序打印二叉树把二叉树打印成多行序列化二叉树和反序列化二叉树二叉搜索树的第k个结点数据流中的中心数滑动窗口的最大值矩阵中的路径机器人的运动范围]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数据结构与算法</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法设计技巧]]></title>
    <url>%2Fp%2F468a.html</url>
    <content type="text"><![CDATA[五大常用算法 分治法分治法是把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题……直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并。这个技巧是很多算法的基础，比如排序算法(快排，归并排序)…… 适用情况： 分治法所能解决的问题一般具有一些几个特征： （1）该问题的规模缩小到一定程度就可以容易地解决。 （2）该问题可以分解成为若干个规模较小的相同问题，即该问题具备最优子结构性质。 （3）利用该问题分解出的子问题的解可以合并为该问题的解。 （4）该问题所分解出的各个子问题是相互独立的，即子问题之间不包含公共的子子问题。 第一条特征是绝大多数问题都可以满足的，因为问题的计算复杂性一般是随着问题规模的增加而增加。 第二条特征是应用分治法的前提它也是大多数问题可以满足的，此特征反映了递归思想的应用。 第三条特征是关键，能否利用分治法完全取决于问题是否具有第三条特征，如果具备了第一条和第二条特征，而不具备第三条特征，则可以考虑用贪心法或动态规划法。 第四条特征涉及到分治法的效率，如果各子问题是不独立的则分治法要做许多不必要的工作，重复地解公共的子问题，此时虽然可用分治法，但一般用动态规划法较好。 可用分治法的经典问题： （1）二分查找（2）大整数乘法（3）合并排序（4）快速排序还有很多，遇到问题在列出来。 动态规划(dynamic programming)动态规划常常适用于有重复子问题和最优子结构性质的问题，动态规划方法所消耗时间往往元小于朴素解法。 基本思想与分治法类似，也就是将带求解的问题分解成为若干个子问题（阶段），按顺序求解子阶段，前一子问题的解，为后一子问题的求解提供了有用的信息。 与分治法最大的区别：适用于动态规划法求解的问题，经分解后得到的子问题往往不是相互独立的（即下一个子阶段的求解是建立在上一个子阶段的解的基础上，进行进一步求解）。 适用情况： 能采用动态规划求解的问题的一般要有3个性质： （1）最优化原理：如果问题的最优解所包含的字问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。 （2）无后效性：即某阶段状态一旦确定，就不受这个状态以后决策的影响。 （3）有重叠子问题：即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。（该性质并不是DP适用的必要条件，但是如果缺少这条性质，则DP比其他算法没有了优势）。 回溯法(backtracking)回溯法是一种暴力搜寻法，是一种可以找出所有(或部分)解的一般性算法，尤其适用于约束满足问题。 许多复杂的，规模较大的问题都可以使用回溯法，有通用解题方法的美称。 回溯法采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题答案。 回溯法通常用最简单的递归方法来实现，在反复重复上述的步骤后可能出现两种情况： 找到一个可能存在的正确答案 在尝试了所有可能的分步方法后宣告该问题没有答案 教科书般的用例：N个皇后问题。 最坏的情况下，回溯法会导致一次复杂度为指数时间的计算。 LeetCode的一个系列回溯法的题：39.Combination Sum 40.Combination Sum II216.Combination Sum III 还存在337.Combination Sum IV,不过回溯法会很慢，LeetCode直接超时，但用动态规划保存已计算的值，会非常的快。 回溯其它练习，见github或者上LeetCode 贪婪法贪婪算法分阶段地工作，每一个阶段，可以认为是所做决定是最好的，而不考虑将来的结果。通常，这意味着选择的是某个局部最优。当算法结束时，希望局部最优等于全局最优。如果是这样的话，算法是正确的；否则，算法得到的是一个次最优解。 对于采用的贪心策略一定要仔细分析其是否满足无后效性。 这个算法不太熟，练习不到位，刷几波LeetCode再来做笔记。 分支限界法与回溯法有点相像，但不同。回溯法的求解目标是找出T中满足约束条件的所有解，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。 遇到题了再来朴充。 最后附上学习地址红脸书生]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7+php7.1.4+mysql5.7+swoole搭建记录]]></title>
    <url>%2Fp%2F7710.html</url>
    <content type="text"><![CDATA[安装php7.1.4查看 centos 版本（我的是centos72） 1cat /etc/centos-release 删除之前的 php 版本 1yum remove php* php-common rpm安装php7.1相应的yum源 123rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm 用下面命令查看yum所拥有版本的各个插件 1yum list php* 安装php7.1，也可以日后看需要什么就可以单独安装插件。 1yum install php71w php71w-opoache php71--cli php71w-devel 到这里php就按装完成了。 查看php版本 1php -v 安装swoole安装git 1yum install git 在home目录下 1git clone https://github.com/swoole/swoole-src.git 继续 1234cd swoole-srcphpize./configuremake &amp;&amp; make install 注释：如果有报错的可能需要 1yum install php-pear 修改php.ini 1vim /etc/php.ini 添加 1extension=swoole.so 安装完毕。 查看是否成功php -m. 安装mysql5.7MySQL官方网站下载MySQL的YUM源，在MySQL的下载页有一个“NEW！MySQL YUM Repository”，点进去找相对应的系统rpm包进行下载.如果你的Linux系统能联网，就把下载链接拷贝下： 1wget http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm 下载完后就是一个mysql57-community-release-el7-7.noarch.rpm的文件，可以用以下命令查看该文件都包含哪些包： 1rpm -qpl mysql57-community-release-el7-7.noarch.rpm 安装rpm包 1rpm -ivh mysql57-community-release-el7-7.noarch.rpm 安装完上述包后，查看yum库， 1yum list Mysql* 之后就可以用yum安装MySQL了： 1yum install mysql-community-server 安装完成。 查看已安装版本 1mysql -V 设置mysql密码 1、停止mysql服务 1systemctl stop mysqld.service 2、mysql配置文件修改为免密码登录。 1vi /etc/my.cfg 修改 123# Disabling symbolic-links is recommended to prevent assorted security risksskip-grant-tables #添加这句话，这时候登入mysql就不需要密码symbolic-links=0 3、启动 mysql 服务 1systemctl start mysqld.service 4、以root身份登录mysql, 输入密码的时候直接回车 123456789mysql -u root -p #输入命令回车进入，出现输入密码提示直接回车。 mysql&gt; set password for root@localhost = password(&apos;123456&apos;);ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement mysql&gt; flush privileges; Query OK, 0 rows affected (0.00 sec)mysql&gt; set password for root@localhost = password(&apos;123456&apos;); or update user set authentication_string=PASSWORD(&quot;123456&quot;) where user=&quot;root&quot;;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt;flush privileges; #更新权限mysql&gt;quit; #退出 继续 12systemctl stop mysqld.service # 停止mysql服务, 恢复mysql配置vi /etc/my.cnf 修改my.cnf 123Disabling symbolic-links is recommended to prevent assorted security risksskip-grant-tables # 注释掉这句话symbolic-links=0 设置完毕。 12systemctl start mysqld.service # 启动mysql服务mysql -uroot -p # 输入新密码登录 mysql5.7安装完成。 centos7防火墙查看已经开放的端口号和协议 1firewall-cmd --list-ports 开放端口 1firewall-cmd --zone=public --add-port=80/tcp --permanent –zone是作用域，设置为public。 –add-port是添加开放的端口与协议，设置为80/tcp。 –permanent是设置为永久生效，否则重启后就无效了。 设置后需要重启防火墙 1firewall-cmd --reload 如果要关闭防火墙，并且开机不在自动启动防火墙，需要输入下面的两个命令 123systemctl stop firewalld.service #停止systemctl enable firewalld.service #开机时启动]]></content>
      <categories>
        <category>环境搭建教程</category>
      </categories>
      <tags>
        <tag>php</tag>
        <tag>swoole</tag>
        <tag>centos 7</tag>
        <tag>mysql 5.7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海量数据处理算法]]></title>
    <url>%2Fp%2F1c09.html</url>
    <content type="text"><![CDATA[基本方法hash法前言 适用于在处理海量数据的过程中，使用hash方法一般可以快速存取、统计某些数据，将大量数据进行分类，例如提取某日访问网站次数最多的ip地址。 简介 hash主要用来进行“快速存取”，在O(1)时间复杂度里就可以找到目标元素，或者判断是否存在。hash一般被称为散列，给定关键字key，按一个确定的散列函数计算出hash(key),把hash(key)作为关键字key对应元素的存储地址（或者称散列地址）。散列函数就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。散列函数是用于关键字与存储地址之间的一种映射关系。散列表是具有固定大小的数组，其中，表长（数组的大小）应该为质数。但是不能保证每个关键字与函数值一一对应，就是有可能存在冲突。所以确定一个好的散列函数至关重要。 常用散列函数的构建方法一般有以下几种： (1) 直接寻址法取关键字或关键字的某个线性函数值为散列地址，即h(key) = key 或 h(key)= a*key+b,其中a,b均为正整常数，这种散列函数叫做自身函数。散列函数取关键字自身，但是这种方法效率比较低，时间复杂度为O(1)，空间复杂度O(n),n为关键字的个数。直接寻址法不会产生冲突，但是由于没有压缩映像，因此集合很大时变得不切实际。 (2) 取模法选择一个适合的正整数m，令h(key)=key mod m, m选择较大的素数比较好，一般是散列表的表长。 (3) 数值分析法设关键字的d位的以r为基的数（例如以10为基的十进制数），且共有n个关键字，则关键字的每个位可能有r个不同的数符出现（即0，1，…，9），但这个r个数符在各个位上出现的频率不一定相同，选取其中各种符号分布均匀的的若干位作为散列地址。例如：散列表地址范围有三位数，观察下列关键吗后，取各关键吗的4，5，6位作为记录的散列地址。 123458 5 3 2 4 88 5 2 3 6 98 5 1 5 3 78 5 2 8 1 5① ② ③ ④ ⑤ ⑥ 数值分析法仅适用于事先明确知道所有关键字的每一位的分布情况，所以使用局限。 (4)折叠法将关键字分成位数有t的几部分（最后一部分的位数可能小于t），然后把各部分按位对齐相加，将所的和舍弃进位，留下t位作为散列地址。当关键字很多，而且关键字中每位上的数字分布比较均匀时，采用折叠法比较合适。 (5)平方取中法这是一个较常用的方法，将关键字进行平方运算，然后从结果的中间取出若干位（位数与散列地址的位数相同），将其作为散列地址。 (6)除留余数法这是一种比较常用的散列函数，其主要原理是取关键字除以某数m（m不大于散列表长度）的余数作为散列地址，即hash(key)=key % m,m一般是散列表的长度，并且时质数。 (7)随机数法选择一个随机函数，然后用关键字key的随机函数值作为散列地址，即hash(key)=random(key),random()为随机函数。但关键字长度不相等时，采用这个方法适合。 在构造散列函数的过程，不管用什么散列函数总是会存在散列冲突问题，下面是几种解决散列冲突的方法: (1)开放定址法基本思想是当发生冲突时，在散列表中再按照某种中方法继续探测其他的存储地址，直到找到空闲的地址为止。该过程可描述为 1hash_i(key) = (hash(key) +d(i)) mod m (i=1,2,...,k( k&lt;=m-1 )) 其中，hash(key)为关键字key的直接散列地址，m 为散列表的长度，d(i)为每次再探测时的地址增量。增量d(i)可以有不同的取法，常用有以下三种：a. d(i)=1，2，…，m-1， 称为线性探测法。b. d(i)=1^2，-1^2，2^2，-2^2，…， 称为二次探测法。c. d(i)=为伪随机序列，称为伪随机探测法。 下面举个例子来说明查找成功的平均查找长度(ASLsucc)和查找不成功的平均查找长度(ASLunsucc)：例题一：关键字为{75，29，48，94，25，22，45}，散列函数为hash(key)=key%23，采用线性探测法处理冲突。 123地址号 0 1 2 3 4 5 6 7 ... 22关键字 45 48 94 25 75 29 22探查次数 2 1 2 2 1 2 1 ASLsucc=(2+1+2+2+1+2+1)/7=1.59ASLunsucc=(16+2+4+3+2+3+2+3)/23=1.52 (2)链接法基本思想是通过链表数组来解决冲突（HashMap的数据结构），通过hash函数定位到固定长的数组下标，如有冲突则添加到相连的链表中。 (3)再散列法当发生冲突时，使用第二个、第三个、第四个hash函数计算地址，直到没有冲突为止。这种方法的缺点是计算时间大幅度增加。 (4)建立一个公共溢出区假设散列函数的值域为[0,m-1],则设向量HashTable[0…m-1]为基本表，另外设立存储空间向量OverTable[0…v]用于存储发生冲突的记录。 Bit-map法前言 适用于海量数据的快速查找、判重、删除等。 简介 Bit-map(位图)法的基本原理是使用位数组来表示某些元素是否存在。具体而言，Bit-map法的结果是生成一个N位长的串，每位上以“1”或“0”表示需要排序的集合中的数。例如集合位{2，6，4，9，1，10}，则生成一个10位的串，将2、6、4、9、1、10位置为“1”，其余是“0”，当把串中所有的位都置完后，排序也自动完成了，结果是11001001011。Bit-map法排序的时间复杂度是O(n)，比一般的排序都快，但它是以空间换时间的，而且有些限制，即数据状态不是很多，例如排序前集合的大小最好已知，而且集合中的元素的最大重复次数必须已知，最好是惆集数据（不然空间浪费很大）。 Bloom Filter法前言 在海量数据处理中，经常需要判断一个元素是否在集合中，Bloom Filter法可以解决此问题。 简介 Bloom Filter法是一种空间效率和时间效率都很高的随机数据结构，可用来检测一个元素是否属于一个集合。但他同样带来了问题，牺牲了正确性。具体而言，查询结果俩种可能，即“不属于这个集合（绝对正确）”和“属于这个集合（可能错误）”。基本原理是位数组和hash函数的联合使用。首先，Bloom Filter是包含了m位的位数组，数组的每一个位都是初始化为0，；其次，定义k个不同的hash函数，每个函数都可以集合中的元素映射到数组的某一位，将这些位置于“1”.如果查询某个元素是否属于集合，那么根据k个hash函数可以得到位数组的k位，查看k位，如果有的位不为“1”，则钙元素肯定不存在；如果全部为“1”，则可能存在。 Trie树前言 Trie树的典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。 简介 Trie 这个单词来自“retrieve”，Trie 树又称字典树或键树，它是一种用于快速字符串检索的多叉树结构，其原理是利用字符串的公共前缀来减少时空开销，即空间换时间。它的优点是：最大限制地减少无谓的字符串比较，查询效率比散列表高。缺点：若系统中存在大量字符串且这些字符串基本没有公共前缀，这对应的Trie树非常的消费内存。Trie树的一般具有3个特性：(1)根节点不包含字符，除根结点外每一个节点都只包含一个字符。(2)从根节点到某一结点，路径上经过的字符串连起来，为该节点对应的字符串。(3)每个节点的所有子节点包含的字符不相同。 例子：要保存5个字符串tea,ten,to,int,inn.如图所示： 堆前言 堆特别适合海量数据中寻找前N大数据（最大堆）或者前N小数据问题，其中N比较小。 简介 堆是一种树形结构，每个节点都有一个值，而通常所说的堆，一般是指二叉堆，有最大堆和最小堆。 双层桶法前言 桶排序一般适用于寻找第K大的数、寻找中位数、寻找不重复或重复的数字等情况。 简介 双层桶不是数据结构，而是一种算法实现，类似于分治思想，其次并不是分两次，“双”只是个拟词，只要没达到要求就可以继续分。因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。桶排序的平均时间复杂度为O(n),最坏情况下O(n^2)。例子：存在集合{11，2，5，17，45，31，36，27，26}，用桶排序。解题：最大值为45，所以分成了5个桶，并在桶内进行选择/堆/快排后在整合在一起。 经典实例分析top K问题前言 在海量数据处理时经常遇到需要寻找出现频率最高的或最大的前K个数，这类问题通常被称为top K问题，例如搜索引擎中，统计搜索引擎的最热门10个查询词，或者最热门的20个歌曲。 简介 针对top K类问题，通常比较好的方案是分治+Trie树/hash+小顶堆，即先将数据集按照hash方法分解成多个小的数据集，然后使用Trie树或者hash统计每个小数据集中的查询词频之后用小顶堆求出每个数据集中出频率最高的前K个数，最后在所有top K中求出最终的top K。 例如，有1亿个浮点数，如何找出其中的最大的10000个？ 第一种方法：将数据全部排序，然后在排序后的集合中查找，最快的排序算法的时间复杂度为O(nlogn),例如快排、归并。这个方法效率不高，不想讨论。 第二种方法：局部淘汰法，该方法于排序方法类似，用一个容器保存前10000个数，然后将剩余的数字跟容器内最小的数比较，如果大则替换，最后容器的10000数就是前10000个数了。此时的时间复杂度为O(n+m^2)，m为容器大小，即10000. 第三种方法：分治法，将1亿数据分成100份，每份100万个数据，找出每份数据中最大的10000个，最后剩下100×10000数据找出最大的10000个。如果100万数据选的足够理想，那么可以过滤掉99%的数据。100万个数据里面查找最大的10000个数据的方法如下：用快速排序的方法，将数据分成两堆，如果大的那个堆个数N大于10000个，继续对大堆快速排序依次分成两堆；如果大堆个数n小于10000个，就在小的堆里面快排一次，找第10000-n大的数字；递归以上操作，就可以找到10000大的数了。此方法每次需要的内存空间为100万*4=4M，一共需要101次这样的比较。 第四种方法：hash法，如果这1亿个数据里面哟据很多的重复的数据，则通过hash方法去重，可以提高效率，然后再运用分治法或最小堆法进行查找最大的10000个数。 第五种方法：最小堆方法，先读入前10000个数字来创建小顶堆，建堆的时间为O(mlogm)（m为数组的大小），然后遍历其余数据，并于堆顶比较，若比堆顶小，则替换调整最小堆。遍历完数据按中序遍历输出10000个数。该算法的时间复杂度为O(nmlogm)，空间复杂度为10000。 究竟采用哪种方法，按实际问题和实际应用场景而定，灵活运用。 下面针对不同的应用场景，分析了合适相应应用场景的解决方案。 (1)单机+单核+足够大的内存内存够用的情况下，直接全部读取，顺序遍历查找top K即可。 (2)单机+多核+足够大的内存这时可以直接再内存中使用hash方法将数据分成c*n个部分（c&gt;1,n为处理器个数），每一部分都分给一个线程来执行，当一个线程完成时主动获取下一个部分，直至数据处理完毕，并用一个线程将结果归并。 (3)单机+单核+内存受限这时需要将原数据文件割成一个一个小文件，如采用hash（x）% M，将文件分割成M个小文件，这样每个文件放到内存中既可以采用方法（1）一次处理每个小文件了。 (4)多机+内存受限这是纪要把文件分发到多台设备来处理。 其实在现实中1-4的方法并不可行，因为在大规模的数据处理环境中，作业的效率并不是首要考虑的问题，算法的扩展性和容错性才是首要考虑的。所以top K问题很适用于MapReduce上解决。 下面是一些历年来经常被各大互联网公司提及的top K问题： (1)搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，请你统计最热门的10个查询串，要求使用的内存不能超过1G。。 分析：有1000万记录，去重后大概有300万记录，每个算255字节，因此把他们都装内存需要3M*1KB4=0.75GB，所以可以把所有记录在内存中处理，用HashMap在合适不过了。放弃分而治之方法。 解题：第一步：先对这批海量数据预处理。具体方法是：维护一个Key为Query字串，Value为该Query出现次数的HashMap，即HashMap(Query，Value)，每次读取一个Query，如果该字串不在Map中，那么加入该字串，并将Value值设为1；如果该字串在Map中，那么将该字串的计数加1即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计。第二步：可以维护k个元素的最小堆，即用容量为k的最小堆存储k个元素，每次堆顶比较，如果更小则替换并调整最小堆。总费时O(k*logk+(n-k)*logk)=O(n*logk)。 (2)提取某日访问网站次数最多的ip。 分析：百度作为国内第一大搜索引擎，每天访问它的IP数量巨大，如果想一次性把所有IP数据装进内存处理，则内存容量明显不够，故针对数据太大，内存受限的情况，可以把大文件化成(取模映射)小文件，从而大而化小，逐个处理。换言之，先映射，而后统计，最后排序。 解题：分而治之hash映射：首先把这一天访问百度的日志的所有IP提取出来，然后逐个写入到一个大文件中，接着采用映射的方法，比如%1000，把整个大文件映射为1000个小文件。hash_map统计：当大文件转化了小文件，那么我们便可以采用hash_map(ip，value)来分别对1000个小文件中的IP进行频率统计，再找出每个小文件中出现频率最大的IP及相应的频率。堆/快速排序：统计出1000个频率最大的IP后，进行排序(可采取堆排序)，找出那个频率最大的IP，即为所求。 说明：Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中去的情况，即这里采用的是%1000算法，那么同一个IP在hash后，只可能落在同一个文件中，不可能被分散的。 (3)有一个10文件，每个文件1GB，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。请按照query的频率排序。 解题：hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件。最后，对这10个文件进行归并排序（内排序与外排序相结合）。 重复问题前言 在海量数据中查找出重复出现的元素或者去除重复出现的元素也是常考的内容。例如，已知某文件包含一些电话号码，每个号码为8位数，统计不同的号码的个数。 简介 针对此类问题，一般用位图法实现。8位整数可以表示的最大进制数值为99999999，如果每个数字对应于位图中的1个bit位，那么存储8位正整数需要99Mbit，因为1byte=8bit，所以99Mbit折合成内存为99/8=12.375MB的内存，即用12.375MB来表示所有的8位电话号码的内容。 下面是一些历年来经常被各大互联网公司提及的重复问题： (1)10亿个正整数，只有1个重复出现过，要求再O(n)的时间里找出这个数。 解题：用位图法，10亿数据就需要用1Gbit空间，即125MB空间，不算多，并且能找到重复的数字。 (2)给定a,b两个文件，个存放50亿个url，每个url各占用64byte，要求再O(n)的时间里找出啊a、b文件共同的url。 分析：50亿数据就是5*64GB=320GB大小，一般内存都装不下，所以需要分而治之法。 解题：分而治之/hash映射：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,..,a999)中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,…）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 排序问题前言 海量数据处理中类常见的问题就是排序。例如，9亿条不重复的9位整数，对这个文件中的数字进行排序。 简介 对于此类问题，最直接的方法是全部导入内存并排序，但是这个一般不可行，需要考虑其它的方法：方法一，数据库排序法。将文本文件导入到数据库中，让数据库进行索引排序后在读取到文件中。该方法虽然操作简单、方便，但是运算慢，而且对数据库设备要求比较高。 方法二，分治法。通过hash法将9亿条数据分为20段，每段大约5000万条，大约占5000万*4B=200MB空间，在文件中一次搜索05000万，5001万1亿…将排序的结果存入文件。该方法需要遍历20次文件，虽然缩小了每次使用内存空间大小，但是编码复杂，速度慢。 方法三，位图法。由于9亿条数据，所以9Gbit空间，即大约120MB空间。如果存在某数字，则下标置为“1”，这样就排序完了。 下面是一些历年来经常被各大互联网公司提及的排序问题： 有关正整数的情况就是用位图法。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>大数据</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows10用hexo+yilia搭建github page详细过程记录]]></title>
    <url>%2Fp%2F504f.html</url>
    <content type="text"><![CDATA[简单的记录自己的github page搭建全部过程。 准备环境依次下载安装： Nodejs下载 Git Bash下载 github 需要注册GitHub，且创建仓库，仓库名称一定是yourname.githuab.io，以后yourname.github.io来访问自己的博客。 配置安装hexo前需要在自己想放代码或博客的地方新建一个文件夹，并命令下cd到此文件下再继续下面操作。 安装hexo命令行输入 1npm install hexo-cli -g 可能会看到一个WARN，但是不用担心，这不会影响你的正常使用。 然后输入 1npm install hexo --save 查看是否安装好了 1hexo -v 如果正常显示就可以了。 初始化hexo接着上面的操作 12hexo init` 成功后 1npm install 之后会自动安装全部。 2.3 首次体验继续操作 1hexo g 再输入(开启本地服务器的命令) 1hexo s 然后在浏览器中输入 1http://localhost:4000/ 就可以看见首页了，到目前为止，Hexo在本地的配置已经全都结束了。 使用使用方面就不做详细笔记了，把自己搭建参考的文章附上。 史上最详细的Hexo博客搭建图文教程 如何搭建一个独立博客——简明Github Pages与Hexo教程 还有图片上传的教程 hexo博客图片问题 并附上一些牛逼的hexo主题 Hexo 主题]]></content>
      <categories>
        <category>环境搭建教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github page</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8学习笔记(1)]]></title>
    <url>%2Fp%2F3393.html</url>
    <content type="text"><![CDATA[Java 8新特性之Java语言新特性 Lambda表达式和函数式接口Lambda表达式(也称为闭包)，它允许我们将函数当成参数传递给某个方法，或者把代码本身当作数据处理。Java8以前，java开发者只能使用匿名类来代替Lambda表达式。 官方文档 匿名类和Lambda表达式相比于匿名类，Lambda表达式更加的简洁。 代码如下： Person类： 12345678910111213141516171819202122232425262728public class Person &#123; public enum Sex &#123; MALE, FMALE &#125; String name; LocalDate birthday; Sex gender; String emailAddress; public Person(String name) &#123; this.name=name; this.gender=Sex.MALE; this.emailAddress="emailAddress"; this.birthday=LocalDate.now(); &#125; public int getAge() &#123; // ... return 0; &#125; public void printPerson() &#123; //... System.out.println("yes"); &#125;&#125; LambdaExpressions类： 12345678910111213141516171819202122232425262728293031323334353637383940public class LambdaExpressions &#123; public static void printPersons(List&lt;Person&gt; roster, CheckPerson tester) &#123; for (Person p : roster) &#123; if (tester.test(p)) &#123; p.printPerson(); &#125; &#125; &#125; public static void main(String[] args) &#123; //自己添加的测试用例 LinkedList&lt;Person&gt; roster = new LinkedList&lt;&gt;(); Person person=new Person("max"); roster.add(person); // 匿名类 printPersons(roster, new CheckPerson() &#123; @Override public boolean test(Person p) &#123; return p.gender == Person.Sex.MALE &amp;&amp; p.getAge() &gt;= 0 &amp;&amp; p.getAge() &lt;= 25; &#125; &#125;); // Lambda表达式 printPersons(roster, p -&gt; p.gender == Person.Sex.MALE &amp;&amp; p.getAge() &gt;= 0 &amp;&amp; p.getAge() &lt;= 25); &#125;&#125;// 函数接口@FunctionalInterfaceinterface CheckPerson &#123; boolean test(Person p);&#125; Lambda表达式语法组成Lambda表达式可由逗号分隔的参数列表、-&gt;符号和主体(单个语句或语句块)组成，例如： 123p -&gt; p.gender == Person.Sex.MALE &amp;&amp; p.getAge() &gt;= 18 &amp;&amp; p.getAge() &lt;= 25 在上面的代码中的参数p的类型由编译器推理得出的，你也可以显式指定该参数的类型，例如： 123(Person p) -&gt; p.gender == Person.Sex.MALE &amp;&amp; p.getAge() &gt;= 18 &amp;&amp; p.getAge() &lt;= 25 如果Lambda表达式需要更复杂的语句块，则可以使用花括号将该语句块括起来，类似于Java中的函数体，例如： 1234Arrays.asList("a","b","c").forEach(e-&gt;&#123; System.out.println(e); System.out.println(e);&#125;); Lambda表达式有返回值，返回值的类型也有编译器推理得出。如果Lambda表达式中的语句只有一行，这可以不用使用return语句，return语句不是表达式; 在lambda表达式中，您必须用大括号（{}）括起来,下面两个代码片段效果相同： 1Arrays.asList("a","b","c").sort((e1,e2)-&gt;e1.compareTo(e2)); 和 1234567891011Arrays.asList("a","b","c").sort((e1,e2)-&gt;&#123; return e1.compareTo(e2);&#125;);``` Lambda表达式可以应用类成员和局部变量(会将这些变量隐式的转换成final)，在上面的匿名类和Lambda表达式中也用到了类成员。 ```javaString separator = ",";Arrays.asList( "a", "b", "d" ).forEach( ( String e ) -&gt; System.out.print( e + separator ) ); Lambda的设计者们为了更好的兼容现有的功能，提出了函数式接口。 函数接口是指只有一个函数的接口，这样的接口可以隐式的转换为Lambda表达式。java.lang.Runnable和java.util.concurrent.Callable是函数式接口的最佳例子。 在实践中函数接口非常的脆弱,只要某个开发者在该接口中添加一个函数，则该接口就不再是函数式接口进而导致编译失败。为了克服这种代码层面的脆弱性，并显式说明某个接口是函数式接口，Java 8 提供了一个特殊的注解@FunctionalInterface（Java 库中的所有相关接口都已经带有这个注解了），举个简单的函数式接口的定义(上面也用到了函数接口)： 1234@FunctionalInterfaceinterface CheckPerson &#123; boolean test(Person p);&#125; 注意：默认方法和静态方法是不会破坏函数接口的定义，下面代码是合法的： 1234567@FunctionalInterfacepublic interface FunctionalDefaultMethods &#123; void method(); default void defaultMethod() &#123; &#125; &#125; 接口的默认方法和静态方法官方文档 Java 8使用两个新概念扩展了接口的含义：默认方法和静态方法。 默认方法：默认方法使得开发者可以在 不破坏二进制兼容性的前提下，往现存接口中添加新的方法，即不强制那些实现了该接口的类也同时实现这个新加的方法。 默认方法和抽象方法之间的区别在于抽象方法需要实现，而默认方法不需要。接口提供的默认方法会被接口的实现类继承或者覆写，例子代码如下： 1234567891011121314151617private interface Defaulable &#123; // Interfaces now allow default methods, the implementer may or // may not implement (override) them. default String notRequired() &#123; return "Default implementation"; &#125; &#125;private static class DefaultableImpl implements Defaulable &#123;&#125;private static class OverridableImpl implements Defaulable &#123; @Override public String notRequired() &#123; return "Overridden implementation"; &#125;&#125; Defaulable接口使用关键字default定义了一个默认方法notRequired()。DefaultableImpl类实现了这个接口，同时默认继承了这个接口中的默认方法。OverridableImpl类也实现了这个接口，但覆写了该接口的默认方法，并提供了一个不同的实现。 静态方法：接口可以定义静态方法。代码如下： 123456private interface DefaulableFactory &#123; // Interfaces now allow static methods static Defaulable create( Supplier&lt; Defaulable &gt; supplier ) &#123; return supplier.get(); &#125;&#125; 下面代码片段整合了默认方法和静态方法的使用场景： 1234567public static void main( String[] args ) &#123; Defaulable defaulable = DefaulableFactory.create( DefaultableImpl::new ); System.out.println( defaulable.notRequired() ); defaulable = DefaulableFactory.create( OverridableImpl::new ); System.out.println( defaulable.notRequired() );&#125; 这段代码的输出结果如下： 12Default implementationOverridden implementation 默认方法允许在不打破现有继承体系的基础上改进接口。该特性在官方库中的应用是：给java.util.Collection接口添加新的方法，如stream()，parallelStream(),forEach(),removeIf()等等。 方法引用官方文档]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java 8</tag>
      </tags>
  </entry>
</search>
